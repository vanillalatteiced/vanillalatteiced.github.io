{"0": {
    "doc": "Terraform Basics",
    "title": "Terraform Basics",
    "content": "# Terraform Basics {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Install Terraform ```zsh brew tap hashicorp/tap brew install hashicorp/terraform terraform -version ``` --- ## Configuration The set of files used to declare infrastructure. Such files have an extension of `.tf` and are required to be in its own working directory. ```zsh mkdir tf-aws-instance cd tf-aws-instance touch main.tf ``` The following is an example configuration `main.tf`: ```tf terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 3.27\" } } required_version = \">= 0.14.9\" } provider \"aws\" { profile = \"default\" region = \"us-west-2\" } resource \"aws_instance\" \"app_server\" { ami = \"ami-830c94e3\" instance_type = \"t2.micro\" tags = { Name = \"ExampleAppServerInstance\" } } ``` {: .note} Terraform also provides `terraform fmt` and `terraform validate` for formatting configuration files and checking its syntax. `terraform fmt` does not produce any output if no modification is made. For details, see [Terraform Configuration](/docs/terraform/terraform-config.html). --- ## Initialize After creating a configuration or checking out an existing configuration, initialize directory with ```zsh # Installs providers in .terraform folder and also creates .terraform.lock.hcl terraform init ``` --- ## Create infrastructure and inspect state To see the execution plan, ```zsh terraform plan ``` To actually apply, ```zsh # Will print an execution plan, type yes to perform the actions terraform apply # OR terraform apply --auto-approve ``` A Terraform state file `terraform.tfstate` will be generated. The file contains sensitive info, so share with only those trusted. ```zsh # Inspect the current state terraform show ``` For manual/advanced state management, use `terraform state`. One example of the command is, ```zsh # List resources in state terraform state list ``` --- ## Output file You can query data after `apply` using an output file. Create a file called `output.tf` (*name doesn't matter*) with the following ```tf output \"instance_id\" { description = \"ID of the EC2 instance\" value = aws_instance.app_server.id } output \"instance_public_ip\" { description = \"Public IP address of the EC2 instance\" value = aws_instance.app_server.public_ip } ``` You will see the queried output when you run `terraform apply`. You can also inspect the output by ```zsh # Call after `terraform apply` terraform output ``` --- ## Destroy infrastructure The following terminates all resources managed with project state ```zsh # Just like apply, shows you the execution plan. Type yes to destroy. terraform destroy # OR terraform destroy --auto-approve ``` --- References: - [Terraform: AWS Get Started](https://learn.hashicorp.com/collections/terraform/aws-get-started) - [Terraform Registry: AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs) --- ",
    "url": "/docs/terraform/basics.html",
    "relUrl": "/docs/terraform/basics.html"
  },"1": {
    "doc": "Elastic Beanstalk",
    "title": "Elastic Beanstalk",
    "content": "# AWS Elastic Beanstalk (EB) {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## + What is Elastic Beanstalk? **Elastic Beanstalk** is a Platform as a Service (PaaS) that helps you deploy web apps with little knowledge about what kind of infrastructure is managed underneath. It configures its components to provide an environment for your application to run on. EB is basically a Cloudformation template with a UI. --- ## + Elastic Beanstalk Application An Elastic Beanstalk application is a logical collection of application version and environments. --- ## + Environment Tier When you create an EB application, you are asked to choose an environment tier. This tier determines which resources EB should provision to form your environment. When creating a web app, you often require both environment tiers. --- ### Web Server Environment Key resources launched in the EB container: - Elastic Load Balancer - EC2 (Auto Scaling Group, Security Group) A web server environment serves HTTP requests. Web server environment is given a URL of `myapp.region.elasticbeanstalk.com`. This environment creates an Elastic Load Balancer with a URL of of `elb-id.region.elb.amazonaws.com`. In Amazon Route 53, this ELB URL has a CNAME Record to the environment URL. This ELB sits in front of EC2 instances in a Auto Scaling Group (ASG). The stack on EC2 instances depends on which platform you chose (eg. Python 3.8 running on 64bit Amazon Linux 2). However, in each instance sits one common component called the host manager (HM). HM manages all sorts of monitoring, deploying, and metrics related to the instance. By default, EC2 instances are placed in a security group which allows all connection through port 80 (HTTP). Additional security groups maybe configured as needed. --- ### Worker Environment Key resources launched in EB container: - SQS - EC2 (Auto Scaling Group), Sqsd - Cloudwatch Worker environment is usually set up for long running tasks to run in the background. A worker environment sets up an Amazon SQS queue. This queue often consists of messages from a web server environment. On each EC2 instance runs a Sqsd daemon and a processing application. The daemon reads the message from the SQS queue and sends it as an HTTP POST request to the processing application. Upon a `200 OK` response from the processing application, Sqsd sends a delete message call to SQS. EC2 instances publish their metrics to Amazon Cloudwatch. Auto Scaling retrieves usage data from Cloudwatch and scales instances accordingly. --- ## + Deployment Each deployment is identified with a deployment ID which increments from 1. --- ### In-Place Deployment Policies ![Deployment Policies](../../assets/eb-deployment-policies.png) #### All at once Every instance is killed and updated at the same time. The deployment is quick in that sense, but it results in a short loss of service. Also, it can be dangerous in case of a failure to deploy, and may be tricky to rollback. #### Rolling Updates one batch of instances at a time. So a batch can be down during an update which may result in reduced availability for a short time. However, there is no downtime unlike 'All at once', but the entire deployment process takes a longer time. #### Rolling with additional batch To avoid any reduced bandwidth in regular rolling deployment, an extra batch of instances is launched and rolling update is performed there. Hence, the number of instances up during deployment stays the same. This takes longer time. #### Immutable Instead of updating instances, a complete new Auto Scaling Group set of instances is created. This is even slower. #### Traffic Splitting Create a new set of instances and test it with a portion of the incoming traffic, while the rest of the traffic is still going to the old deployment version. This is as slow as 'Immutable'. --- ### Blue/Green Deployment Policy One additional deployment option is the **Blue/Green deployment**. All the other deployment policies above performs an **In-Place** deployment, which means the update happens within an EB environment. However, Blue/Green deployment goes beyond the instances inside the environment. To avoid downtime, your deployment is launched to a complete new set of **environment** and then the CNAMEs of old and new environments are swapped to redirect traffic instantly. --- ## + Configuring Environments There are many different ways to configure environments. ### Order of Precedence 1. Settings applied directly during create/update environment 2. Saved configuration objects in S3 3. Configuration files (.ebextensions, env.yaml) 4. Default values --- ### Configuration Files (.ebextensions) You can place `.config` files in a folder `.ebextensions` at the root of the application source bundle. Each `.config` files are applied in alphabetical order. YAML is recommended for configuration files but both YAML and JSON are supported. --- #### Option Settings Use `option_settings` key to configure environment options ```yaml option_settings: - namespace: namespace option_name: option name value: option value ``` --- #### Linux Server You can also configure the software running on your instances. Check these [link1][linux_server], [link2][linux-server-platform] for details. --- ### Environment Manifest (env.yaml) Place an `env.yaml` file at the root of the application source bundle to configure the environment. You can configure the name, solution stack, and links to other environments. There are some overlaps between `.config`s and `env.yaml`. It seems `env.yaml` is more environment specific, while `.config` files can handle overall configuration of the application. Check the [link][env.yaml] for details. --- ## + EB CLI EB CLI is an open-source project hosted in [this][eb-cli] repository. To use the CLI application, however, clone [this][eb-cli-setup] setup repository instead. --- References: - [Web Server Environment][web_server_env]] - [Worker Environment][worker_env]] - [Deployments][deployment_policies] - [Configuring Environment][configuration] - [EB CLI][eb-cli-doc] [web_server_env]: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html [worker_env]: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html [deployment_policies]: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html [configuration]: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/beanstalk-environment-configuration-advanced.html [env.yaml]: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-manifest.html [linux_server]: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html [linux-server-platform]: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platforms-linux-extend.html [eb-cli]: https://github.com/aws/aws-elastic-beanstalk-cli [eb-cli-setup]: https://github.com/aws/aws-elastic-beanstalk-cli-setup [eb-cli-doc]: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3.html ",
    "url": "/docs/aws/beanstalk.html",
    "relUrl": "/docs/aws/beanstalk.html"
  },"2": {
    "doc": "Chalice",
    "title": "Chalice",
    "content": "# Chalice {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## What is Chalice? It is a python serverless microframework. What it essentially does is combine AWS API Gateway and associated Lambda functions to help you quickly deploy a microservice. Everything you can do in Chalice you can do in the AWS console, but it is easier to manage via code. The syntax and the concept is very much similar to Flask if you're familiar with it. --- ## Install Chalice ```zsh pip3 install chalice ``` {: .note} As of now (2021-05), `chalice` best supports Python 3.8. There may be issues if you use versions more recent than 3.8. --- ## Create a new project To create a new project, ```zsh chalice new-project myproj ``` This will create a `myproj` directory ```zsh myproj ├── .chalice ├── app.py └── requirements.txt ``` --- ## Deploy / Delete The AWS credentials must already be set in `~/.aws/config`. To deploy, simply ```zsh chalice deploy ``` To delete, ```zsh chalice delete ``` --- ## Multifiles If you want to have multiple `.py` files apart from the `app.py` (which you will), place all the `lib` or `utils` related file in a folder called `chalicelib`. Anything you add to this directory is recursively added to the deployment. ```zsh myproj ├── .chalice ├── app.py ├── chalicelib └── requirements.txt ``` --- ## Configuration File In `.chalice`, there is a file called `config.json`. This folder contains all the configurations related to this package. You can set app name, deploment stages, environment variables, etc. ### Environment Variables For general environment variables, add the following syntax to `.chalice/config.json` ```json { \"environment_variables\": { \"ENV_VAR\": \"value\", \"ENV_VAR2\": \"value2\" } } ``` You can also set stage specific environment variables by, ```json { \"stages\": { \"dev\": { \"environment_variables\": { \"MY_ENV\": \"value\" } }, \"prod\": { \"environment_variables\": { \"MY_PROD_ENV\": \"value\" } } } } ``` --- ## Deploying with Terraform ```zsh # Will generate deployment.zip and chalice.tf.json chalice package --pkg-format terraform output_dir ``` `chalice package` will generate the Lambda deployments and Terraform configuration files. You can then use Terraform CLI to deploy. See [here][terraform] for details. --- References: - [Chalice: Quickstart](https://aws.github.io/chalice/quickstart.html) - [Chalice: Terraform Support][terraform] - [Chalice: Multifile](https://aws.github.io/chalice/topics/multifile.html) - [Chalice: Configuration File](https://aws.github.io/chalice/topics/configfile.html) [terraform]: https://aws.github.io/chalice/topics/tf.html ",
    "url": "/docs/aws/chalice.html",
    "relUrl": "/docs/aws/chalice.html"
  },"3": {
    "doc": "Cognito",
    "title": "Cognito",
    "content": "# AWS Cognito {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Token: Hosted UI / AWS SDK ### Hosted UI Cognito hosts a login portal and an authorization server by default. This UI is hosted on the `/login` enpoint. After user types in their credentials, a request is automatically made to the `/oauth2/authorize` endpoint. Upon successful authentication, client is redirected to a URL configured for the user pool client. If you're using an implicit flow (not recommended), you will be redirected with a `token` directly. If you're using an authorization code flow, you will be redirected with a `code` parameter which you can exchange later to a token at the `/oauth2/token` endpoint. ### AWS SDK Although the hosted UI option is convenient, one downside of it is that customization is limited. --- ## How to use a custom domain --- [AWSSDK]: https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-authentication-flow.html ",
    "url": "/docs/aws/cognito.html",
    "relUrl": "/docs/aws/cognito.html"
  },"4": {
    "doc": "Docker Container",
    "title": "Docker Container",
    "content": "# Docker Container {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Start an existing container in background (detached) The container will run in background and you will not see its `stdout`/`stderr` ```zsh docker start my-container ``` --- ## Attach container If you want to see outputs from the container in your terminal (ie. logging), you would want to run the container in attached mode. You can either run it in attached mode to begin with by ```zsh docker start my-container --attach # OR docker start my-container -a ``` Or you can attach a running container later ```zsh docker attach my-container ``` ",
    "url": "/docs/docker/container.html",
    "relUrl": "/docs/docker/container.html"
  },"5": {
    "doc": "Dangling Images",
    "title": "Dangling Images",
    "content": "# Docker Dangling Images {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## What are dangling images? When you do ```zsh docker images -a | grep '' # OR docker image ls -a | grep '' ``` Or check the **Images** tab in Docker Desktop, you may see a bunch of images with the name and tag of ``. This is a residue / intermediate image created from previous image builds. It seems they exist as a cached layer for subsequent builds. But it is safe to delete them. You can remove these dangling images by ```zsh docker image prune ``` {: .note} `docker image prune -a` not only removes dangling images but also any unused images. This can come in handy, but if you're keeping any pulled Docker registry images (unused in containers at the moment) in your local storage for some reason, this is not what you want. --- ## Docker Image / Images You may have noticed that there are two Docker CLI commands that seem similar - `docker image` - `docker images` There is a bit of a difference between the two. --- ### Docker Image Actually builds, pulls, and removes images. This command is used to physically manage the images. You can of course list images as well. ```zsh docker image ls ``` ### Docker Images This has to do with displaying in a high-level fashion what kind of images exist. Primary purpose is to display image metadata. ```zsh docker images ``` --- ",
    "url": "/docs/docker/dangling.html",
    "relUrl": "/docs/docker/dangling.html"
  },"6": {
    "doc": "With docker-compose",
    "title": "With docker-compose",
    "content": "{: .note} To be added. ",
    "url": "/docs/demo/flask-login-app/docker-compose.html",
    "relUrl": "/docs/demo/flask-login-app/docker-compose.html"
  },"7": {
    "doc": "DynamoDB",
    "title": "DynamoDB",
    "content": "# DynamoDB {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Local Setup Detailed documentation is provided [here][localddb]. Docker option is available as well. --- ## NoSQL Workbench for DynamoDB This will be a great lifesaver while designing data models and testing connection. You can download it [here][workbench]. --- ## Key design / Data model If you are used to relational database schemas, it is easy to end up designing your database to use multiple tables, to structure logical joins using foreign key-like attribute and what not, or to use multi-level nested structure. However, in NoSQL, all these familiar patterns are not only inefficient, but also almost impossible to manage. There really is no such thing as a schema design in DynamoDB but a careful design of primary key is useful. ### Primary Key There are two types of keys that can consist a primary key in DynamoDB: partition (hash) key and sort (range) key. A primary key could just consist of a partition key or be a compound of partition and sort key. Because each item is identified by a unique primary key, you must use a unique partition key if your primary key only consists of it. However, if you also use the sort key, the partition key may overlap but the sort key must be unique. Partition key and sort key are also called hash and range keys. The naming indicates that the partition key serves as a hashed index to a physical storage internal unit called a partition. The sort key sorts the items within a partition into groups of similar items, effectively providing an efficient way to query for a range. Hence, design of primary key has an impact on the performance of the DB. ### Design In relational databases, primary keys are usually a single attribute (like `StudentID`) of a homogeneous type. However, in DynamoDB it is common to use a multi-purpose (or heterogeneous) key attributes. Typically, every item is given an attribute called `PK` and `SK` for partition and sort key. This way the key attributes may contain any information without restriction. --- ## Itty Bitties - Compared to SQL statements, querying in DynamoDB can be a real pain in the xxx... --- [localddb]: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.DownloadingAndRunning.html [workbench]: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/workbench.html ",
    "url": "/docs/aws/dynamodb.html",
    "relUrl": "/docs/aws/dynamodb.html"
  },"8": {
    "doc": "Frontend Web",
    "title": "Frontend Web",
    "content": "# Frontend Web {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## SPA vs SSR vs Static Site Here is an attempt to understand the exact differences between the three. ### Single-Page Application (SPA) An SPA uses CSR (client-side rendering). Just by that I can already see the glaring difference to SSR (server-side rendering). In CSR, as the name suggests the client (browser) dynamically renders the web app. All the HTML, CSS, and Javascript are loaded in the beginning of the app's lifecycle. Script makes AJAX calls to the API when it needs new data and dynamically integrates it. So technically, there really is only one page that is being presented to the user, it's just that the contents within the page change to meet your needs. One advantage of SPA is that it provides better UX, because there is little to no lag time during navigation within the app. This comes from the fact that SPA does not require duplicate resources again and again after each click unlike MPA (Multiple-Page Application)/SSR. Things that always stay static on a website, like the general frame or style can stay as is and only new data are fetched from server. One disadvantage is that it is generally considered to have poor SEO (Search-Engine Optimization) compared to server-side apps. This is because without JS rendering, the HTML of an SPA is pretty much empty. If you check the source code of an SPA (not from the console), you will see that it does not contain much other than all the scripts that are sitting and waiting to execute upon interaction. In addition, SPAs might not have unique URLs for each content delivered. In many cases the URL stays the same throughout the entire site. Therefore crawling and indexing becomes slow and difficult. ### Server-Side Rendering With all that being said about SPA, SSR is easier to understand. When navigation happens (e.g via click), the server builds the page and hands it over to the browser. Within a browser, you will only see the resources that consist the current page that you are on. You can already see why this could be slow, since it's like asking the chef to dip your nachos every single bite when you could've just had the chips and cheese in front of you and dip it yourself. Due to this nature of SSR, you will see the page flicker upon navigation unlike the smooth UX of SPA. However, the benefit of SSR compared to SPA is that it is more secure, less heavy on the browser (and no memory leaks), and better SEO. ### Static Site Static sites do not have dynamic content and consist of only the static files (HTML, CSS, JS). You could think of this as if the SSR had already rendered every single page that the client might request and had it prepared for you. There is no backend component to static sites and no rendering is involved. --- ",
    "url": "/docs/learned/frontend-web.html",
    "relUrl": "/docs/learned/frontend-web.html"
  },"9": {
    "doc": "Git",
    "title": "Git",
    "content": "# Git {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Fix previous commit This comes in handy when you make a small change in your code after you've committed, but you realize you probably wanted it included in your last commit. ```zsh # Modify commit message as well git commit --amend ``` ```zsh # Keep the commit message git commit --amend --no-edit ``` {: .note} If you already pushed the commit to a remote before the `ammend`, then you need to force push the new changes by `git push -f origin master`. ",
    "url": "/docs/git/git.html",
    "relUrl": "/docs/git/git.html"
  },"10": {
    "doc": "GitHub",
    "title": "GitHub",
    "content": "# GitHub {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Use multiple GitHub accounts with SSH First navigate to `~/.ssh`. For organization, create a directory and name it `github`. All private and public keys for GitHub connection will be placed here. --- ### Generate a new SSH key Follow the `ssh-keygen` prompt. It will ask you to decide on a name for the file, passphrase, etc. ```zsh # If Ed25519 algorithm is supported ssh-keygen -t ed25519 -C \"your_github@email.com\" ``` ```zsh # Legacy RSA ssh-keygen -t rsa -b 4096 -C \"your_github@email.com\" ``` --- ### Add the new SSH key to GitHub account Easiest part. Refer to [GitHub documentation][addssh] for step-by-step screencaps. --- ### Modify config Suppose I have two GitHub accounts each associated with `personal_email@address.com` and `work_email@address.com`. I'll assume the private keys are named `github-personal` and `github-work` respectively. Now append the following to `~/.ssh/config` ``` Host github-personal HostName github.com User git IdentityFile ~/.ssh/github/github-personal Host github-work HostName github.com User git IdentityFile ~/.ssh/github/github-work ``` You can define custom host for both as such or have one of them keep the default `github.com`. --- ### Local config per repository First start a local repo with ```zsh git init ``` Then config local name and email that will be used for that repo ```zsh git config --local user.name \"work_name\" git config --local user.email \"work_email@address.com\" ``` --- ### Add remote Normally you would add an ssh remote by ```zsh git remote add origin git@github.com:github_username:repo_name # OR git remote add origin github.com:github_username:repo_name ``` But this time, ```zsh git remote add origin github-work:work_username:repo_name ``` --- References: - [GitHub: SSH](https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent) [addssh]: https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account ",
    "url": "/docs/git/github.html",
    "relUrl": "/docs/git/github.html"
  },"11": {
    "doc": "Terraform",
    "title": "Terraform",
    "content": "# Terraform ## What is Terraform? **Infrastructure as Code (IaC)**: Terraform is a software tool that codes the infrastructure with a declarative configuration language. Your entire infrastructure is managed through a set of declarations. The benefit of IaC is that everything is collected within a single tool. This gets rid of the pain of having to jump to different tools every time you want to configure your resources. ",
    "url": "/docs/terraform/",
    "relUrl": "/docs/terraform/"
  },"12": {
    "doc": "Docker",
    "title": "Docker",
    "content": "# Docker ## Explained in a really dumb way You build an image that contains all the resources that compose a project. This packaging makes porting really easy because all the resources that made your project run at one time is now completely captured in it. You could think of this as a snapshot of your project. This image can be run in a docker container. A container is basically a process isolated from your computer. Think of it as a mini sandbox that mimics your system. Inside a container resources will be downloaded, installed, and copied just as you would normally, but whatever that happened during a container execution will not meddle with your actual computer (unless you specifically configure it to). ",
    "url": "/docs/docker/",
    "relUrl": "/docs/docker/"
  },"13": {
    "doc": "Git/GitHub",
    "title": "Git/GitHub",
    "content": "# Git/GitHub ## You probably already know what this is. Version control system; awesome stuff. ",
    "url": "/docs/git/",
    "relUrl": "/docs/git/"
  },"14": {
    "doc": "Home",
    "title": "Home",
    "content": "# Online Long-Term Memory of a Novice Programmer {: .fs-8 } Personal documentation of itty bitties and all the hacky decisions I've made throughout my learning (and maybe life). {: .fs-5 .fw-300 } --- ## Intro ### Why? Learning is always fun; I love jamming new things into my head. However, I've noticed that my long-term memory is in fact not long enough to guide me back after a while. Hence, the docs: I can't do anything about the things that have already left my head, but I am hoping that I can at least keep an itty bitty documentation ofmy future learnings. ### Disclaimer The information contained in this document is not necessarily correct or comprehensive. It will be biased in many ways and may contain naive and pitiful approaches made by a novice. Its sole purpose is to document my footsteps. ",
    "url": "/",
    "relUrl": "/"
  },"15": {
    "doc": "Flask",
    "title": "Flask",
    "content": "# Flask ",
    "url": "/docs/flask/",
    "relUrl": "/docs/flask/"
  },"16": {
    "doc": "Vue",
    "title": "Vue",
    "content": "# Vue ",
    "url": "/docs/vue/",
    "relUrl": "/docs/vue/"
  },"17": {
    "doc": "Jekyll",
    "title": "Jekyll",
    "content": "# Jekyll {: .no_toc } And GitHub Pages {: .text-delta } 1. TOC {:toc} --- ## Ruby installation with rbenv I've decided to use `rbenv` only because I didn't want to mess with the system `ruby` that comes with OSX (I am currently using Catalina). Assuming you have [Homebrew](https://brew.sh/) installed. ```bash # Install rbenv and ruby-build brew install rbenv # Set up rbenv integration with your shell rbenv init # Then follow the instruction that appears on screen ``` ```zsh # rbenv init will ask you to add the following to .zshrc eval \"$(rbenv init -)\" ``` Now that you have installed `rbenv`, create a folder that will contain your Jekyll site. I will refer to the folder as `blog`. Once created, move into `blog`. ```zsh cd blog # List latest stable versions rbenv install -l # I chose 3.0.0 rbenv install 3.0.0 rbenv rehash # Following creates .ruby-version in cwd rbenv local 3.0.0 # Confirm ruby version in folder ruby -v ``` All the `ruby` versions are installed in `~/.rbenv`. --- ## Install Jekyll Before installing the gems, check where they are being installed via ```zsh # Refer to INSTALLATION DIRECTORY / GEM PATHS gem env # OR gem env home ``` Now, the Jekyll documentation tells you to do a local install with the `--user-install` flag. If you're not using `rbenv` this is indeed more desirable as it does not require `sudo`. However, with `rbenv` it was unnecessary for my purpose. As you'll notice by inspecting the `gem env` outputs, it will install the gems outside of the `~/.rbenv` directory to some local folder (`USER INSTALLATION DIRECTORY`). It still works, but you have to add it to `PATH` to execute the gems. With `rbenv`, \"global\" (it really isn't global anymore) installation is more convenient because it installs them in `~/.rbenv` (so no `sudo` and additional path config required). ```zsh gem install jekyll bundler ``` --- ## Create a Jekyll blog First create a new Jekyll project by ```zsh # Assuming you're still in the blog folder jekyll new . ``` It will create a default website you can test locally. ```zsh # Will generate a static html site in _site bundle exec jekyll serve # With live-reloading bundle exec jekyll serve --livereload ``` {: .note} If you get any errors regarding `webrick`: `cannot load such file -- webrick (LoadError)`, add `webrick` by `bundle add webrick`. This is due to `ruby 3.0.0` excluding `webrick` as a default bundled gem. --- ## Bundle Clean ```zsh bundle clean [--dry-run] [--force] ``` ",
    "url": "/docs/others/jekyll/",
    "relUrl": "/docs/others/jekyll/"
  },"18": {
    "doc": "Pipenv",
    "title": "Pipenv",
    "content": "# Pipenv {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Advantage to `venv` `Pipenv` serves as an upgrade version of `requirements.txt`. It allows you to separately mark production and development dependencies. --- ## Basic usage ### Install ```zsh brew install pipenv ``` ### Create environment and install Go to the desired project folder. To create and use a virtual environment for this root: ```zsh pipenv pipenv install # Install specific package ``` If there's already a `Pipfile`, create env and install listed requirements: ```zsh pipenv install # With Pipfile ``` {: .note} All the `pipenv` environments are in `~/.local/share/virtualenvs/root-dir-name-hash/` by default. ### Activate and deactivate environment #### Activate ```zsh pipenv shell ``` #### Deactivate ```zsh exit ``` {: .note} Do not use `deactivate`. ### Create `Pipfile.lock` ```zsh pipenv lock ``` ### Delete environment To delete the environment for current directory: ```zsh pipenv --rm ``` --- ## Use pipenv with conda (Optional) You can set a specific version of Python when creating a pipenv virtual environment. ```zsh pipenv --python 3.x install ``` However, the way this works is that it requires that Python 3.x is already installed on your local machine unlike `conda create -n myenv python=3.x`. You can `brew install` the desired Python 3.x, but if you don't want to clog up your `/usr/local/bin` with different versions of Python binaries, you can instead create a conda environment with the desired version: ```zsh conda create -n myenv3-x python=3.x conda activate myenv3-x # Activate which python # Confirm '~/miniconda3/envs/myenv3-x/bin/python' ``` Then navigate to the root of the project and make pipenv use the 3.x binary installed in conda: ```zsh pipenv --python=$(which python) --site-packages # Creates an env in cwd pipenv run which python # It will point to a binary in `~/.local/share/virtualenvs/some-root-dir-hash/bin/python` pipenv run python -V # Check that it is indeed 3.x ``` When you no longer need the 3.x version, you can delete the conda env along with the pipenv. ```zsh pipenv --rm conda env remove -n myenv3-x ``` {: .note} I prefer this method because it feels like I have a better control and I dislike having all the `python@x.x` binaries that sit in `/usr/local/bin`. --- ## Some basic Python sanity checks - `which python` / `which python3` will point to the python binary - `which pip` / `which pip3` will point the pip binary - `pip -V` / `pip3 -V` will point to the `site-packages` - `conda run which python` / `conda run python -V` does the expected for the base conda env or the active env - `pipenv run which python` / `pipenv run python -V` does the expected for the current root directory env - However, `pipenv run pip -V` will create an env for the cwd and add pip to the `Pipfile` for cwd --- ",
    "url": "/docs/others/pipenv/",
    "relUrl": "/docs/others/pipenv/"
  },"19": {
    "doc": "Conda",
    "title": "Conda",
    "content": "# Basic Conda {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Advantage to `venv` `venv` is simple and is packaged with python by default. If your environment only needs to care about the python packages installed and their versions, `venv` is a good option. However, `venv` environments have a fixed python binary version that matches the python version used to invoke `venv`. So for example, ```zsh python3.6 -m venv venv/myenv ``` Then every environment created has the python version of 3.6. But `conda` makes using different python versions in different environments possible. --- ## Install Conda To be added. If you don't want the base environment activated all the time ```zsh conda config --set auto_activate_base false ``` --- ## Create environment Simplest method is ```zsh conda create -n myenv ``` To use a specific Python version ```zsh conda create --name myenv python=3.8 ``` Created environments are located in `~/anaconda3/env` or `~/miniconda3/env`. {: .note} If you installed `conda` via GUI installer, the `conda` folder may be in `/opt`. Confirm environment creation via ```zsh conda env list # OR conda info --envs ``` --- ## Activate / Deactivate ```zsh conda activate myenv ``` ```zsh conda deactivate ``` --- ## Install packages To install packages in current active environment, ```zsh conda install pkg-name # OR for a specific version conda install pkg-name=1.0.0 ``` To install packages in some other environment, ```zsh conda install pkg-name -n myenv ``` --- ## List and export dependencies ```zsh conda list ``` To export dependencies (like `pip3 freeze > requirements.txt`), ```zsh conda list --export > requirements.txt ``` To duplicate an environment using the list (like `pip3 install -r requirements.txt`), ```zsh conda create -n myenv --file requirements.txt ``` --- References: - [Conda: Managing Environments](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) - [Conda: Install Packages](https://docs.anaconda.com/anaconda/user-guide/tasks/install-packages/) ",
    "url": "/docs/others/conda/",
    "relUrl": "/docs/others/conda/"
  },"20": {
    "doc": "MongoDB",
    "title": "MongoDB",
    "content": "# MongoDB {: .no_toc } On-prem community edition {: .text-delta } 1. TOC {:toc} --- ## Install MongoDB (locally) ```zsh brew tap mongodb/brew brew install mongodb-community@4.4 ``` This installs - `mongod` server - `mongos` sharded cluster query router - `mongo` shell And also - `/usr/local/etc/mongod.conf` configuration file - `/usr/local/var/log/mongodb` log directory - `/usr/local/var/mongodb` data directory And finally [MongoDB Database Tools](https://docs.mongodb.com/database-tools/) {: .note} Location varies by system. Check with `brew --prefix`. --- ## Run and stop MongoDB (locally) Run MongoDB as a macOS service (recommended) ```zsh brew services start mongodb-community@4.4 # Verify it is running (should be in started status) brew service list | grep mongodb-community ``` You can then use the mongo shell via ```zsh mongo ``` Stop MongoDB ```zsh brew services stop mongodb-community@4.4 ``` --- References: - [MongoDB: Install](https://docs.mongodb.com/manual/tutorial/install-mongodb-on-os-x/) ",
    "url": "/docs/others/mongodb/",
    "relUrl": "/docs/others/mongodb/"
  },"21": {
    "doc": "Poetry",
    "title": "Poetry",
    "content": "# Poetry Yet another Python virtual environment & package manager! {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Installation Fetch and install the script: ```zsh curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py python3 install-poetry.py ``` Check installation via ```zsh poetry --version ``` {: .note} In my case, poetry was installed in `~/Library/Python/3.9/bin`. So I had to add `export PATH=~/Library/Python/3.9/bin:$PATH` to my shell config. ### Uninstall To uninstall: ```zsh python3 install-poetry.py --uninstall # OR POETRY_UNINSTALL=1 python3 install-poetry.py ``` --- ## Enable tab completion You can enable `poetry` tab completion for various shells. Check `poetry help completions` for other shells. For `Oh-My-Zsh`: ```zsh mkdir $ZSH_CUSTOM/plugins/poetry poetry completions zsh > $ZSH_CUSTOM/plugins/poetry/_poetry ``` Then go to `~/.zshrc` and add the following plugin: ```zsh # ~/.zshrc plugins( ... poetry ) ``` --- ## Using Poetry a specific Python version First init a Poetry project to create a `pyproject.toml` file. ```zsh cd poetry init ``` Then reate a conda environment with a Python version of your choice: ```zsh conda create -n python=3.x conda activate poetry env use `which python` ``` {: .note} Poetry virtual environments are created in `~/Library/Caches/pypoetry/virtualenvs`. --- ## Managing environments ### See all virtual envs associated with this directory/project ```zsh poetry env list ``` ### Delete environments ```zsh poetry env remove ## Check exact name with poetry env list ``` --- ## Basic usage ### Activate environment ```zsh poetry shell # Creates a new child shell # OR source {entire_long_path_to_env}/bin/activate # Does not open a child shell ``` ### Deactivate environment ```zsh exit # If in child shell # OR deactivate # If activated with source /bin/activate ``` ### Add dependencies ```zsh poetry add # OR poetry add --dev ``` ### Install dependencies ```zsh poetry install # OR poetry install --no-dev ``` ### Remove dependencies ```zsh poetry remove # OR poetry remove --dev ``` --- References: - [Poetry][poetry-install] - [Poetry Commands][poetry-commands] [poetry-install]: https://python-poetry.org/docs/master/#installation [poetry-commands]: https://python-poetry.org/docs/cli/ ",
    "url": "/docs/others/poetry/",
    "relUrl": "/docs/others/poetry/"
  },"22": {
    "doc": "Others",
    "title": "Others",
    "content": "# List of All Documentations ",
    "url": "/docs/others/",
    "relUrl": "/docs/others/"
  },"23": {
    "doc": "Demo",
    "title": "Demo",
    "content": "# Demo ",
    "url": "/docs/demo/",
    "relUrl": "/docs/demo/"
  },"24": {
    "doc": "OAuth 2.0",
    "title": "OAuth 2.0",
    "content": "# OAuth 2.0 {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## What is an OAuth 2.0 protocol? According to Google, it is an 'open standard for access delegation'. While it sounds intimidating, it is essentially made to 'let this application access my Google photos', 'let this site use my Facebook contacts', etc. So it was developed as method for **authorization** to a 3rd party resource. Some terms: - Resource owner: that's the user (you) wanting to grant access - Resource server: the API you want to access - Client: application requesting access - User Agent: the thing user is using to talk to client (browser, mobile app) - Authorization server: authorizes and grants access tokens to client ### OpenID One thing to note is the word authorization, and you shoud not to confuse it with authentication. When I first read about OAuth, I thought, \"Well isn't this the 'Sign in with Google/Facebook' button that I see quite a lot on websites these days?\". It sort of is, because the protocol behind that button is **OpenID** which is built on top of OAuth 2.0. So the way they operate are very similar, but it is good to know the difference that OAuth is for authorization and OpenID builds a layer on top of OAuth for authentication. --- ## Client types There are two different types of clients in OAuth. One is a public client and the other is a confidential client. To understand the difference, you need to know the term **client secret**. ### Client secret A client secret is nothing more than a random string generated. It is usually created by generating a secure random string of 256-bit (32 bytes) and then converting it to hex. This value should never be revealed to the outside except for the authorizing server and the client app. Hence the name 'client secret'. Inside your code, client secret will be used to successfully authorize users. But the issue that arises is where should the client store this secret. ### Public clients If the client cannot keep the client secret a *secret*, it is called a public client. For example, single-page apps that expose everything on the browser with no backend or mobile apps that can have their HTTPS request intercepted and revealed are considered **public clients**. In case of an SPA, everything is exposed on the browser. Chrome inspect will reveal the source code, local storage, session, and cookies. So storing client secret is infeasible. For a mobile app, apparently it is possible to provide a fake HTTPS certificate that goes to your own API. So you can catch an HTTPS leaving the phone, route it to a different API, have that API make a request to the initial intended API, and return the response to phone as if it would normally, while the proxy API in the middle can inspect all the requests (which may contain the client secret at some point). ### Confidential clients This is typically a traditional web server or anything backed by a server where nobody can take a peek at the source code or have the requests intercepted. --- ## Authorization flow There are a few different flows, but I will only document three of them: implicit flow, authorization code flow, authorization code flow with PKCE. The general process is as below: 1. Client sends request to autorization server 2. Client gets an authorization code back 3. Client sends request to a token endpoint 4. Client gets an access token 5. Client places this token in a header when sending a request to resource server ### Implicit flow Implicit flow is much more simplified. After step 1, implicit flow skips right to step 4. Because the access token is revealed on the browser url, this is considered an insecure lecay method. ### Authorization code flow Client gets an authorization code back as a request parameter embedded in the url. The client then uses this code to exchange it for an access token. Usually secure random strings such as state and client secret are used to validate the process. ### Authorization code flow with PKCE For public clients that cannot keep any secret strings, PKCE (Proof Key for Code Exchange) is implemented. This step includes an additional code challenge and verifying step. --- ## Authorization server API Typically there are two endpoints during the process. ### Authorize Typical request is an HTTPS GET to a path that often looks like `oauth/authorize`. Parameters - `response_type`: `code` for authorization code flow and `token` for implicit flow - `client_id`: client app id - `redirect_uri`: absolute uri to be redirected after authorization - `state`: a random value that will be returned back in redirect. This is a protection against CSRF. - `scope`: the scope of resources you want to protect - `code_challenge_method` (PKCE only): the encryption used in code challenge; typically S256 for SHA256 - `code_challenge` (PKCE only): the generated challenge from `code_verifier` ### Token After extracting the authorization code from the redirect url, you make an HTTPS POST request to `oauth/token`. Header: - `Authorization`: `Basic Base64_url_encode('client_id:client_secret')` - `Content-Type`: `application/x-www-form-urlencoded` Body: - `grant_type`: `authorization_code`, `refresh_token`, `client_credentials` - `client_id` - `redirect_uri`: should be the same as the one used for authorization request - `scope` - `code`: extracted from url - `code_verifier`: proof key for the code_challenge Response: ```json { \"id_token\": \"~\", \"access_token\": \"~\", \"refresh_token\": \"~\", \"token_type\": \"Bearer\", \"expires_in\": 10000 } ``` --- ## PKCE Code Challenge ### Code verifier According to [here][oauthpkce] it is a 'cryptographically random string using the characters A-Z, a-z, 0-9, and the punctuation characters -._~ (hyphen, period, underscore, and tilde), between 43 and 128 characters long'. ### Code challenge Code challenge is created by hashing the `code_verifier` with SHA256 and then encoding as a BASE6-URL string. --- References: - [OAuth: PKCE][oauthpkce] - [AWS Cognito: AUTHORIZE](https://docs.aws.amazon.com/cognito/latest/developerguide/authorization-endpoint.html) - [AWS Cognito: TOKEN](https://docs.aws.amazon.com/cognito/latest/developerguide/token-endpoint.html) - [Auth0: PKCE](https://auth0.com/docs/flows/add-login-using-the-authorization-code-flow-with-pkce) [oauthpkce]: https://www.oauth.com/oauth2-servers/pkce/authorization-request/ ",
    "url": "/docs/learned/oauth2/",
    "relUrl": "/docs/learned/oauth2/"
  },"25": {
    "doc": "AWS",
    "title": "AWS",
    "content": "# AWS ",
    "url": "/docs/aws/",
    "relUrl": "/docs/aws/"
  },"26": {
    "doc": "Things I Learned",
    "title": "Things I Learned",
    "content": "# Things I Learned List of itty bitty things that I want to keep a note of, but couldn't quite find a category to place yet. Contents listed here may be moved or grouped with other pages if more related contents are produced. ",
    "url": "/docs/learned/",
    "relUrl": "/docs/learned/"
  },"27": {
    "doc": "Network",
    "title": "Network",
    "content": "# Network Basics {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## IP Address IP (Internet Protocol) address is a unique address that **identifies a device on a network** using an Internet Protocol. ### Private IP Address Reserved range for private networks #### IPv4: RFC1918 - [24-bit block] CIDR: `10.0.0.0/8`, Subnet mask: `255.0.0.0` - [20-bit block] CIDR: `172.16.0.0/12`, Subnet mask: `255.240.0.0` - [16-bit block] CIDR: `192.168.0.0/16`, Subnet mask: `255.255.0.0` --- ",
    "url": "/docs/learned/network/",
    "relUrl": "/docs/learned/network/"
  },"28": {
    "doc": "Vue Project Setup",
    "title": "Vue Project Setup",
    "content": "# Vue Project Setup {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Start project directory Details are listed [here][viteinit]. ```zsh yarn create vite --template vue-ts # Init project cd yarn # Install packages yarn dev # Check build ``` --- ## Install Tailwind CSS Details are listed [here][tailwindinit]. But for the brief summary: ```zsh yarn add tailwindcss@latest postcss@latest autoprefixer@latest --dev npx tailwindcss init -p ``` `npx tailwind init -p` generates two files `tailwind.config.js` and `postcss.config.js`. ### Remove unused styles in production builds In `tailwind.config.js`, replace the `purge` to following line, ```js purge: ['./index.html', './src/**/*.{vue,js,ts,jsx,tsx}'] ``` ### Include Tailwind CSS Create a file `src/index.css`. Then add the following to the file, ```css @tailwind base; @tailwind components; @tailwind utilities; ``` Then in `src/main.js`, import `src/index.css`. ```js import { createApp } from 'vue' import App from './App.vue' import './index.css' createApp(App).mount('#app') ``` --- ## Add path alias Unlike webpack, Vite does not automatically provide the `@` path alias to `src`. To enable this alias go to `vite.config.ts` and `import path from 'path'`. {: .note} If `import path from 'path'` shows a type warning: `yarn add @types/node --dev`. Then add the following to `defineConfig` in `vite.config.ts`: ```ts // vite.config.ts resolve:{ alias: [ { find: '@', replacement: path.resolve(__dirname, './src') } ] } ``` --- ## Desktop App with Electron (Optional) Really nice detail in [this blog][electron]. {: .note} Don't forget to `yarn add concurrently cross-end wait-on electron-buider --dev`. They are needed to run the `package.json` scripts. --- ## Install ESLint and Prettier (Optional) ```zsh yarn add eslint prettier eslint-plugin-vue eslint-config-prettier --dev ``` Then create two files `.eslintrc.js` and `.prettierrc.js` in the project root directory, ```js // .eslintrc.js module.exports = { extends: [ 'plugin:vue/vue3-essential', 'prettier', ], rules: { // override/add rules settings here, such as: 'vue/no-unused-vars': 'error', }, } ``` ```js // .prettierrc.js module.exports = { semi: false, tabWidth: 2, useTabs: false, printWidth: 80, endOfLine: 'auto', singleQuote: true, trailingComma: 'es5', bracketSpacing: true, arrowParens: 'always', } ``` [viteinit]: https://vitejs.dev/guide/#scaffolding-your-first-vite-project [tailwindinit]: https://tailwindcss.com/docs/guides/vue-3-vite [electron]: https://dev.to/brojenuel/vite-vue-3-electron-5h4o ",
    "url": "/docs/vue/init.html",
    "relUrl": "/docs/vue/init.html"
  },"29": {
    "doc": "JS/TS Cheatsheet",
    "title": "JS/TS Cheatsheet",
    "content": "# Javascript/Typescript Cheatsheet {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Installation I personally prefer to install typescript compiler per project directory. ```zsh npm i typescript --save-dev # or -D # OR yarn add typescript --dev ``` --- ## Typescript compiler Basic usage: ```zsh tsc # One-time compile tsc --watch # Livereloading ``` When specifiying the filename for `tsc` you may include or leave out the `.ts` extension. ```zsh tsc --init ``` `init` creates the `tsconfig.json` file. With a `tsconfig.json` file, you can leave out the `filename` when runnig `tsc`. --- ## Basic typing ### Primitive types Primitives are all lowercase. ```ts let a: number = 1; let b: string = \"a\"; let c: boolean = true; // Lowercase let d: any = { x: 0 }; ``` Unless declared without initialization, these are usually inferred. ### Array and tuple types Simply add brackets: ```ts let a: number[] = [1, 2, 3, 4]; // You can also use Array let b: [number, string] = [1, \"a\"]; let c: [number, string][] = [[1, \"a\"], [2, \"b\"]]; ``` ### Union and enum types Union: ```ts let id: string | number; ``` Enum: ```ts enum MyEnum { Up = 1, // Default is 0 Down, Left, Right } ``` The first constant in an enum always has the value of 0. If you set it to 1, the rest will have an ascending value of 2, 3, and 4. You can also give `string` values to enums. ### Object types and type alias Object typing without the `type` alias can be messy: ```ts const obj: { a: number, b?: string, readonly c: boolean, } = { a: 1, c: true } ``` Using `type`: ```ts type MyObj = { a: number, b?: string, readonly c: boolean }; const obj: MyObj = { a: 1, c: true } ``` {: .note} The `?` means it is an optional property or an optional parameter if used in functions. ### Function types ```ts function f(x: number, y: string): void { ... } ``` ### Literal types You can use this like a constant or quicky enum: ```ts function f(x: number, y: \"a\" | \"b\"): -1 | 0 | 1 { ... } ``` To change an object to a literal type use `as const`: ```ts const x = { a: \"hello\", b: \"world\" } as const ``` --- ## Type assertions To give `any` variables explicit types: ```ts let a: any = 1; let b = a as number // OR let c = a ``` --- ## Interface ### Object interface ```ts interface MyInterface { a: number, b?: string } ``` ### Function interface ```ts interface FuncInterface { (x: number, y: string): void } ``` While it is similar to `type` aliasing, there are some differences: - You cannot use union types with an interface. ```ts type MyType = string | number; // OK // interface MyType2 = string | number; // NO ``` - You can add new fields to existing interfaces but not in type aliasing. ```ts interface MyInterface{ a: number } interface MyInterface{ b: string } ``` --- ## Undefined values Use `null` or `undefined`. ```ts function f(x: number | null): void{ ... } ``` ### Non-null assertions (!) ```ts someObj!.runFunction(); ``` --- ## Index signature --- ## Generics --- ## Classes --- ## Readonly arrays and tuples --- ## Symbol type --- ## Computed property names --- ## Template strings ```ts let a = `Put ${variableName} here.` ``` [viteinit]: https://vitejs.dev/guide/#scaffolding-your-first-vite-project [tailwindinit]: https://tailwindcss.com/docs/guides/vue-3-vite ",
    "url": "/docs/vue/jsts.html",
    "relUrl": "/docs/vue/jsts.html"
  },"30": {
    "doc": "Docker Networks",
    "title": "Docker Networks",
    "content": "# Docker Networks {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Some useful commands ```zsh # List all networks docker network ls ``` ```zsh docker network inspect network-name ``` ```zsh # Disconnect any containers using this network docker network disconnect network-name my-container docker network rm network-name ``` ```zsh # Remove unused networks docker network prune ``` [Link to documentation][dockernetwork]. --- ## How do I talk to the container? When a container is created, none of the ports inside the container are exposed. In order for the Docker host (your computer) or other containers to talk to it, it must first publish a port. The following maps a port 1234 inside a container to 4321 on Docker host. ``` docker create -p 1234:4321 ``` Now you can communicate with the container via `http://localhost:4321`. --- ## How can containers talk to each other? If the containers are running on the **same Docker daemon host** (ie. all running on your computer), then the easiest way is to put them on the same bridge network. --- ### Default bridge network Check the existing docker networks with ```zsh docker network ls ``` You will see a network with the name `bridge`. That is the default bridge network. Every started container is **automatically added** to the default bridge network if you didn't specify anything else. With the default bridge you talk to other containers by using their **IP Address**. ```zsh docker inspect my-container | grep IPAddress ``` **Downsides** to using the default bridge network: - Using IP address sucks (*at least in my opinion*): it is not immediate which container I'm referring to. - Every container can talk to every other container, which may cause security issues. --- ### User-defined bridge network You can add a user-defined bridge network. It still uses the same `bridge` driver, but unlike the default bridge not everyone is invited to it. #### Docker-compose ```yaml services: my-container: image: some-db-image networks: - backend networks: backend: driver: bridge ``` #### Or via Terminal ```zsh docker network create my-bridge # You can add after container creation docker network connect my-bridge my-container # Or when you create it docker create --network my-bridge ``` In user-defined bridge network, you can talk to the container using the container name as hostname. So if my container was named `my-db` with port published at `1234`, then url would be ``` http://my-db:1234 ``` --- References: - [Docker Networking](https://docs.docker.com/network/) [dockernetwork]: https://docs.docker.com/engine/reference/commandline/network/ ",
    "url": "/docs/docker/networks.html",
    "relUrl": "/docs/docker/networks.html"
  },"31": {
    "doc": "Vue Quick Notes",
    "title": "Vue Quick Notes",
    "content": "# Vue Quick Notes Quick notes for dummies. Using ` ``` ```vue // Child.vue ``` --- ## Provide / Inject 1. To make typing work, you gotta use the `InjectionKey`. See [here][injectionkey] for details. 2. To update provided reactive props, make the parent component provide mutation functions as well. Always recommended to have the root (providing) component to be in charge of mutations. --- ## Etc 1. You can use `$event`, `$router`, `$route`, `$slots`, `$attrs`, `$emit` in the template tag, but not in the script tag. 2. `ref`, `reactive`, `toRef`, `toRefs`, `computed`, `watch`, `watchEffect` are all in `vue` 3. `attrs` are basically all the stuff passed down to a child naturally from being an HTML element, but not actually a Vue prop. Ex) `class` 4. You can use the normal `` tag along with the ``. Two things you'll have to do within the normal `` tag is setting `name` and `inheritAttrs`. ### `$keyword` equivalent in the script tag See [here][scriptsetup] for details. But basically: ```ts import { useSlots, useAttrs } from 'vue import { useRouter, useRoute } from 'vue-router' const slots = useSlots() const attrs = useAttrs() const router = useRouter() const route = useRoute() ``` [vmodel]: https://v3.vuejs.org/guide/migration/v-model.html#_3-x-syntax [scriptsetup]: https://v3.vuejs.org/api/sfc-script-setup.html [compwatch]: https://v3.vuejs.org/guide/reactivity-computed-watchers.html [injectionkey]: https://v3.vuejs.org/api/composition-api.html#provide-inject ",
    "url": "/docs/vue/quick-notes.html",
    "relUrl": "/docs/vue/quick-notes.html"
  },"32": {
    "doc": "Terraform Configuration",
    "title": "Terraform Configuration",
    "content": "# Terraform Configuration {: .no_toc } With AWS (As of now) {: .text-delta } 1. TOC {:toc} --- ## Terraform Block It contains the Terraform settings and has the basic structure of the following ```tf terraform { required_providers { mylocalname = { source = \"source/address\" version = \"~> 1.0\" } } required_version = \">= 0.14.9\" } ``` Throughout the module, Terraform refers to providers using a **local name**. Here I've given it a name of `mylocalname`. Source address takes the form of `[Hostname/]Namespace/Type`. If `Hostname` is ommitted, it defaults to `registry.terraform.io` which is Terraform's default provider install source. `hashicorp/aws` is a shorthand for `registry.terraform.io/hashicorp/aws`. For the version constraint syntax, refer to [Version Constraint Syntax](https://www.terraform.io/docs/language/expressions/version-constraints.html). --- ## Provider You can configure each provider using the local name you have provided in the `required_providers` of the Terraform block. For example, ```tf provider \"mylocalname\" { # ... } ``` Reference [Provider Configuration](https://www.terraform.io/docs/language/providers/configuration.html) for details. --- ## Resource Basic syntax is as follows, ```tf resource \"aws_instance\" \"my_server\" { ami = \"ami-a1b2c3d4\" instance_type = \"t2.micro\" } ``` The example block above declares a **resource type** `\"aws_instance\"` and gives it a **local name** of `\"my_server\"`. Just like the provider local name, resource local name is used to refer to this resource throughout the module. In addition, the **unique ID** for the resource becomes `aws_instance.my_server`. The resource configuration arguments within the block body are specific to each resource type. For example, refer to documentation [here](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance) for `aws_instance`. --- ## Using variables To avoid using hard-coded values in configuration, create a new file `variables.tf` (*name of the file can be anything you want*) with the following, ```tf variable \"variable_name\" { description = \"Some description of what this is\" type = string default = \"This is the value of the variable\" } ``` You can then use the variables in other `.tf` files as, ```tf var.variable_name ``` You can also pass in a new variable value for testing by ```zsh terraform apply -var 'variable_name=SomeOtherValue' ``` It will modify the state so that all the variables use the new value. {: .note} This does not update the original variable declaration. If you run `terraform apply` again without the `-var` flag, the state will be modified using the original value. --- References: - [Terraform: Terraform Settings](https://www.terraform.io/docs/language/settings/index.html) - [Terraform: Providers](https://www.terraform.io/docs/language/providers/index.html) - [Terraform: Resources](https://www.terraform.io/docs/language/resources/index.html) - [Terraform: Version Constraint Syntax](https://www.terraform.io/docs/language/expressions/version-constraints.html) --- ",
    "url": "/docs/terraform/terraform-config.html",
    "relUrl": "/docs/terraform/terraform-config.html"
  },"33": {
    "doc": "Terraform Module",
    "title": "Terraform Module",
    "content": "# Terraform Module {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## What is a Terraform module It is sort of like a class in programming. Given input variables, it can be reused to create multiple instances of the infra described in the module. For example, when you're creating an AWS lambda resource, there are typically some other resources associated with it, such as the REST API, IAM role, etc. If you think you're going to be using this pattern often, you can create a module containing all the common resources and only expose some input variables that need to be configured at the top level. --- ## Typical file structure The main Terraform execution point is called the root module. This is often the `main.tf` file in the top level driectory. Modules are often placed in a folder called `modules`. ```zsh . ├── README.md ├── main.tf ├── modules ├── outputs.tf └── variables.tf ``` --- ## Module structure Inside `modules` directory, create a child directory with a name of your module: e.g. `mymodule`. ```zsh . ├── main.tf ├── modules │   └── mymodule │      ├── main.tf │      ├── outputs.tf │      └── variables.tf ├── outputs.tf └── variables.tf ``` The structure inside `mymodule` is optional. They can be separated as above or smashed into a single file. --- ## Module input variables Unless you plan to reuse your module as-is every single time, you typically provide input variables to modules. Any variables defined with `variable` must be provided by the calling module or an error will be raised. --- ## How to call a module All you need to do is feed the necessary input variables. In `main.tf`, ```tf # main.tf module \"module_a\" { source = \"./modules/mymodule\" my_input_var = \"Here you go\" } ``` Notice that the `source` attribute points the the directory of the module, not any specific files If you defined any output variables in the module with `output`, you can access them by ```tf module.module_a.my_output_var ``` --- ## Itty Bitties - Although you can nest your modules in multiple levels, it is recommended to keep the entire Terraform module as flat as possible. - Even if you don't access them, module `output` variables is always output after `terraform apply`. - Think about whether a module is absolutely necessary. Sometimes you may end up feeding in as many input variables as the original resource. ",
    "url": "/docs/terraform/terraform-module.html",
    "relUrl": "/docs/terraform/terraform-module.html"
  },"34": {
    "doc": "Provision with Terraform",
    "title": "Provision with Terraform",
    "content": "# Provision with Terraform {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## Configuration Folder structure ```zsh flask-mongodb ├── backend │   ├── .gitignore │   ├── .dockerignore │   ├── app.py │   ├── back.dev.Dockerfile │   ├── requirements.txt │   └── venv │   └── flaskmongo └── terraform ├── main.tf └── providers.tf ``` We are going to be using a docker provider. ```tf # flask-mongodb/terraform/main.tf terraform { required_providers { docker = { source = \"kreuzwerker/docker\" version = \"~> 2.11.0\" } } required_version = \"~> 0.15.3\" } ``` ```tf # flask-mongodb/terraform/providers.tf provider \"docker\" { host = \"unix:///var/run/docker.sock\" } resource \"docker_container\" \"backend_tf\" { name = \"backend-tf\" image = docker_image.flask_back.latest volumes { container_path = \"/www\" host_path = \"/full/path/to/flask-mongodb/backend\" read_only = true } ports { internal = 5000 external = 5000 } } resource \"docker_image\" \"flask_back\" { name = \"flask-back:latest\" build { path = \"../backend\" dockerfile = \"back.dev.Dockerfile\" force_remove = true } } ``` Now initialize to download and install providers in `.terraform` and apply to create ```zsh terraform init terraform apply --auto-approve ``` To verify that docker image has been built and container is running ```zsh docker images | grep flask-back docker ps | grep backend-tf ``` Because `volume` is mounted, any change in directory `backend` will be reflected in the container. To destroy all resources created ```zsh terraform destroy ``` ",
    "url": "/docs/demo/flask-login-app/terraform.html",
    "relUrl": "/docs/demo/flask-login-app/terraform.html"
  },"35": {
    "doc": "Docker Volumes",
    "title": "Docker Volumes",
    "content": "# Docker Volumes {: .no_toc } {: .text-delta } 1. TOC {:toc} --- ## What is a Docker volume In short, it maps the volume inside a container with some local directory on your computer. If you set the volume to read-only, every change in local directory will be reflected in the container but not vice versa. If you set read-only to false, the contents inside the container and the local directory will remain same throughout the container execution. --- ## Cases where Docker volume comes in handy ### You want some live-reloading features During development, it is really painful if you have to rebuild or rerun everytime you make a change. By mapping your container volume to a local build context, the container will update itself everytime you make a change to your local code. ### You want to persist data upon container shutdown For example if you're running a DB as a container without a volume mapped, each time the container restarts, the contents of the DB will be erased. However, if you map your container volume to a local directory, the data will be kept even after shutdown. --- ",
    "url": "/docs/docker/volumes.html",
    "relUrl": "/docs/docker/volumes.html"
  }
}
