{"0": {
    "doc": "GitHub Actions",
    "title": "GitHub Actions",
    "content": ". | Add a workflow | Basic workflow syntax | Repository secrets | Self-hosted runner | Add a self-hosted runner | Run self-hosted runner . | To run as a service | . | . ",
    "url": "/docs/git-hub/github/actions.html",
    "relUrl": "/docs/git-hub/github/actions.html"
  },"1": {
    "doc": "GitHub Actions",
    "title": "Add a workflow",
    "content": "Navigate to a GitHub repo. Go to Actions and click on New workflow. You can either create a new workflow from scratch or use a template recommended for your project. You will have to commit your workflow yaml to the main branch. ",
    "url": "/docs/git-hub/github/actions.html#add-a-workflow",
    "relUrl": "/docs/git-hub/github/actions.html#add-a-workflow"
  },"2": {
    "doc": "GitHub Actions",
    "title": "Basic workflow syntax",
    "content": "See details here. Example: . name: Workflow Name on: push: branches: - main pull_request: branches: - main jobs: my-job: name: Job Name timeout-minutes: 10 runs-on: [self-hosted, macOS, X64] env: ENV_NAME: ${{ secrets.MyEnv }} steps: - uses: actions/checkout@v2 - name: Step Name run: | echo $ENV_NAME . ",
    "url": "/docs/git-hub/github/actions.html#basic-workflow-syntax",
    "relUrl": "/docs/git-hub/github/actions.html#basic-workflow-syntax"
  },"3": {
    "doc": "GitHub Actions",
    "title": "Repository secrets",
    "content": "In order to prevent sensitive environment variables from being committed with the workflow file, you can use Actions secrets. Navigate to Settings -&gt; Secrets: Actions . Click on New repository secret. Naming for secrets: . | Must not start with GITHUB_ prefix | Must not start with numbers | Must be alphanumeric + underscores (a-z, A-Z, 0-9, _) | Are not case sensitive | . Created secrets can then be used in workflow files as . ${{ secrets.MySecretName }} # Since secrets are not case sensitive, you could've just used # ${{ secrets.mysecretname }} as well. ",
    "url": "/docs/git-hub/github/actions.html#repository-secrets",
    "relUrl": "/docs/git-hub/github/actions.html#repository-secrets"
  },"4": {
    "doc": "GitHub Actions",
    "title": "Self-hosted runner",
    "content": "By default, GitHub Action Runners are machines managed by the GitHub. However, because you are borrowing a shared resource, your workflow may take a longer time to execute due to the wait time. Or you may be wanting to use GitHub Actions to automate on-prem deployment. To solve any one of these issues, you can add your own machine as a self-hosted runner on GitHub. See details here. ",
    "url": "/docs/git-hub/github/actions.html#self-hosted-runner",
    "relUrl": "/docs/git-hub/github/actions.html#self-hosted-runner"
  },"5": {
    "doc": "GitHub Actions",
    "title": "Add a self-hosted runner",
    "content": "Navigate to a GitHub repo. Go to Settings -&gt; Actions: Runners. Click on New self-hosted runner and follow the instructions. While running ./config.sh --url &lt;repo&gt; --token &lt;token&gt;, you will be asked to configure a label. This label is used to identify a specific runner, in the case you have multiple self-hosted runners. This value can be changed later in GitHub. ",
    "url": "/docs/git-hub/github/actions.html#add-a-self-hosted-runner",
    "relUrl": "/docs/git-hub/github/actions.html#add-a-self-hosted-runner"
  },"6": {
    "doc": "GitHub Actions",
    "title": "Run self-hosted runner",
    "content": "The simplest way to have the runner listening for jobs is to ./run.sh . To run as a service . To have the runner listening as a background job and have it restart itself upon machine failure, install it as a service and start it. sudo ./svc.sh install sudo ./svc.sh start sudo ./svc.sh stop sudo ./svc.sh status sudo ./svc.sh uninstall . To see this usage do: . sudo ./svc.sh . ",
    "url": "/docs/git-hub/github/actions.html#run-self-hosted-runner",
    "relUrl": "/docs/git-hub/github/actions.html#run-self-hosted-runner"
  },"7": {
    "doc": "2 - Add Two Numbers - Medium",
    "title": "Add Two Numbers",
    "content": ". | Problem | Solution . | Simultaneous traversal | Which linked list is longer? | Leftover carry | . | . ",
    "url": "/docs/compsci/leetcode/add-two-numbers.html#add-two-numbers",
    "relUrl": "/docs/compsci/leetcode/add-two-numbers.html#add-two-numbers"
  },"8": {
    "doc": "2 - Add Two Numbers - Medium",
    "title": "Problem",
    "content": "You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as a linked list. You may assume the two numbers do not contain any leading zero, except the number 0 itself. Input: l1 = [2,4,3], l2 = [5,6,4] Output: [7,0,8] Explanation: 342 + 465 = 807. ",
    "url": "/docs/compsci/leetcode/add-two-numbers.html#problem",
    "relUrl": "/docs/compsci/leetcode/add-two-numbers.html#problem"
  },"9": {
    "doc": "2 - Add Two Numbers - Medium",
    "title": "Solution",
    "content": "So basically the number 123 is represented as a linked list 3 -&gt; 2 -&gt; 1, and the number 456 is represented as a linked list 6 -&gt; 5 -&gt; 4. The answer should be 579, which is represented as a linked list 9 -&gt; 7 -&gt; 5. Things to note: . | The two linked lists may not have the same length. | We don’t know the length of the linked lists. | The answer may have at most one more digit than the longer linked list (because of the carry). | . The code consists of three parts: . | Traverse the two linked lists simultaneously and add the corresponding digits. | Check which linked list is longer. | If there is a carry, we need to do leftover work on the longer linked list. | . ",
    "url": "/docs/compsci/leetcode/add-two-numbers.html#solution",
    "relUrl": "/docs/compsci/leetcode/add-two-numbers.html#solution"
  },"10": {
    "doc": "2 - Add Two Numbers - Medium",
    "title": "Simultaneous traversal",
    "content": "We do not create a separate linked list for the answer. The answer is stored in-place in both linked lists. // Keep track of the head of both linked lists. ListNode ans1 = l1; ListNode ans2 = l2; // Why both? // Because we don't know which one is longer, // and we want to use the longer one for the answer. // Keep track of the previous node of both linked lists. ListNode l1_prev = null; ListNode l2_prev = null; // Why? In case we need to add a new node to the end of the linked list. int carry = 0; // Go on until one of the linked lists is exhausted. while (l1 != null &amp;&amp; l2 != null) { int sum = l1.val + l2.val + carry; carry = sum / 10; // Save for later // In-place update on both l1.val = sum % 10; l2.val = sum % 10; l1_prev = l1; l2_prev = l2; l1 = l1.next; l2 = l2.next; } . ",
    "url": "/docs/compsci/leetcode/add-two-numbers.html#simultaneous-traversal",
    "relUrl": "/docs/compsci/leetcode/add-two-numbers.html#simultaneous-traversal"
  },"11": {
    "doc": "2 - Add Two Numbers - Medium",
    "title": "Which linked list is longer?",
    "content": "Now that we’re out of the loop, either l1 or l2, or both, is null. // Tracking candidates for leftover work ListNode ans; // Head to return for the answer ListNode prev; // In case carry requires new node ListNode curr; // Where to start leftover work if (l1 != null) { ans = ans1; prev = l1_prev; curr = l1; } else { ans = ans2; prev = l2_prev; curr = l2; } . ",
    "url": "/docs/compsci/leetcode/add-two-numbers.html#which-linked-list-is-longer",
    "relUrl": "/docs/compsci/leetcode/add-two-numbers.html#which-linked-list-is-longer"
  },"12": {
    "doc": "2 - Add Two Numbers - Medium",
    "title": "Leftover carry",
    "content": "// Carry could go on until the end of the longer list while (carry &gt; 0) { // If we're at the end of the longer list, if (curr == null) { // This is why we had to keep track of the previous node prev.next = new ListNode(1); break; } // Same thing as before, just with one linked list int sum = curr.val + carry; carry = sum / 10; curr.val = sum % 10; prev = curr; curr = curr.next; } // Return the head of the answer return ans; . Complexity is $O(n)$ where $n$ is the length of the longer linked list. ",
    "url": "/docs/compsci/leetcode/add-two-numbers.html#leftover-carry",
    "relUrl": "/docs/compsci/leetcode/add-two-numbers.html#leftover-carry"
  },"13": {
    "doc": "2 - Add Two Numbers - Medium",
    "title": "2 - Add Two Numbers - Medium",
    "content": " ",
    "url": "/docs/compsci/leetcode/add-two-numbers.html",
    "relUrl": "/docs/compsci/leetcode/add-two-numbers.html"
  },"14": {
    "doc": "Adjugate Matrix / Minor / Cofactor",
    "title": "Adjugate Matrix / Minor / Cofactor",
    "content": ". | What is an adjugate matrix . | Product of a matrix and its adjugate matrix | . | Cofactor matrix . | Minor | Cofactor | . | . ",
    "url": "/docs/linalg/notes/adjugate.html",
    "relUrl": "/docs/linalg/notes/adjugate.html"
  },"15": {
    "doc": "Adjugate Matrix / Minor / Cofactor",
    "title": "What is an adjugate matrix",
    "content": "The adjugate matrix of a $n \\times n$ square matrix $A$, denoted by $\\operatorname{adj}(A)$ is defined as the transpose of the cofactor matrix $C$ of $A$. $$ \\operatorname{adj}(A) = C^T $$ . Every square matrix has an adjugate matrix. ",
    "url": "/docs/linalg/notes/adjugate.html#what-is-an-adjugate-matrix",
    "relUrl": "/docs/linalg/notes/adjugate.html#what-is-an-adjugate-matrix"
  },"16": {
    "doc": "Adjugate Matrix / Minor / Cofactor",
    "title": "Product of a matrix and its adjugate matrix",
    "content": "One important property of the adjugate matrix is that the product of a matrix and its adjugate matrix is a diagonal matrix whose diagonal elements are all equal to the determinant of the matrix. $$ A \\operatorname{adj}(A) = \\operatorname{adj}(A) A = \\det(A) I_n $$ . ",
    "url": "/docs/linalg/notes/adjugate.html#product-of-a-matrix-and-its-adjugate-matrix",
    "relUrl": "/docs/linalg/notes/adjugate.html#product-of-a-matrix-and-its-adjugate-matrix"
  },"17": {
    "doc": "Adjugate Matrix / Minor / Cofactor",
    "title": "Cofactor matrix",
    "content": " ",
    "url": "/docs/linalg/notes/adjugate.html#cofactor-matrix",
    "relUrl": "/docs/linalg/notes/adjugate.html#cofactor-matrix"
  },"18": {
    "doc": "Adjugate Matrix / Minor / Cofactor",
    "title": "Minor",
    "content": "To understand the cofactor matrix, we first need to understand the minor of a matrix. Technically, this is the first minor of a matrix. The $(i,j)$ minor of a $n \\times n$ square matrix $A$, denoted . $$ M_{ij} $$ . is the determinant of the $(n-1) \\times (n-1)$ submatrix obtained by removing the $i$-th row and $j$-th column of $A$. There are $n^2$ ways to choose the $i$-th row and $j$-th column of $A$ of which to remove. So these $n^2$ minors can be arranged in a $n \\times n$ matrix $M$. Example Given a $3 \\times 3$ matrix $A$, . \\[A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix}\\] The $(3, 2)$ minor of $A$ is: . \\[M_{32} = \\det \\begin{vmatrix} 1 &amp; 3 \\\\ 4 &amp; 6 \\end{vmatrix} = 1 \\times 6 - 3 \\times 4 = -6\\] ",
    "url": "/docs/linalg/notes/adjugate.html#minor",
    "relUrl": "/docs/linalg/notes/adjugate.html#minor"
  },"19": {
    "doc": "Adjugate Matrix / Minor / Cofactor",
    "title": "Cofactor",
    "content": "The $(i,j)$ cofactor of a $n \\times n$ square matrix $A$ is: . $$ C_{ij} = (-1)^{i+j} M_{ij} $$ . i.e. the minor of $A$ multiplied by $(-1)^{i+j}$. Example continued Continuing from the example above, we have $M_{32} = -6$. Then: . \\[C_{32} = (-1)^{3+2} M_{32} = -(-6) = 6\\] ",
    "url": "/docs/linalg/notes/adjugate.html#cofactor",
    "relUrl": "/docs/linalg/notes/adjugate.html#cofactor"
  },"20": {
    "doc": "Analysis of Variance",
    "title": "Analysis of Variance (ANOVA)",
    "content": ". | General Idea of ANOVA . | Predictor and Response Variables | Omnibus Test | . | F-Statistic (For One-Way ANOVA) . | Sum of Squares . | Total Sum of Squares (SS) | Sum of Squares Between (SSB) | Sum of Squares Within (SSW) | . | Mean Square . | Mean Square Between (MSB) | Mean Square Error (MSE) | . | F-Statistic | . | One-Way ANOVA . | F-Test | One-Way ANOVA Summary Table | . | Two-Way ANOVA | . ",
    "url": "/docs/statistics/basics/anova.html#analysis-of-variance-anova",
    "relUrl": "/docs/statistics/basics/anova.html#analysis-of-variance-anova"
  },"21": {
    "doc": "Analysis of Variance",
    "title": "General Idea of ANOVA",
    "content": "ANOVA is a hypothesis test that compares the means of three or more groups. Just like how t-tests use the t-statistics, ANOVA uses the F-test/F-statistic to compare the means of the groups. ANOVA is a parametric test on quantitative data. ",
    "url": "/docs/statistics/basics/anova.html#general-idea-of-anova",
    "relUrl": "/docs/statistics/basics/anova.html#general-idea-of-anova"
  },"22": {
    "doc": "Analysis of Variance",
    "title": "Predictor and Response Variables",
    "content": "You can think of ANOVA as a comparison of effects of different factors. | Categorical predictor variable: the variable that is manipulated and independent | Continuous response variable: the variable that is measured and dependent | . The type of ANOVA depends on the number of predictor variables (input / factor of influence) and the number of response variables (output / data measured). ",
    "url": "/docs/statistics/basics/anova.html#predictor-and-response-variables",
    "relUrl": "/docs/statistics/basics/anova.html#predictor-and-response-variables"
  },"23": {
    "doc": "Analysis of Variance",
    "title": "Omnibus Test",
    "content": "ANOVA is an omnibus test, meaning it tests for the overall. We would know that at least one pair of means are different, but it doesn’t tell us exactly which pair of means are different. We need to perform post-hoc test to find the specific pair. ",
    "url": "/docs/statistics/basics/anova.html#omnibus-test",
    "relUrl": "/docs/statistics/basics/anova.html#omnibus-test"
  },"24": {
    "doc": "Analysis of Variance",
    "title": "F-Statistic (For One-Way ANOVA)",
    "content": "The exact formulas for the F-statistic is tailored for one-way ANOVA. However, the general feel of the F-statistic is the same for all ANOVA tests. When there are multiple groups, we have two different sources of variation: . | Variation between groups | Variation within groups | . The F-statistic for ANOVA uses the ratio of these two variations: . $$ \\frac{\\text{Average variation between groups}}{\\text{Average variation within groups}} $$ . The above figure illustrates the logic behind the F-statistic. Say the red bar is the calculated mean of the data, and the blue dots are scattered data points around the mean. Between situation A and B, in which situtation does the difference between the two means seem more significant? . It is clear that the difference is more prominent in situation B, because there is more variation between the red bar, and less variation within the blue dots. This ratio makes their differences seem clear, and this is roughly what the F-statistic is trying to measure. Hence, if the F-statistic is large, it means the difference between the groups is more significant. ",
    "url": "/docs/statistics/basics/anova.html#f-statistic-for-one-way-anova",
    "relUrl": "/docs/statistics/basics/anova.html#f-statistic-for-one-way-anova"
  },"25": {
    "doc": "Analysis of Variance",
    "title": "Sum of Squares",
    "content": "The F-statistic is calculated using the sum of squares. Let us first define the following: . | $k$: number of groups | $n_j$: number of data points in group $j$ | $n$: sum of $n_j$ for all $j$ | $x_{ij}$: $i$th data point in group $j$ | $\\bar{x}_j$: sample mean of group $j$ | $\\bar{x}$: mean of all data points (overall mean) | . Total Sum of Squares (SS) . Total sum of squares is the sum of squared differences between each data point and the overall mean: . $$ SS_{Total} = \\sum_{j=1}^k \\sum_{i=1}^{n_j} (x_{ij} - \\bar{x})^2 $$ . $SS_{Total}$ can be decomposed into two parts: . $$ SS_{Total} = SS_{Between} + SS_{Within} $$ . Sum of Squares Between (SSB) . Sum of squares between is the sum of squared differences between each group mean and the overall mean: . $$ \\begin{equation*} \\label{eq:ssb} \\tag{SSB} SS_{Between} = \\sum_{j=1}^k \\sum_{i=1}^{n_j} (\\bar{x}_j - \\bar{x})^2 = \\sum_{j=1}^k n_j (\\bar{x}_j - \\bar{x})^2 \\end{equation*} $$ . Sum of Squares Within (SSW) . Sum of squares within is the sum of squared differences between each data point and its group mean: . $$ \\begin{equation*} \\label{eq:ssw} \\tag{SSW} SS_{Within} = \\sum_{j=1}^k \\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j)^2 \\end{equation*} $$ . This is also known as the residual sum of squares or the error sum of squares. ",
    "url": "/docs/statistics/basics/anova.html#sum-of-squares",
    "relUrl": "/docs/statistics/basics/anova.html#sum-of-squares"
  },"26": {
    "doc": "Analysis of Variance",
    "title": "Mean Square",
    "content": "But we cannot compare the sum of squares directly. Because the sum becomes larger as the sample size increases, we want to take the sample size into account to make the comparison more meaningful. We instead use a modified version called mean square, which is the sum of squares divided by the degrees of freedom. Mean Square Between (MSB) . The degrees of freedom for $\\eqref{eq:ssb}$ is $k-1$. Very roughly, think of this as: the outcome of this formula is already fixed, there’s an overall mean, and we have $k$ group means that affect the overall mean. We are free to choose $k-1$ of the group means, but the last group mean is determined by the rest. Therefore, the mean square between is: . $$ MSB = \\frac{SS_{Between}}{k-1} $$ . Mean Square Error (MSE) . The degrees of freedom for $\\eqref{eq:ssw}$ is $n-k$. Again, roughly we are using all $n$ data points, but the $k$ group means are determined by the $n$ data points. So we lose $k$ degrees of freedom. $$ MSE = \\frac{SS_{Within}}{n-k} $$ . ",
    "url": "/docs/statistics/basics/anova.html#mean-square",
    "relUrl": "/docs/statistics/basics/anova.html#mean-square"
  },"27": {
    "doc": "Analysis of Variance",
    "title": "F-Statistic",
    "content": "The F-value is calculated as follows: . $$ F = \\frac{MSB}{MSE} $$ . Now we compare this F-value to the F-distribution (with $k-1$ and $n-k$ degrees of freedom) to obtain the p-value. ",
    "url": "/docs/statistics/basics/anova.html#f-statistic",
    "relUrl": "/docs/statistics/basics/anova.html#f-statistic"
  },"28": {
    "doc": "Analysis of Variance",
    "title": "One-Way ANOVA",
    "content": " ",
    "url": "/docs/statistics/basics/anova.html#one-way-anova",
    "relUrl": "/docs/statistics/basics/anova.html#one-way-anova"
  },"29": {
    "doc": "Analysis of Variance",
    "title": "Why is it called “one-way”?",
    "content": "It is called “one-way” because this test deals with data with a single factor of influence (predictor variable). For example, if we wanted to compare the effects of three different fertilizers on plant growth, type of fertilizers will be the single predictor variable and plant height will be the response variable. Notice that the predictor is categorical. Each subject (e.g. plant) is allocated to one and only one group (e.g. fertilizer). ",
    "url": "/docs/statistics/basics/anova.html#why-is-it-called-one-way",
    "relUrl": "/docs/statistics/basics/anova.html#why-is-it-called-one-way"
  },"30": {
    "doc": "Analysis of Variance",
    "title": "Assumptions",
    "content": "One-way ANOVA requires the following assumptions: . | Continuous dependent variable: the dependent variable is continuous (quantitative) | Categorial independent variable: no overlapping subjects between groups | No significant outliers: must be accounted for | Normality: each dependent variable is approximately normal . | Although one-way ANOVA is robust to violations of normality to a certain degree, it is still a good idea to check the normality of the residuals | . | Equal variance: each dependent variable has the same variance | . ",
    "url": "/docs/statistics/basics/anova.html#assumptions",
    "relUrl": "/docs/statistics/basics/anova.html#assumptions"
  },"31": {
    "doc": "Analysis of Variance",
    "title": "F-Test",
    "content": "Given $k$ groups, one-way ANOVA tests the following null hypothesis: . $$ H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_k $$ . In this case, the alternative hypothesis is: . $$ \\text{At least one pair of means are significantly different} $$ . When the null hypothesis true, the F-statistic follows the F-distribution with $k-1$ and $n-k$ degrees of freedom. When our null hypothesis is true (the groups are not significantly different), two variations are approximately equal so the F-ratio will be closer to 1. Hence the peak of the F-distribution is at $\\le 1$ (1 ideally, but slightly less than 1 for small degrees of freedom), and being further away to the right means the groups are more significantly different. We place the F-value on the F-distribution and calculate the p-value. ",
    "url": "/docs/statistics/basics/anova.html#f-test",
    "relUrl": "/docs/statistics/basics/anova.html#f-test"
  },"32": {
    "doc": "Analysis of Variance",
    "title": "One-Way ANOVA Summary Table",
    "content": "Typically, we use the following table to summarize the results of ANOVA: . These results from ANOVA are often reused in post-hoc tests. ",
    "url": "/docs/statistics/basics/anova.html#one-way-anova-summary-table",
    "relUrl": "/docs/statistics/basics/anova.html#one-way-anova-summary-table"
  },"33": {
    "doc": "Analysis of Variance",
    "title": "Two-Way ANOVA",
    "content": "To be added . ",
    "url": "/docs/statistics/basics/anova.html#two-way-anova",
    "relUrl": "/docs/statistics/basics/anova.html#two-way-anova"
  },"34": {
    "doc": "Analysis of Variance",
    "title": "Why is it called “two-way”?",
    "content": "It is called “two-way” because this test deals with data with two factors of influence (predictor variables). For example, if we wanted to compare the effects of three different fertilizers and two different types of soil on plant growth, type of fertilizers and type of soil will be the predictor variables, and plant height will be the response variable. ",
    "url": "/docs/statistics/basics/anova.html#why-is-it-called-two-way",
    "relUrl": "/docs/statistics/basics/anova.html#why-is-it-called-two-way"
  },"35": {
    "doc": "Analysis of Variance",
    "title": "Assumptions",
    "content": "Same as one-way ANOVA. ",
    "url": "/docs/statistics/basics/anova.html#assumptions-1",
    "relUrl": "/docs/statistics/basics/anova.html#assumptions-1"
  },"36": {
    "doc": "Analysis of Variance",
    "title": "Analysis of Variance",
    "content": " ",
    "url": "/docs/statistics/basics/anova.html",
    "relUrl": "/docs/statistics/basics/anova.html"
  },"37": {
    "doc": "Autoregressive (AR) Model",
    "title": "Autoregressive (AR) Model",
    "content": "As the name suggests, autoregressive (AR) models are based on the idea that future values can be predicted from past values. | Univariate autoregressive model . | $AR(1)$ | Lag operator | $AR(p)$ | . | . ",
    "url": "/docs/data-science/notes/ar-model.html",
    "relUrl": "/docs/data-science/notes/ar-model.html"
  },"38": {
    "doc": "Autoregressive (AR) Model",
    "title": "Univariate autoregressive model",
    "content": " ",
    "url": "/docs/data-science/notes/ar-model.html#univariate-autoregressive-model",
    "relUrl": "/docs/data-science/notes/ar-model.html#univariate-autoregressive-model"
  },"39": {
    "doc": "Autoregressive (AR) Model",
    "title": "$AR(1)$",
    "content": "The simplest AR model is the $AR(1)$ where the $1$ indicates a lag of $1$ time step: . \\[y_t = \\phi_0 + \\phi_1 y_{t-1} + \\varepsilon_t\\] where $\\varepsilon_t$ is the white noise term with constant variance. We calculate the expected value and variance of $y_t$ given $y_{t-1}$: . \\[\\begin{gather*} \\E[y_t \\mid y_{t-1}] = \\phi_0 + \\phi_1 y_{t-1} + \\varepsilon_t \\\\[1em] \\Var[y_t \\mid y_{t-1}] = \\Var[\\varepsilon_t] = \\sigma_\\varepsilon^2 \\end{gather*}\\] ",
    "url": "/docs/data-science/notes/ar-model.html#ar1",
    "relUrl": "/docs/data-science/notes/ar-model.html#ar1"
  },"40": {
    "doc": "Autoregressive (AR) Model",
    "title": "Lag operator",
    "content": "Lag operator (denoted by $L$) is a convenient notation for autoregressive models. It is also called a backshift operator and is denoted by $B$ in some literature. The lag operator can be raised to an arbitrary power $k$ to indicate a time series lagged by $k$ time steps: . $$ L^k y_t = y_{t-k} $$ . ",
    "url": "/docs/data-science/notes/ar-model.html#lag-operator",
    "relUrl": "/docs/data-science/notes/ar-model.html#lag-operator"
  },"41": {
    "doc": "Autoregressive (AR) Model",
    "title": "$AR(p)$",
    "content": "The general form of $AR(p)$ (read “AR model of order $p$”) is: . $$ y_t = \\sum_{i=1}^p \\phi_i y_{t-i} + \\varepsilon_t $$ . We often simplify the notation by using the lag polynomial notation: . $$ \\phi(L) y_t = \\varepsilon_t $$ . Polynomial notation \\[\\begin{gather*} y_t = \\sum_{i=1}^p \\phi_i y_{t-i} + \\varepsilon_t \\\\[1em] y_t - \\sum_{i=1}^p \\phi_i y_{t-i} = \\varepsilon_t \\\\[1em] (1 - \\sum_{i=1}^p \\phi_i L^i) y_t = \\varepsilon_t \\\\[1em] \\phi(L) y_t = \\varepsilon_t \\end{gather*}\\] ",
    "url": "/docs/data-science/notes/ar-model.html#arp",
    "relUrl": "/docs/data-science/notes/ar-model.html#arp"
  },"42": {
    "doc": "Autocorrelation",
    "title": "Autocorrelation",
    "content": ". | What is autocorrelation? | ACF and PACF . | Autocorrelation Function (ACF) . | Periodicity of ACF . | Additive property | . | Identifying stationarity with ACF | Statistical significance of sample ACF . | Standard normal white noise | General hypothesis test | . | . | Partial Autocorrelation Function (PACF) . | PACF is not periodic | Identifying non-stationarity with PACF | Statistical significance of sample PACF | . | . | . ",
    "url": "/docs/data-science/notes/autocorrelation.html",
    "relUrl": "/docs/data-science/notes/autocorrelation.html"
  },"43": {
    "doc": "Autocorrelation",
    "title": "What is autocorrelation?",
    "content": "Autocorrelation is a measure of how correlated a time series data is with the lagged version of itself. While regular correlation measure the relationship between two different variables, autocorrelation measures the relationship between variables $X_t$ and $X_{t-k}$, where $k$ is the lag. \\[\\rho(k) = \\frac{\\Cov[X_t, X_{t-k}]}{\\sqrt{\\Var[X_t] \\Var[X_{t-k}]}}\\] But assuming that the process is stationary (i.e. mean and variance are constant), the autocorrelation can be simplified to: . $$ \\rho(k) = \\frac{\\Cov[X_t, X_{t-k}]}{\\Var[X_t]} $$ . ",
    "url": "/docs/data-science/notes/autocorrelation.html#what-is-autocorrelation",
    "relUrl": "/docs/data-science/notes/autocorrelation.html#what-is-autocorrelation"
  },"44": {
    "doc": "Autocorrelation",
    "title": "ACF and PACF",
    "content": "Correlogram . ",
    "url": "/docs/data-science/notes/autocorrelation.html#acf-and-pacf",
    "relUrl": "/docs/data-science/notes/autocorrelation.html#acf-and-pacf"
  },"45": {
    "doc": "Autocorrelation",
    "title": "Autocorrelation Function (ACF)",
    "content": "Autocorrelation Function (ACF) is autocorrelation as a function of lagged difference. So obviously, at lag $0$, ACF is always $1$. This trivial result is disregarded. ACF includes both direct correlation and indirect/conditional correlation between $X_t$ and $X_{t-k}$. For example, in calculation of ACF for lag $2$, the direct correlation between $X_t$ and $X_{t-2}$, as well as the indirect correlation between $X_t$ and $X_{t-1}$ and between $X_{t-1}$ and $X_{t-2}$, all contribute to the ACF. ACF is symmetric for positive and negative lags, so only positive lags are plotted. Periodicity of ACF . ACF of a periodic process has the same periodicity as the original process. Therefore there can be repeating patterns in ACF (take a look at the correlogram above). Additive property . For two periodic process $X_t$ and $Y_t$, $ACF(X_t + Y_t) = ACF(X_t) + ACF(Y_t)$. Identifying stationarity with ACF . For stationary process, ACF quickly decays to $0$ as lag increases. If it does not, the process is non-stationary. In that case you should try things like plotting the ACF of the differenced data (remove trend). Statistical significance of sample ACF . Correlograms are often plotted with bands which indicate the critical values for statistical significance of AC. Due to randomness in the data, we rarely get exactly zero for AC. Even for white noise data, we get non-zero values even though there is no autocorrelation between the data points. Therefore, for some significance level, ACs that fall within the confidence interval are considered to be $0$ and thus not statistically significant. Standard normal white noise . For standard normal white noise . \\[X_t \\sim \\mathcal{N}(0, 1)\\] theoretically AC is expected to be $0$. So the 95 confidence interval for AC is . \\[0 \\pm {1.96} \\times \\frac{1}{\\sqrt{n}}\\] where $n$ is the sample size or the length of time series. Although this calculation is based on the assumptions of standard normality (standard error of the mean is $\\frac{1}{\\sqrt{n}}$), seems like it is quite often used as a test of randomness on other time series data as well. Without enough sample size, this test becomes too conservative. General hypothesis test . In general, the statistical significance of AC under the null hypothesis that AC is zero (there is no correlation) and significance level $\\alpha$ is estimated with: . $$ \\pm z_{\\alpha/2} \\times SE(\\hat{\\rho}(k)) $$ . where $\\hat{\\rho}(k)$ is the sample autocorrelation at lag $k$. The standard error depends on the actual sample AC of the data and there are approximation methods to estimate it. These are usually represented by shaded areas in ACF plots. ",
    "url": "/docs/data-science/notes/autocorrelation.html#autocorrelation-function-acf",
    "relUrl": "/docs/data-science/notes/autocorrelation.html#autocorrelation-function-acf"
  },"46": {
    "doc": "Autocorrelation",
    "title": "Partial Autocorrelation Function (PACF)",
    "content": "Partial Autocorrelation Function (PACF) is basically ACF with the indirect/conditional correlation removed. Unlike ACF, PACF only considers the direct correlation between $X_t$ and $X_{t-k}$. PACF controls other intermediate lags by including them as regressors. For example, for lag $2$, PACF fits the following regression model: . \\[y_t = \\beta_0 + \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\epsilon_t\\] Then outpus $\\beta_2$ as the PACF for lag $2$, the direct effect of $y_{t-2}$ on $y_t$. Because of this, PACF is used as a heuristic to determine the order of AR models. However, this heuristic can be used only if data is stationary. For data with a lot of random noise, PACF gets harder to interpret because it won’t diminish. PACF is not periodic . Unlike ACF which carries the periodicity of the original process, PACF does not carry this redundancy. Therefore sum of two periodic processes is not additive unlike ACF. Identifying non-stationarity with PACF . For a trending non-stationary process, the value at lag $1$ is often positive and large and the values at other lags are not statistically significant. This can be interpreted as that the only thing affecting each data point is the previous data point. The previous point contains all the information that is needed to predict the next point, which typically indicates a trend. Statistical significance of sample PACF . It is calculated in the same way as ACF. ",
    "url": "/docs/data-science/notes/autocorrelation.html#partial-autocorrelation-function-pacf",
    "relUrl": "/docs/data-science/notes/autocorrelation.html#partial-autocorrelation-function-pacf"
  },"47": {
    "doc": "AWS CLI",
    "title": "AWS CLI",
    "content": ". | Install | Set up AWS CLI . | Configure credentials | Show configuration | . | Upload file to S3 | . ",
    "url": "/docs/aws/aws-cli.html",
    "relUrl": "/docs/aws/aws-cli.html"
  },"48": {
    "doc": "AWS CLI",
    "title": "Install",
    "content": "brew install awscli . ",
    "url": "/docs/aws/aws-cli.html#install",
    "relUrl": "/docs/aws/aws-cli.html#install"
  },"49": {
    "doc": "AWS CLI",
    "title": "Set up AWS CLI",
    "content": "Configure credentials . To create a named profile: . $ aws configure --profie ${profile_name} AWS Access Key ID [None]: ... AWS Secret Access Key [None]: ... Default region name [None]: ... Default output format [None]: json . You can create multiple profiles. Your configuration is saved in two files: ~/.aws/config and ~/.aws/credentials. To modify an existing profile you can either modify it directly in the files or use the aws configure command again: . $ aws configure set region ${value_to_set} --profile ${profile_name} . Show configuration . To see the list of all profiles: . aws configure list-profiles . To see the configuration of a specific profile: . aws configure list --profile ${profile_name} . To see the value of a specific variable of a specific profile: . aws configure get region --profile ${profile_name} . ",
    "url": "/docs/aws/aws-cli.html#set-up-aws-cli",
    "relUrl": "/docs/aws/aws-cli.html#set-up-aws-cli"
  },"50": {
    "doc": "AWS CLI",
    "title": "Upload file to S3",
    "content": "To upload a folder to a subdirectory in a bucket: . aws s3 cp ${dir_name} s3://${bucket_name}/${sub_dir}/${dir_name} --recursive --profile ${profile} . References: . | AWS CLI: Named profiles | AWS CLI: Configuration and credential file settings | . ",
    "url": "/docs/aws/aws-cli.html#upload-file-to-s3",
    "relUrl": "/docs/aws/aws-cli.html#upload-file-to-s3"
  },"51": {
    "doc": "Basic Preprocessing",
    "title": "Basic Time Series Preprocessing",
    "content": ". | Missing Data . | Different types of missing data . | Random | Systematic | . | Common ways to handle missing data . | Imputation . | Forward fill | Moving average (MA) | Interpolation | . | Delete data with missing values | . | . | Downsampling and Upsampling . | Downsampling . | Reasons to downsample . | Redundant recordings | Focus on a specific time scale | Match frequency to other data | . | . | Upsampling . | Reasons to upsample . | Irregularly sampled data | To match frequency to other data | Prior knowledge about data | . | . | . | Smoothing Data . | Reasons to smooth data . | Visualization | Data preparation | Feature generation | Prediction . | Mean reversion | . | . | Methods to smooth data . | Moving average | Exponential smoothing | Other methods | . | . | Dealing with Seasonality | Beware of Time Zones | Beware of Lookahead | . ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#basic-time-series-preprocessing",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#basic-time-series-preprocessing"
  },"52": {
    "doc": "Basic Preprocessing",
    "title": "Missing Data",
    "content": "It is quite common to have missing data due to the longitudinal nature of time series data. ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#missing-data",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#missing-data"
  },"53": {
    "doc": "Basic Preprocessing",
    "title": "Different types of missing data",
    "content": "Random . Could occur due to multiple reasons: . | Recording malfunction | Human error | Data corruption | Unforeseen circumstances… | . Systematic . Could occur due to multiple reasons: . | Defined events (i.e. holidays) | Data collection process (i.e. only weekdays) | Policy and regulations | Basically on purpose… | . ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#different-types-of-missing-data",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#different-types-of-missing-data"
  },"54": {
    "doc": "Basic Preprocessing",
    "title": "Common ways to handle missing data",
    "content": "Imputation . Most common way to fix missing data. Depending on the type of missing data (random or systematic), each following method shows different performance. Whether you include future values in imputation also impacts performance, but needs caution due to lookahead. Forward fill . Fill with the last known value before the missing value. Pros: . | Computationally cheap | Can be easily applied to live-streamed data | . Cons: . | Susceptible to introducing unwanted noise / outliers | . Backward fill goes the opposite way. However, since it is a case of lookahead, it should not be used for prediction training and be used only if it makes sense via domain knowledge. Moving average (MA) . Fill with the average of some window of values near the missing value. Calculation is not limited to arithmetic mean. It can be weighted, geometric, etc. It is also called rolling mean. Whether the window should include future values is up to discretion. Including future values is a case of lookahead, but it does improve the estimation of the missing value. One suggestion is to include future values for visualization and exploration, but exclude them for prediction training. Pros: . | Good for noisy data | . Cons: . | Moving average reduces variance. So it must be kept in mind when evaluating model accuracy with $R^2$ or other error statistics because it leads to overestimation of model performance. | . Using total mean to fill missing values is often not a good choice for time series data although it is common for some other data analysis. Also, it is again a case of a lookahead. Interpolation . Using nearby neighbors to decide how missing point should behave. What defines a neighbor is up to decision. For example, linear interpolation uses neighbors to constrain missing point to a linear fit. Just like moving average, you want to decide whether to include future values in imputation. Pros: . | Interpolation is especially useful if you know how the data behaves or prior knowledge about the data (e.g. trend, seasonality). | . Cons: . | If you don’t have prior knowledge or it doesn’t make sense to expect a known behavior, it’s not very helpful. | . When is interpolation better than moving average? As previously mentioned, interpolation is useful if you have prior knowledge. For example if you know there is an upward trend, using moving average will systematically underestimate the missing value. Interpolation can avoid this. Delete data with missing values . Aside from imputation, you could also decide not to use portions of data with missing values. It is not desirable because you would lose that much data, but it is a valid option if you have to patch up too many missing values. ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#common-ways-to-handle-missing-data",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#common-ways-to-handle-missing-data"
  },"55": {
    "doc": "Basic Preprocessing",
    "title": "Downsampling and Upsampling",
    "content": " ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#downsampling-and-upsampling",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#downsampling-and-upsampling"
  },"56": {
    "doc": "Basic Preprocessing",
    "title": "Downsampling",
    "content": "Reasons to downsample . Redundant recordings . Redundant recordings with not much new information takes up storage space and processing time. Possible downsampling method is to select every $n$th data point. Focus on a specific time scale . For data with seasonal cycle, you may want to downsample to certain months (i.e. only January) to focus on the season of interest for your analysis. Match frequency to other data . If you have multiple time series data, analysis becomes difficult if they are recorded at varying frequencies. So you may choose to downsample to the lower frequency among the data. Rather than simply selecting and dropping points, it is better to perform aggregation (i.e. mean, sum, weighted mean etc.). ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#downsampling",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#downsampling"
  },"57": {
    "doc": "Basic Preprocessing",
    "title": "Upsampling",
    "content": "This is not exactly the opposite of downsampling, in the sense that you can’t really create new data points out of thin air. However, you can choose to label/timestamp data at a higher frequency than the original recording frequency. Beware of creating lookahead when upsampling. Reasons to upsample . Irregularly sampled data . If you want to convert irregularly sampled data to regularly sampled data, you can upsample the data by upsampling the lags between data points. To match frequency to other data . Same with downsampling, you may upsample to align data points. Prior knowledge about data . If you have prior knowledge about the data and you intend to upsample, you can use interpolation just like imputation. This would actually be in some sense “creating new data” unlike other upsampling methods. ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#upsampling",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#upsampling"
  },"58": {
    "doc": "Basic Preprocessing",
    "title": "Smoothing Data",
    "content": "Smoothing is related to imputing missing data, so some of the methods (like moving average) apply when smoothing data. In addition, you must be cautious about lookahead just like imputation. It is a good idea to check that smoothing does not compromise any assumptions your model may have (i.e. model assumes noisy data). ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#smoothing-data",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#smoothing-data"
  },"59": {
    "doc": "Basic Preprocessing",
    "title": "Reasons to smooth data",
    "content": "Visualization . | Most trivial reason | To understand the data better before analysis | . Data preparation . | Eliminate noise and outliers (i.e. via moving average). | Remove seasonality | . Feature generation . | In order to effectively summarize data, you may want to smooth or essentially simplify the data to a lower dimension or a less complex form to generate characteristic features. | . Prediction . Simplest form of prediction comes from smoothing data. Mean reversion . Mean reversion is a financial theory that states that even though price seems to deviate in the short term, it eventually remains close to its long-term mean. So if you smooth a wave-like time series data, you would get a line and you could predict that future price will remain near that line. ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#reasons-to-smooth-data",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#reasons-to-smooth-data"
  },"60": {
    "doc": "Basic Preprocessing",
    "title": "Methods to smooth data",
    "content": "Moving average . See above . Exponential smoothing . It is similar to a weighted moving average, but it differs in that it gives more weight to recent data points and less weight to older data points. It also differs from moving average in that it uses all past data points (aggregated in a single forecast value) unlike moving average which uses only a window of data points. The simplest form of exponential smoothing is as follows: . Given some smoothing factor $\\alpha$, the simplest smoothed value $S_t$ at time $t$ is defined as: . $$ \\begin{cases} S_0 = y_0 \\\\[1em] S_t = \\alpha y_t + (1 - \\alpha) S_{t-1} \\end{cases} $$ . With direct substitution, this recursion expands to weights of . \\[1, 1 - \\alpha, (1 - \\alpha)^2, (1 - \\alpha)^3, \\dots\\] And past values are associated with higher order weights making them less significant. Hence the name exponential smoothing. Simple exponential smoothing doesn’t work well on data with a trend. Use Holt’s Method for data with trend and Holt-Winters for data with trend and seasonality. Other methods . | Kalman filter | LOESS (locally estimated scatterplot smoothing) | . Both are more computationally expensive and are cases of lookahead because they incorporate both past and future values. ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#methods-to-smooth-data",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#methods-to-smooth-data"
  },"61": {
    "doc": "Basic Preprocessing",
    "title": "Dealing with Seasonality",
    "content": "Read more here . ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#dealing-with-seasonality",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#dealing-with-seasonality"
  },"62": {
    "doc": "Basic Preprocessing",
    "title": "Beware of Time Zones",
    "content": "Not much to say here, but beware of time zones because time zone, daylight savings, etc. can be a pain. ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#beware-of-time-zones",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#beware-of-time-zones"
  },"63": {
    "doc": "Basic Preprocessing",
    "title": "Beware of Lookahead",
    "content": "During training or evaluation of a model, it is important to avoid using data that would not have been available at the time of the prediction. (If you already know the future, the prediction is gonna be easy, but that’s not what we want to do.) . Any knowledge of the future used in a model is called lookahead. If you have a lookahead you would find tht your model does not perform as well in real life as it did in training. Lookahead does not only pertain to calendar time. Some preprocessing steps such as imputing or smoothing may introduce lookahead, so you want to make sure if this is desirable for your purpose and make sure it is not used for prediction training. ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html#beware-of-lookahead",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html#beware-of-lookahead"
  },"64": {
    "doc": "Basic Preprocessing",
    "title": "Basic Preprocessing",
    "content": " ",
    "url": "/docs/data-science/time-series/basic-preprocessing.html",
    "relUrl": "/docs/data-science/time-series/basic-preprocessing.html"
  },"65": {
    "doc": "Basic Probability",
    "title": "Basic Probability Theory",
    "content": ". | Probability | Random Variable | Probability Distribution . | Discrete Probability Distribution . | Probability Mass Function | . | Continuous Probability Distribution . | Probability Density Function | . | . | Expected Value . | Expected Value of Discrete Random Variable | Expected Value of Continuous Random Variable | . | Variance and Standard Deviation . | Variance of Discrete Random Variable | Variance of Continuous Random Variable | . | Joint Probability . | Independent Random Variables | Conditional Probability | . | Common Probability Distributions . | Normal Distribution (Gaussian Distribution) . | Characteristics of Normal Distribution | . | Standard Normal Distribution . | Z-Score | Z-Score Table | . | Others | . | . ",
    "url": "/docs/statistics/basics/basic-probs.html#basic-probability-theory",
    "relUrl": "/docs/statistics/basics/basic-probs.html#basic-probability-theory"
  },"66": {
    "doc": "Basic Probability",
    "title": "Probability",
    "content": "In general, we use the following notation for probability of an event $A$: . $$ P(A) $$ . ",
    "url": "/docs/statistics/basics/basic-probs.html#probability",
    "relUrl": "/docs/statistics/basics/basic-probs.html#probability"
  },"67": {
    "doc": "Basic Probability",
    "title": "Random Variable",
    "content": "Let $X$ be a random variable. Probability of $X$ taking a value $x$ is denoted by: . $$ P(X = x) $$ . ",
    "url": "/docs/statistics/basics/basic-probs.html#random-variable",
    "relUrl": "/docs/statistics/basics/basic-probs.html#random-variable"
  },"68": {
    "doc": "Basic Probability",
    "title": "Probability Distribution",
    "content": "Let $X$ be a random variable. A probability distribution is a function that maps each value of $X$ to its probability. In inferential statistics, we assume a certain probability distribution for the population. Then the samples become random variables that follow the same probability distribution. ",
    "url": "/docs/statistics/basics/basic-probs.html#probability-distribution",
    "relUrl": "/docs/statistics/basics/basic-probs.html#probability-distribution"
  },"69": {
    "doc": "Basic Probability",
    "title": "Discrete Probability Distribution",
    "content": "The probability distribution of a discrete random variable $X$, is essentially the probability of each value of $X$. Probability Mass Function . The probability mass function (PMF) of a discrete variable $X$ is denoted by: . $$ p_{X}(x) $$ . This function maps each value of $X$ to its exact probability. For discrete random variables, the PMF is the probability distribution. $$ P(X = x) = p_{X}(x) $$ . ",
    "url": "/docs/statistics/basics/basic-probs.html#discrete-probability-distribution",
    "relUrl": "/docs/statistics/basics/basic-probs.html#discrete-probability-distribution"
  },"70": {
    "doc": "Basic Probability",
    "title": "Continuous Probability Distribution",
    "content": "The probability distribution of a continuous random variable $X$, is the area under the curve of the probability density function (PDF) of $X$ for a given interval. Probability Density Function . The probability density function (PDF) of a continuous variable $X$ is denoted by . $$ f_{X}(x) $$ . The absolute probability of $X$ taking a specific value is zero, since there are infinitely many values that $X$ can take. Therefore, the PDF of $X$ is not an absolute probability, but a relative probability per unit range. Let $f_{X}(x)$ be the PDF of $X$. Then the probability that $X$ takes a value in the interval $[a, b]$ is: . $$ P(a \\leq X \\leq b) = \\int_{a}^{b} f_{X}(x) dx $$ . ",
    "url": "/docs/statistics/basics/basic-probs.html#continuous-probability-distribution",
    "relUrl": "/docs/statistics/basics/basic-probs.html#continuous-probability-distribution"
  },"71": {
    "doc": "Basic Probability",
    "title": "Expected Value",
    "content": "The expected value of a quantitative random variable $X$ is denoted by: . $$ E(X) $$ . It is a weighted average of the values of $X$, where the weights are the probabilities of each value. ",
    "url": "/docs/statistics/basics/basic-probs.html#expected-value",
    "relUrl": "/docs/statistics/basics/basic-probs.html#expected-value"
  },"72": {
    "doc": "Basic Probability",
    "title": "Expected Value of Discrete Random Variable",
    "content": "Let $X$ be a discrete random variable. Then: . $$ E(X) = \\sum_{x} x \\cdot p_{X}(x) $$ . ",
    "url": "/docs/statistics/basics/basic-probs.html#expected-value-of-discrete-random-variable",
    "relUrl": "/docs/statistics/basics/basic-probs.html#expected-value-of-discrete-random-variable"
  },"73": {
    "doc": "Basic Probability",
    "title": "Expected Value of Continuous Random Variable",
    "content": "Let $X$ be a continuous random variable. Then: . $$ E(X) = \\int x \\cdot f_{X}(x) dx $$ . The range of the integral is the entire range of $X$. ",
    "url": "/docs/statistics/basics/basic-probs.html#expected-value-of-continuous-random-variable",
    "relUrl": "/docs/statistics/basics/basic-probs.html#expected-value-of-continuous-random-variable"
  },"74": {
    "doc": "Basic Probability",
    "title": "Variance and Standard Deviation",
    "content": "Variance and standard deviation measures how much the values of a random variable $X$ are spread out from the expected value. Standard deviation is the square root of variance. ",
    "url": "/docs/statistics/basics/basic-probs.html#variance-and-standard-deviation",
    "relUrl": "/docs/statistics/basics/basic-probs.html#variance-and-standard-deviation"
  },"75": {
    "doc": "Basic Probability",
    "title": "Variance of Discrete Random Variable",
    "content": "Let $X$ be a discrete random variable. Then: . $$ V(X) = \\sum_{x} (x - E(X))^{2} \\cdot p_{X}(x) $$ . ",
    "url": "/docs/statistics/basics/basic-probs.html#variance-of-discrete-random-variable",
    "relUrl": "/docs/statistics/basics/basic-probs.html#variance-of-discrete-random-variable"
  },"76": {
    "doc": "Basic Probability",
    "title": "Variance of Continuous Random Variable",
    "content": "Let $X$ be a continuous random variable. Then: . $$ V(X) = \\int (x - E(X))^{2} \\cdot f_{X}(x) dx $$ . Some interesting properties of the variance/standard deviation: . | $V(X) \\geq 0$ | $V(X) = 0$ if and only if $X$ is a constant | Higher the value, higher the dispersion from the expected value | . ",
    "url": "/docs/statistics/basics/basic-probs.html#variance-of-continuous-random-variable",
    "relUrl": "/docs/statistics/basics/basic-probs.html#variance-of-continuous-random-variable"
  },"77": {
    "doc": "Basic Probability",
    "title": "Joint Probability",
    "content": "Let $X$ and $Y$ be two random variables. The joint probability of $X$ and $Y$ is denoted by: . $$ P(X, Y) $$ . ",
    "url": "/docs/statistics/basics/basic-probs.html#joint-probability",
    "relUrl": "/docs/statistics/basics/basic-probs.html#joint-probability"
  },"78": {
    "doc": "Basic Probability",
    "title": "Independent Random Variables",
    "content": "Two random variables $X$ and $Y$ are independent if: . $$ P(X, Y) = P(X) \\cdot P(Y) $$ . ",
    "url": "/docs/statistics/basics/basic-probs.html#independent-random-variables",
    "relUrl": "/docs/statistics/basics/basic-probs.html#independent-random-variables"
  },"79": {
    "doc": "Basic Probability",
    "title": "Conditional Probability",
    "content": "The conditional probability of $X$ given $Y$ is denoted by $P(X | Y)$. $$ P(X | Y) = \\frac{P(X, Y)}{P(Y)} $$ . If $X$ and $Y$ are independent, then: . $$ P(X | Y) = P(X) $$ . ",
    "url": "/docs/statistics/basics/basic-probs.html#conditional-probability",
    "relUrl": "/docs/statistics/basics/basic-probs.html#conditional-probability"
  },"80": {
    "doc": "Basic Probability",
    "title": "Common Probability Distributions",
    "content": "As described earlier, we assume a certain probability distribution for the population. Each probability distribution has its own set of parameters. These parameters determine the shape of the distribution. Therefore, knowing the parameters is equivalent to understanding the population. Below are some common probability distributions and their parameters. ",
    "url": "/docs/statistics/basics/basic-probs.html#common-probability-distributions",
    "relUrl": "/docs/statistics/basics/basic-probs.html#common-probability-distributions"
  },"81": {
    "doc": "Basic Probability",
    "title": "Normal Distribution (Gaussian Distribution)",
    "content": "The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution. It is the most common distribution in statistics. The normal distribution is characterized by two parameters: . | $\\mu$: Mean (location of distribution) | $\\sigma$: Standard deviation (width of distribution) | . The notation for the normal distribution is: . $$ X \\sim N(\\mu, \\sigma^{2}) $$ . The PDF of the normal distribution is: . $$ f_{X}(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-\\frac{(x - \\mu)^{2}}{2 \\sigma^{2}}} $$ . Hard to see the exponent in the PDF? $$ -\\frac{(x - \\mu)^{2}}{2 \\sigma^{2}} $$ . Characteristics of Normal Distribution . | Symmetric around the mean | Mean, median, and mode are equal | . | 68% of the values are approximately within $\\mu \\pm 1 \\sigma$ | 95% of the values are approximately within $\\mu \\pm 2 \\sigma$ | 99.7% of the values are approximately within $\\mu \\pm 3 \\sigma$ | . ",
    "url": "/docs/statistics/basics/basic-probs.html#normal-distribution-gaussian-distribution",
    "relUrl": "/docs/statistics/basics/basic-probs.html#normal-distribution-gaussian-distribution"
  },"82": {
    "doc": "Basic Probability",
    "title": "Standard Normal Distribution",
    "content": "The normal distribution of $x$ against the probability density transformed into $z$ against the probability density is the standard normal distribution. We can denote the standard normal distribution as: . $$ Z \\sim N(0, 1) $$ . Z-Score . Standardization or z-score normalization is the process of normalizing a dataset so that . $$ \\begin{align*} \\mu &amp;= 0 \\\\ \\sigma &amp;= 1 \\end{align*} $$ . We do this by obtaining the standard score or z-score of each value by: . $$ z = \\frac{x - \\mu}{\\sigma} $$ . Z-score is essentially calculating how many $\\sigma$ a value is away from the mean. Higher the z-score, higher the value is from the mean. Z-Score Table . Below is a part of the z-score table. This table specifically, is for the left-tailed z-score. It shows the probability of a value being less than a certain z-score. Since the normal distribution is symmetric, the logic can be applied to the right-tailed z-score as well. | Z | .00 | .01 | .02 | .03 | .04 | .05 | .06 | .07 | .08 | .09 | . | … | … | … | … | … | … | … | … | … | … | … | . | -2.1 | .01786 | .01743 | .01700 | .01659 | .01618 | .01578 | .01539 | .01500 | .01463 | .01426 | . | -2.0 | .02275 | .02222 | .02169 | .02118 | .02068 | .02018 | .01970 | .01923 | .01876 | .01831 | . | -1.9 | .02872 | .02807 | .02743 | .02680 | .02619 | .02559 | .02500 | .02442 | .02385 | .02330 | . | … | … | … | … | … | … | … | … | … | … | … | . Let’s use the z-score of 1.96 (absolute) as an example. | Find the row with the z-score of $1.9$ | Find the column with the z-score of $0.06$ | You can see that the probability is $0.025$ | . Since the normal distribution is symmetric, we know that the probability of a value being greater than $1.96$ is also $0.025$. Hence, the probability of a z-score falling outside of $\\pm 1.96$ is $0.05$. ",
    "url": "/docs/statistics/basics/basic-probs.html#standard-normal-distribution",
    "relUrl": "/docs/statistics/basics/basic-probs.html#standard-normal-distribution"
  },"83": {
    "doc": "Basic Probability",
    "title": "Others",
    "content": "To be added . | Uniform distribution (discrete and continuous) | Binomial distribution (discrete) | Poisson distribution (discrete) | Negative binomial distribution (discrete) | Exponential distribution (continuous) | etc. | . ",
    "url": "/docs/statistics/basics/basic-probs.html#others",
    "relUrl": "/docs/statistics/basics/basic-probs.html#others"
  },"84": {
    "doc": "Basic Probability",
    "title": "Basic Probability",
    "content": " ",
    "url": "/docs/statistics/basics/basic-probs.html",
    "relUrl": "/docs/statistics/basics/basic-probs.html"
  },"85": {
    "doc": "Basic Stats",
    "title": "Basic Statistical Concepts",
    "content": ". | Variable . | Quantitative Variable . | Discrete Variable | Continuous Variable | . | Qualitative Variable (Categorical Variable) | . | Statistic . | Representative Value . | Mean | Median | Mode | . | Dispersion . | Variance | Standard Deviation | . | . | Visualizing Distribution of Data . | Histogram . | For Discrete Data | For Continuous Data | . | Box and Whisker Plot . | Quartile | Box | Whiskers | Outliers | . | Error Bar | Violin Plot | Swarm Plot | . | . ",
    "url": "/docs/statistics/basics/basic-stats.html#basic-statistical-concepts",
    "relUrl": "/docs/statistics/basics/basic-stats.html#basic-statistical-concepts"
  },"86": {
    "doc": "Basic Stats",
    "title": "Variable",
    "content": "A variable is a characteristic of an object or an event. It is a set of values obtained by a common measurement of the characteristic. A data can be univariate or multivariate depending on the number of variables. The number of variables is sometimes called the dimension of the data. There are two types of variables: quantitative and qualitative. ",
    "url": "/docs/statistics/basics/basic-stats.html#variable",
    "relUrl": "/docs/statistics/basics/basic-stats.html#variable"
  },"87": {
    "doc": "Basic Stats",
    "title": "Quantitative Variable",
    "content": "A quantitative variable is a variable that can be measured numerically. Discrete Variable . A discrete variable is a variable that can only take on a finite number of values. Continuous Variable . A continuous variable is a variable that can take on an infinite number of values. ",
    "url": "/docs/statistics/basics/basic-stats.html#quantitative-variable",
    "relUrl": "/docs/statistics/basics/basic-stats.html#quantitative-variable"
  },"88": {
    "doc": "Basic Stats",
    "title": "Qualitative Variable (Categorical Variable)",
    "content": "A qualitative variable is a variable that cannot be measured numerically. It is also called a categorical variable. Examples of qualitative variable include: yes/no, gender, country, etc. ",
    "url": "/docs/statistics/basics/basic-stats.html#qualitative-variable-categorical-variable",
    "relUrl": "/docs/statistics/basics/basic-stats.html#qualitative-variable-categorical-variable"
  },"89": {
    "doc": "Basic Stats",
    "title": "Statistic",
    "content": "A statistic is a number obtained via some calculations on the data. Such calculations are called descriptive statistics or summary statistics. If histogram is a visualization of the distribution of data, then statistic is a numerical characterization of the data. Obviously enough, descriptive statistics is mostly performed on quantitative variables. Some common descriptive statistics include: . | Representative Value: shows the tendency of the data in a distribution . | Mean | Median | Mode | . | Dispersion: shows the spread of the data in a distribution . | Variance | Standard Deviation | . | . ",
    "url": "/docs/statistics/basics/basic-stats.html#statistic",
    "relUrl": "/docs/statistics/basics/basic-stats.html#statistic"
  },"90": {
    "doc": "Basic Stats",
    "title": "Representative Value",
    "content": "In a perfectly normal distribution, the mean, median, and mode are all the same. However, if the distribution is skewed, these representative values will be different. We can use these values to understand the tendency of the data, hence the name “representative value”. The representative value is not always the best way to understand the data. For example, if the data is skewed, the mean will be greatly affected by the outliers. Therefore it is important to understand the distribution of the data with the help of a histogram, before using the representative value to understand the data. Mean . The mean is the average of the data. Also called the sample mean or the arithmetic mean. For a quantitative variable $x$ with a sample of size $n$, the mean is: . $$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i $$ . We use the bar above $\\bar{x}$ to denote the mean of a sample. The mean of a population is denoted by $\\mu$. Median . The median is the middle value of the data. When the sample size is even, the median is the average of the two middle values. Due to its nature, unlike the mean, the median is not greatly affected by outliers. Mode . The mode is the most frequent value of the data. When the sample is from a continuous variable, we must first group the data into bins just like in a histogram. This statistic is not used as often as the mean or the median. ",
    "url": "/docs/statistics/basics/basic-stats.html#representative-value",
    "relUrl": "/docs/statistics/basics/basic-stats.html#representative-value"
  },"91": {
    "doc": "Basic Stats",
    "title": "Dispersion",
    "content": "Variance . The variance is the average of the squared difference between each value and the mean. $$ s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\tag{unbiased variance} $$ . You may be wondering why we divide by $n-1$ instead of $n$. Refer to this link for details. But in short, it is to make the sample variance an unbiased estimator of the population variance. Division by $n$ is good enough when we’re only measuring the dispersion in descriptive statistics, but when we’re using the statistic to estimate the population parameter in inferential statistics, it results in an underestimation of the population variance/standard deviation. To emphasize that the statistic comes from a sample, we sometimes call it the sample variance. Some interesting properties of the variance/standard deviation: . | $s^2 \\geq 0$ | $s^2 = 0$ if and only if all values are the same | Higher the dispersion, higher the variance | . Standard Deviation . The standard deviation is the square root of the variance. $$ s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\tag{unbiased standard deviation} $$ . Population standard deviation is denoted by $\\sigma$. ",
    "url": "/docs/statistics/basics/basic-stats.html#dispersion",
    "relUrl": "/docs/statistics/basics/basic-stats.html#dispersion"
  },"92": {
    "doc": "Basic Stats",
    "title": "Visualizing Distribution of Data",
    "content": "The distribution of data is the pattern of the data. By looking at the distribution, we can understand its tendency and dispersion. ",
    "url": "/docs/statistics/basics/basic-stats.html#visualizing-distribution-of-data",
    "relUrl": "/docs/statistics/basics/basic-stats.html#visualizing-distribution-of-data"
  },"93": {
    "doc": "Basic Stats",
    "title": "Histogram",
    "content": "Histogram is one common way to visualize the distribution of data. It is a bar graph that shows the frequency of each value. The purpose of a histogram is to understand the general overview of the data, not to confirm any specific observation. For Discrete Data . For discrete data, the histogram is a bar graph with the value on the x-axis and the frequency on the y-axis. For Continuous Data . For continuous data, the histogram is a bar graph with the range of values (“bin width”) on the x-axis and the frequency on the y-axis. It is important to choose the bin width wisely, because it can greatly affect the shape of the histogram and hence the interpretation of the data. ",
    "url": "/docs/statistics/basics/basic-stats.html#histogram",
    "relUrl": "/docs/statistics/basics/basic-stats.html#histogram"
  },"94": {
    "doc": "Basic Stats",
    "title": "Box and Whisker Plot",
    "content": "Box and whisker plot is another common way to visualize the distribution of data. Quartile . A quartile is a value that divides the data into four equal parts. To find the quartiles, we first need to sort the data in ascending order. Then, we can find the quartiles as follows: . | The first quartile $Q_1$ divides the data into the bottom 25%. | The second quartile $Q_2$ divides the data into the bottom 50%. | This is the same as the median. | . | The third quartile $Q_3$ divides the data into the bottom 75%. | . Box . The box in the box and whisker plot is a rectangle that shows the quartiles. The length of the box is the interquartile range (IQR), which is the difference between the third and first quartiles: . $$ IQR = Q_3 - Q_1 $$ . The median is shown as a line inside the box. Whiskers . The whiskers in the box and whisker plot are the lines that extend from the box. The ends of the whiskers end at an observed data point. There are many different ways to define the whiskers. One way to draw the whiskers is to extend them to the minimum and maximum values. Another way is to extend them to the values that are within 1.5 times the IQR, and all other values are considered outliers. Whiskers are not always drawn in the same way. They may be even or uneven depending on the definition and data. Outliers . An outlier is a data point that is far away from the rest of the data. There is really no single definition of what an outlier exactly is. It could be points that are outside the whiskers, or data points that are more than 2 or 3 standard deviations away from the mean. ",
    "url": "/docs/statistics/basics/basic-stats.html#box-and-whisker-plot",
    "relUrl": "/docs/statistics/basics/basic-stats.html#box-and-whisker-plot"
  },"95": {
    "doc": "Basic Stats",
    "title": "Error Bar",
    "content": "Error bar is a line that shows the range of values. Depending on the context, the range may be the standard deviation, the standard error, or the confidence interval. Because it can mean different things, it is important to describe via a legend what the error bar represents. In descriptive statistics, the error bar is often used to show the standard deviation. In this case, the longer the error bar, the higher the dispersion. In inferential statistics, the error bar is often used to show the standard error or the confidence interval. In this case, the error bars shows you a sense of “significant difference” between two groups. ",
    "url": "/docs/statistics/basics/basic-stats.html#error-bar",
    "relUrl": "/docs/statistics/basics/basic-stats.html#error-bar"
  },"96": {
    "doc": "Basic Stats",
    "title": "Violin Plot",
    "content": "Violin plot is a combination of a box and whisker plot and a plot of a probability density function. Read more about violin plots here. ",
    "url": "/docs/statistics/basics/basic-stats.html#violin-plot",
    "relUrl": "/docs/statistics/basics/basic-stats.html#violin-plot"
  },"97": {
    "doc": "Basic Stats",
    "title": "Swarm Plot",
    "content": "Each data point is plotted along an axis in a swarm plot. Can be used with other plots such as box and whisker plot. ",
    "url": "/docs/statistics/basics/basic-stats.html#swarm-plot",
    "relUrl": "/docs/statistics/basics/basic-stats.html#swarm-plot"
  },"98": {
    "doc": "Basic Stats",
    "title": "Basic Stats",
    "content": ". ",
    "url": "/docs/statistics/basics/basic-stats.html",
    "relUrl": "/docs/statistics/basics/basic-stats.html"
  },"99": {
    "doc": "Terraform Basics",
    "title": "Terraform Basics",
    "content": ". | Install Terraform | Configuration | Initialize | Create infrastructure and inspect state | Output file | Destroy infrastructure | Refresh infrastructure | Workspaces . | To create a new workspace | To switch to a workspace | . | Import remote infrastructure | To see the current configuration state of a resource | To delete a resource from the state | . ",
    "url": "/docs/terraform/basics.html",
    "relUrl": "/docs/terraform/basics.html"
  },"100": {
    "doc": "Terraform Basics",
    "title": "Install Terraform",
    "content": "brew tap hashicorp/tap brew install hashicorp/terraform terraform -version . ",
    "url": "/docs/terraform/basics.html#install-terraform",
    "relUrl": "/docs/terraform/basics.html#install-terraform"
  },"101": {
    "doc": "Terraform Basics",
    "title": "Configuration",
    "content": "The set of files used to declare infrastructure. Such files have an extension of .tf and are required to be in its own working directory. mkdir tf-aws-instance cd tf-aws-instance touch main.tf . The following is an example configuration main.tf: . terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~&gt; 3.27\" } } required_version = \"&gt;= 0.14.9\" } provider \"aws\" { profile = \"default\" region = \"us-west-2\" } resource \"aws_instance\" \"app_server\" { ami = \"ami-830c94e3\" instance_type = \"t2.micro\" tags = { Name = \"ExampleAppServerInstance\" } } . Terraform also provides terraform fmt and terraform validate for formatting configuration files and checking its syntax. terraform fmt does not produce any output if no modification is made. For details, see Terraform Configuration. ",
    "url": "/docs/terraform/basics.html#configuration",
    "relUrl": "/docs/terraform/basics.html#configuration"
  },"102": {
    "doc": "Terraform Basics",
    "title": "Initialize",
    "content": "After creating a configuration or checking out an existing configuration, initialize directory with . # Installs providers in .terraform folder and also creates .terraform.lock.hcl terraform init . ",
    "url": "/docs/terraform/basics.html#initialize",
    "relUrl": "/docs/terraform/basics.html#initialize"
  },"103": {
    "doc": "Terraform Basics",
    "title": "Create infrastructure and inspect state",
    "content": "To see the execution plan, . terraform plan . To actually apply, . # Will print an execution plan, type yes to perform the actions terraform apply # OR terraform apply --auto-approve # With variables terraform apply -var-file=variables.tfvars . A Terraform state file terraform.tfstate will be generated. The file contains sensitive info, so share with only those trusted. # Inspect the current state terraform show . For manual/advanced state management, use terraform state. One example of the command is, . # List resources in state terraform state list . ",
    "url": "/docs/terraform/basics.html#create-infrastructure-and-inspect-state",
    "relUrl": "/docs/terraform/basics.html#create-infrastructure-and-inspect-state"
  },"104": {
    "doc": "Terraform Basics",
    "title": "Output file",
    "content": "You can query data after apply using an output file. Create a file called output.tf (name doesn’t matter) with the following . output \"instance_id\" { description = \"ID of the EC2 instance\" value = aws_instance.app_server.id } output \"instance_public_ip\" { description = \"Public IP address of the EC2 instance\" value = aws_instance.app_server.public_ip } . You will see the queried output when you run terraform apply. You can also inspect the output by . # Call after `terraform apply` terraform output . ",
    "url": "/docs/terraform/basics.html#output-file",
    "relUrl": "/docs/terraform/basics.html#output-file"
  },"105": {
    "doc": "Terraform Basics",
    "title": "Destroy infrastructure",
    "content": "The following terminates all resources managed with project state: . # Just like apply, shows you the execution plan. Type yes to destroy. terraform destroy # OR terraform destroy --auto-approve # With variables terraform destroy -var-file=variables.tfvars . ",
    "url": "/docs/terraform/basics.html#destroy-infrastructure",
    "relUrl": "/docs/terraform/basics.html#destroy-infrastructure"
  },"106": {
    "doc": "Terraform Basics",
    "title": "Refresh infrastructure",
    "content": "The following updates terraform’s state file to match the configuration in remote: . terraform refresh terraform refresh -var-file=variables.tf . ",
    "url": "/docs/terraform/basics.html#refresh-infrastructure",
    "relUrl": "/docs/terraform/basics.html#refresh-infrastructure"
  },"107": {
    "doc": "Terraform Basics",
    "title": "Workspaces",
    "content": "If you want to work on multiple stages, use workspaces to manage different states. By default, you work in a workspace named default. All the other non-default workspace states are stored in a directory named terraform.tfstate.d. To create a new workspace . terraform workspace new my-dev . To switch to a workspace . terraform workspace select default . ",
    "url": "/docs/terraform/basics.html#workspaces",
    "relUrl": "/docs/terraform/basics.html#workspaces"
  },"108": {
    "doc": "Terraform Basics",
    "title": "Import remote infrastructure",
    "content": "To import a remote infrastructure into a local state file, first create an appropriate empty resource in a configuration file: . resource \"aws_s3_bucket\" \"my_bucket\" { } . Then, import the remote resource into the local state file: . terraform import aws_s3_bucket.my_bucket my-remote-bucket-name . Note that the id/key used for an import varies per provider/resource. Refer to the documentation for the provider to see the correct syntax. However, doing so does not actually update the configuration itself, but only updates the state file. To actually bring the remote resource under Terraform’s management, you must copy over the configurations and run terraform apply. Easiest way to see the current configuration is to use terraform state show. ",
    "url": "/docs/terraform/basics.html#import-remote-infrastructure",
    "relUrl": "/docs/terraform/basics.html#import-remote-infrastructure"
  },"109": {
    "doc": "Terraform Basics",
    "title": "To see the current configuration state of a resource",
    "content": "terraform state show aws_s3_bucket.my_bucket . ",
    "url": "/docs/terraform/basics.html#to-see-the-current-configuration-state-of-a-resource",
    "relUrl": "/docs/terraform/basics.html#to-see-the-current-configuration-state-of-a-resource"
  },"110": {
    "doc": "Terraform Basics",
    "title": "To delete a resource from the state",
    "content": "terraform state rm aws_s3_bucket.my_bucket . References: . | Terraform: AWS Get Started | Terraform Registry: AWS Provider | . ",
    "url": "/docs/terraform/basics.html#to-delete-a-resource-from-the-state",
    "relUrl": "/docs/terraform/basics.html#to-delete-a-resource-from-the-state"
  },"111": {
    "doc": "SSH Basics",
    "title": "SSH Basics",
    "content": ". | Create a key pair | Copy public key to remote server | . ",
    "url": "/docs/security/ssh/basics.html",
    "relUrl": "/docs/security/ssh/basics.html"
  },"112": {
    "doc": "SSH Basics",
    "title": "Create a key pair",
    "content": "Base command structure is as follows: . ssh-keygen -t &lt;algorithm&gt; -C &lt;comments&gt; . Then follow the prompt. Do not use ssh-rsa algorithm as it is rejected by modern OpenSSH servers. Use rsa-sha2-256, rsa-sha2-512, ed25519, etc. instead. ",
    "url": "/docs/security/ssh/basics.html#create-a-key-pair",
    "relUrl": "/docs/security/ssh/basics.html#create-a-key-pair"
  },"113": {
    "doc": "SSH Basics",
    "title": "Copy public key to remote server",
    "content": "The following command will copy your public key to remote server’s authorized_keys file. It will also check for duplicate keys and will not add the key if it is. ssh-copy-id -i &lt;path_to_pub_key&gt; &lt;remote_host&gt; . References: . | Comparing SSH Keys | . ",
    "url": "/docs/security/ssh/basics.html#copy-public-key-to-remote-server",
    "relUrl": "/docs/security/ssh/basics.html#copy-public-key-to-remote-server"
  },"114": {
    "doc": "Vault Server Basics",
    "title": "Vault Server Basics",
    "content": ". | Vault server | Initialization . | GnuPG | Vault init | . | Unseal / Seal . | Unseal | Seal | . | Enabling authentication . | Enable userpass | . | Policies . | Create a policy | Add a policy | . | . ",
    "url": "/docs/security/vault/basics.html",
    "relUrl": "/docs/security/vault/basics.html"
  },"115": {
    "doc": "Vault Server Basics",
    "title": "Vault server",
    "content": "When you first start the server with vault server, you need to first initialize and unseal it. One option is to use the web UI, but you can also use the vault CLI. You can access the web UI at http://localhost:8200/ui. ",
    "url": "/docs/security/vault/basics.html#vault-server",
    "relUrl": "/docs/security/vault/basics.html#vault-server"
  },"116": {
    "doc": "Vault Server Basics",
    "title": "Initialization",
    "content": "In the beginning, Vault server is in a sealed state. There needs to be a master key to unseal it. Upon initialization, Vault will attempt to split this key into pieces, and you will get to decide how many pieces it’ll be. You will also have to decide the threshold of number of pieces that need to be put together in order to access the final key and unseal Vault. One security hole of Vault was that the root initializer receives a raw text of all these keys. Therefore, a recommended practice is that you encrypt these keys using a PGP key. Have a PGP key for each piece of the key. GnuPG . One way to do acquire a PGP key is with GnuPG. # Follow prompts to create a PGP key gpg --full-generate-key # Export to disk as base64 gpg --export &lt;key-id&gt; | base64 &gt; my-name.asc . Vault init . Now define a key share number and the threshold, and provide your PGP keys. vault operator init \\ -key-shares=3 \\ -key-threshold=2 \\ -pgp-keys=\"person1.asc,person2.asc,person3.asc\" -root-token-pgp-key=\"some.asc\" . Then you will get, in this example, three PGP encrypted keys and a root token in your console. Remember these encrypted keys and token. The order in which you put pgp-keys matter. The first PGP key will be used to decrypt the first unseal key, the second will be used to decrypt the second, and so on. ",
    "url": "/docs/security/vault/basics.html#initialization",
    "relUrl": "/docs/security/vault/basics.html#initialization"
  },"117": {
    "doc": "Vault Server Basics",
    "title": "Unseal / Seal",
    "content": "Unseal . To decrypt an unseal key, . echo \"whatever that was printed during init\" | base64 --decode | gpg -dq . The output will be the decrypted key. Then unseal vault, . vault operator unseal # Enter decrypted key on prompt . Seal . vault operator seal . ",
    "url": "/docs/security/vault/basics.html#unseal--seal",
    "relUrl": "/docs/security/vault/basics.html#unseal--seal"
  },"118": {
    "doc": "Vault Server Basics",
    "title": "Enabling authentication",
    "content": "You can always login with a root token, . vault login &lt;root-token&gt; . But it is generally a bad idea to persist a root token. Therefore, we instead enable different authentication methods to login to Vault. The simplest auth method is userpass. Enable userpass . # By default it is mounted to auth/userpass vault auth enable userpass # You can set your own path though vault auth enable -path=my-path-here userpass . Before you enable any other auth methods, initially you’ll have to be logged in with your root token. Then create a user by: . vault write auth/userpass/users/&lt;username&gt; \\ password=&lt;password&gt; \\ policies=\"list,of,policies,separated,by,comma\" . You can now login with the created user: . vault login -method=\"userpass\" username=\"&lt;username&gt;\" . ",
    "url": "/docs/security/vault/basics.html#enabling-authentication",
    "relUrl": "/docs/security/vault/basics.html#enabling-authentication"
  },"119": {
    "doc": "Vault Server Basics",
    "title": "Policies",
    "content": "Create a policy . Create an .hcl file. The name is irrelevant. Details of what goes into this file can be found here. Add a policy . To add a policy: . vault policy write &lt;policy-name&gt; some-policy.hcl . References: . | Vault Server | Vault PGP | Vault Auth Methods | Vault Policies | . ",
    "url": "/docs/security/vault/basics.html#policies",
    "relUrl": "/docs/security/vault/basics.html#policies"
  },"120": {
    "doc": "Basics",
    "title": "Kubernetes Basics",
    "content": ". | What is Kubernetes? | Installation | Managed Kubernetes services from cloud providers | . ",
    "url": "/docs/kubernetes/basics.html#kubernetes-basics",
    "relUrl": "/docs/kubernetes/basics.html#kubernetes-basics"
  },"121": {
    "doc": "Basics",
    "title": "What is Kubernetes?",
    "content": ". | Container orchestration system | Declarative API | . ",
    "url": "/docs/kubernetes/basics.html#what-is-kubernetes",
    "relUrl": "/docs/kubernetes/basics.html#what-is-kubernetes"
  },"122": {
    "doc": "Basics",
    "title": "Installation",
    "content": "Details here. brew install kubernetes-cli . Check installation via kubectl version --output=yaml. ",
    "url": "/docs/kubernetes/basics.html#installation",
    "relUrl": "/docs/kubernetes/basics.html#installation"
  },"123": {
    "doc": "Basics",
    "title": "Managed Kubernetes services from cloud providers",
    "content": ". | Google Kubernetes Engine (GKE) | Amazon Elastic Kubernetes Service (Amazon EKS) | Azure Kubernetes Service (AKS) | . ",
    "url": "/docs/kubernetes/basics.html#managed-kubernetes-services-from-cloud-providers",
    "relUrl": "/docs/kubernetes/basics.html#managed-kubernetes-services-from-cloud-providers"
  },"124": {
    "doc": "Basics",
    "title": "Basics",
    "content": " ",
    "url": "/docs/kubernetes/basics.html",
    "relUrl": "/docs/kubernetes/basics.html"
  },"125": {
    "doc": "Elastic Beanstalk",
    "title": "AWS Elastic Beanstalk (EB)",
    "content": ". | What is Elastic Beanstalk? | Elastic Beanstalk Application | Environment Tier . | Web Server Environment | Worker Environment | . | Deployment . | In-Place Deployment Policies . | All at once | Rolling | Rolling with additional batch | Immutable | Traffic Splitting | . | Blue/Green Deployment Policy | . | Configuring Environments . | Order of Precedence | Configuration Files (.ebextensions) . | Option Settings | Linux Server | . | Environment Manifest (env.yaml) | . | EB CLI | . ",
    "url": "/docs/aws/beanstalk.html#aws-elastic-beanstalk-eb",
    "relUrl": "/docs/aws/beanstalk.html#aws-elastic-beanstalk-eb"
  },"126": {
    "doc": "Elastic Beanstalk",
    "title": "What is Elastic Beanstalk?",
    "content": "Elastic Beanstalk is a Platform as a Service (PaaS) that helps you deploy web apps with little knowledge about what kind of infrastructure is managed underneath. It configures its components to provide an environment for your application to run on. EB is basically a Cloudformation template with a UI. ",
    "url": "/docs/aws/beanstalk.html#what-is-elastic-beanstalk",
    "relUrl": "/docs/aws/beanstalk.html#what-is-elastic-beanstalk"
  },"127": {
    "doc": "Elastic Beanstalk",
    "title": "Elastic Beanstalk Application",
    "content": "An Elastic Beanstalk application is a logical collection of application version and environments. ",
    "url": "/docs/aws/beanstalk.html#elastic-beanstalk-application",
    "relUrl": "/docs/aws/beanstalk.html#elastic-beanstalk-application"
  },"128": {
    "doc": "Elastic Beanstalk",
    "title": "Environment Tier",
    "content": "When you create an EB application, you are asked to choose an environment tier. This tier determines which resources EB should provision to form your environment. When creating a web app, you often require both environment tiers. Web Server Environment . Key resources launched in the EB container: . | Elastic Load Balancer | EC2 (Auto Scaling Group, Security Group) | . A web server environment serves HTTP requests. Web server environment is given a URL of myapp.region.elasticbeanstalk.com. This environment creates an Elastic Load Balancer with a URL of of elb-id.region.elb.amazonaws.com. In Amazon Route 53, this ELB URL has a CNAME Record to the environment URL. This ELB sits in front of EC2 instances in a Auto Scaling Group (ASG). The stack on EC2 instances depends on which platform you chose (eg. Python 3.8 running on 64bit Amazon Linux 2). However, in each instance sits one common component called the host manager (HM). HM manages all sorts of monitoring, deploying, and metrics related to the instance. By default, EC2 instances are placed in a security group which allows all connection through port 80 (HTTP). Additional security groups maybe configured as needed. Worker Environment . Key resources launched in EB container: . | SQS | EC2 (Auto Scaling Group), Sqsd | Cloudwatch | . Worker environment is usually set up for long running tasks to run in the background. A worker environment sets up an Amazon SQS queue. This queue often consists of messages from a web server environment. On each EC2 instance runs a Sqsd daemon and a processing application. The daemon reads the message from the SQS queue and sends it as an HTTP POST request to the processing application. Upon a 200 OK response from the processing application, Sqsd sends a delete message call to SQS. EC2 instances publish their metrics to Amazon Cloudwatch. Auto Scaling retrieves usage data from Cloudwatch and scales instances accordingly. ",
    "url": "/docs/aws/beanstalk.html#environment-tier",
    "relUrl": "/docs/aws/beanstalk.html#environment-tier"
  },"129": {
    "doc": "Elastic Beanstalk",
    "title": "Deployment",
    "content": "Each deployment is identified with a deployment ID which increments from 1. In-Place Deployment Policies . All at once . Every instance is killed and updated at the same time. The deployment is quick in that sense, but it results in a short loss of service. Also, it can be dangerous in case of a failure to deploy, and may be tricky to rollback. Rolling . Updates one batch of instances at a time. So a batch can be down during an update which may result in reduced availability for a short time. However, there is no downtime unlike ‘All at once’, but the entire deployment process takes a longer time. Rolling with additional batch . To avoid any reduced bandwidth in regular rolling deployment, an extra batch of instances is launched and rolling update is performed there. Hence, the number of instances up during deployment stays the same. This takes longer time. Immutable . Instead of updating instances, a complete new Auto Scaling Group set of instances is created. This is even slower. Traffic Splitting . Create a new set of instances and test it with a portion of the incoming traffic, while the rest of the traffic is still going to the old deployment version. This is as slow as ‘Immutable’. Blue/Green Deployment Policy . One additional deployment option is the Blue/Green deployment. All the other deployment policies above performs an In-Place deployment, which means the update happens within an EB environment. However, Blue/Green deployment goes beyond the instances inside the environment. To avoid downtime, your deployment is launched to a complete new set of environment and then the CNAMEs of old and new environments are swapped to redirect traffic instantly. ",
    "url": "/docs/aws/beanstalk.html#deployment",
    "relUrl": "/docs/aws/beanstalk.html#deployment"
  },"130": {
    "doc": "Elastic Beanstalk",
    "title": "Configuring Environments",
    "content": "There are many different ways to configure environments. Order of Precedence . | Settings applied directly during create/update environment | Saved configuration objects in S3 | Configuration files (.ebextensions, env.yaml) | Default values | . Configuration Files (.ebextensions) . You can place .config files in a folder .ebextensions at the root of the application source bundle. Each .config files are applied in alphabetical order. YAML is recommended for configuration files but both YAML and JSON are supported. Option Settings . Use option_settings key to configure environment options . option_settings: - namespace: namespace option_name: option name value: option value . Linux Server . You can also configure the software running on your instances. Check these link1, link2 for details. Environment Manifest (env.yaml) . Place an env.yaml file at the root of the application source bundle to configure the environment. You can configure the name, solution stack, and links to other environments. There are some overlaps between .configs and env.yaml. It seems env.yaml is more environment specific, while .config files can handle overall configuration of the application. Check the link for details. ",
    "url": "/docs/aws/beanstalk.html#configuring-environments",
    "relUrl": "/docs/aws/beanstalk.html#configuring-environments"
  },"131": {
    "doc": "Elastic Beanstalk",
    "title": "EB CLI",
    "content": "EB CLI is an open-source project hosted in this repository. To use the CLI application, however, clone this setup repository instead. References: . | Web Server Environment] | Worker Environment] | Deployments | Configuring Environment | EB CLI | . ",
    "url": "/docs/aws/beanstalk.html#eb-cli",
    "relUrl": "/docs/aws/beanstalk.html#eb-cli"
  },"132": {
    "doc": "Elastic Beanstalk",
    "title": "Elastic Beanstalk",
    "content": " ",
    "url": "/docs/aws/beanstalk.html",
    "relUrl": "/docs/aws/beanstalk.html"
  },"133": {
    "doc": "Bias-Variance Tradeoff",
    "title": "Bias-Variance Tradeoff",
    "content": "Bias-variance tradeoff describes a dilemma in model training. When a model is more complex, it becomes more flexible and lowers the bias of the model. Remember that bias is the difference between the expected value of the estimator and the true value. So basically, the error. However, as the model becomes more complex, sensitivity to different training data increases, which increases the variance of the model. High variance of a model is a sign of overfitting. On the other hand, if you aim for low variance in training, you end up with a high bias. ",
    "url": "/docs/statistics/notes/bias-variance-trade.html",
    "relUrl": "/docs/statistics/notes/bias-variance-trade.html"
  },"134": {
    "doc": "Binomial Test",
    "title": "Binomial Test",
    "content": ". | Binomial Distribution . | Probability Mass Function | . | Hypothesis Test . | Null Hypothesis | Calculating the p-value | . | . ",
    "url": "/docs/statistics/basics/hypothesis-test/binomial-test.html",
    "relUrl": "/docs/statistics/basics/hypothesis-test/binomial-test.html"
  },"135": {
    "doc": "Binomial Test",
    "title": "Binomial Distribution",
    "content": "Binomial distribution is a discrete probability distribution that describes the probability of a success in a binomial experiment. Each yes-no experiment is also called a Bernoulli trial: . | Each trial has binary outcome: success or failure | Each trial is independent of each other | The probability of success is the same for each trial, denoted as $p$ | The number of trials is fixed, denoted as $n$ | . When probability of success $p$ is: . | $p = 0.5$, the distribution is symmetric | $p &lt; 0.5$, the distribution is skewed to the left | $p &gt; 0.5$, the distribution is skewed to the right | . ",
    "url": "/docs/statistics/basics/hypothesis-test/binomial-test.html#binomial-distribution",
    "relUrl": "/docs/statistics/basics/hypothesis-test/binomial-test.html#binomial-distribution"
  },"136": {
    "doc": "Binomial Test",
    "title": "Probability Mass Function",
    "content": "Let $X$ be the random variable that represents the number of successes in $n$ trials, and $p$ be the probability of success in each trial. Then, the probability mass function of $X$ is: . $$ Pr\\left[X = k\\right] = \\binom{n}{k} p^k (1-p)^{n-k} $$ . $$ \\tag{Binomial Coefficient} \\binom{n}{k} = \\frac{n!}{k!(n-k)!} $$ . where $k$ is the number of successes in $n$ trials. Then we say that $X$ follows a binomial distribution with parameters $n$ and $p$: . $$ X \\sim B(n, p) $$ . ",
    "url": "/docs/statistics/basics/hypothesis-test/binomial-test.html#probability-mass-function",
    "relUrl": "/docs/statistics/basics/hypothesis-test/binomial-test.html#probability-mass-function"
  },"137": {
    "doc": "Binomial Test",
    "title": "Hypothesis Test",
    "content": " ",
    "url": "/docs/statistics/basics/hypothesis-test/binomial-test.html#hypothesis-test",
    "relUrl": "/docs/statistics/basics/hypothesis-test/binomial-test.html#hypothesis-test"
  },"138": {
    "doc": "Binomial Test",
    "title": "Null Hypothesis",
    "content": "Let $P$ be the probability of success in a binomial experiment. Supose we were testing whether a coin is fair or not. Then, the null hypothesis would be: . $$ H_0: P = 0.5 $$ . Meaning the chances of getting heads or tails are equal. ",
    "url": "/docs/statistics/basics/hypothesis-test/binomial-test.html#null-hypothesis",
    "relUrl": "/docs/statistics/basics/hypothesis-test/binomial-test.html#null-hypothesis"
  },"139": {
    "doc": "Binomial Test",
    "title": "Calculating the p-value",
    "content": "When the null hypothesis is true, we can plug in $P = 0.5$ into the probability mass function: . \\[Pr\\left[X = k\\right] = \\binom{n}{k} 0.5^n\\] Suppose our sample had 21 heads out of 30 trials. Now we want to calculate our p-value which is the probability of observing an outcome as extreme as the one we observed, in a binomial distribution with $n = 30$ and $p = 0.5$. The probability of this extreme case is the sum of the probabilities of getting 21 or more heads or 9 or less heads: . \\[Pr\\left[X \\geq 21\\right] + Pr\\left[X \\leq 9\\right] = \\sum_{k=21}^{30} \\binom{30}{k} 0.5^{30} + \\sum_{k=0}^{9} \\binom{30}{k} 0.5^{30} \\approx 0.043\\] Remember, with two-tailed test, we need to take both extremes into account. Based on the results, we can reject the null hypothesis as $0.043 &lt; 0.05$. ",
    "url": "/docs/statistics/basics/hypothesis-test/binomial-test.html#calculating-the-p-value",
    "relUrl": "/docs/statistics/basics/hypothesis-test/binomial-test.html#calculating-the-p-value"
  },"140": {
    "doc": "SSH Certificates",
    "title": "SSH Certificates",
    "content": ". | SSH authentication methods | SSH certificates | Host certificate . | Configure a host CA | Sign the host key with CA | Save host CA public key on client | . | Client certificate . | Configure a client CA | Sign the client key with CA | Save client CA public key on host | . | Host sshd_config settings | Test connection | Debugging . | From the client side | From the host side . | Linux | macOS | . | To check certificate metadata | . | SSH certificate extensions | . ",
    "url": "/docs/security/ssh/cert.html",
    "relUrl": "/docs/security/ssh/cert.html"
  },"141": {
    "doc": "SSH Certificates",
    "title": "SSH authentication methods",
    "content": "There are two popular ways to SSH into a server. | Use a system password | Use SSH key | . Using an SSH key adds more security layers compared to using a system password. However, even with an SSH key, there still exists a problem that once an SSH key is placed on the authorized_keys of the server, it is permanent unless someone actively removes it. This may not matter if you’re the one and only person trying to SSH into a server, but if the number of people that need to be given or revoked access increases, this can become a hassle. Using SSH certificates can alleviate these issues. ",
    "url": "/docs/security/ssh/cert.html#ssh-authentication-methods",
    "relUrl": "/docs/security/ssh/cert.html#ssh-authentication-methods"
  },"142": {
    "doc": "SSH Certificates",
    "title": "SSH certificates",
    "content": "SSH certificate authentication goes in both directions. The server/host presents its certificate to the user/client, and vice versa. The whole idea of a certificate is simple: . | Both parties agree on some authority and trust what it signs. | . Such authority is called a Certificate Authority (CA). Technically, our CA is just a pair of cryptographic keys. So instead of trusting each user keys, parties will decide trust the CA that will sign those keys. This eliminates having to accumulate public keys in authorized_keys. In addition, CAs can set a expiration date on their signatures. So if you decide to give someone a server access for only a single day, but don’t want to bother remembering to come back after a day to remove his/her key from your authorized_keys, you can have the CA sign the key to only be valid for a day. In the following example, I will be using Vault to manage these CAs. ",
    "url": "/docs/security/ssh/cert.html#ssh-certificates-1",
    "relUrl": "/docs/security/ssh/cert.html#ssh-certificates-1"
  },"143": {
    "doc": "SSH Certificates",
    "title": "Host certificate",
    "content": "Usually people dismiss the need for host certificates. However it is a good security layer to prevent SSHing into a bad machine. When you’re SSHing without a certificate, you’ve probably seen something like this. $ ssh server The authenticity of host 'badsiteindisguise.com' can't be established...cryptographic key fingerprint... Are you sure you want to continue connecting (yes/no/[fingerprint])? . Usually you just end up typing yes, which adds the fingerprint to ~/.ssh/known_hosts and you never see the prompt again. However, just like the prompt says, are you sure that this site can be trusted? Are you sure that there wasn’t a man in the middle that redirected you to one of his bad machines? . By placing a trusted host CA’s public key on the client before the first SSH, this security risk can be avoided. Configure a host CA . Create a key pair for host CA: . ssh-keygen -t ed25519 -C \"hostca\" -f hostca . Which produces hostca and hostca.pub. vault write ssh-host-signer/config/ca generate_signing_key=true currently only generates rsa keys, which is deprecated in newer OpenSSH. To use different crypto algorithms such as ed25519, you have to generate one and upload it to Vault. Mount the SSH secrets engine on Vault: . vault secrets enable -path=ssh-host-signer ssh . The path can be anything you like, just make sure you are logged in to Vault and has the policy to read/write to that path. Upload the CA key pair: . vault write ssh-host-signer/config/ca \\ private_key=@hostca \\ public_key=@hostca.pub . The @ points to the file. Use \"...\" to copy and paste the values. Extend the host key certificate TTL (time-to-live): . vault secrets tune -max-lease-ttl=87600h ssh-host-signer . 87600h is 10 years. Create a role to sign host keys: . vault write ssh-host-signer/roles/hostrole \\ key_type=ca \\ ttl=87600h \\ allow_host_certificates=true \\ allowed_domains=\"example.com,something.com\" \\ allow_bare_domains=true \\ allow_subdomains=true . Vault uses these roles to sign keys. The above configuration basically says it can sign host certificates for domains of example.com, *.example.com, something.com, *.something.com, and the certificate will be valid for 10 years. Sign the host key with CA . Sign and save the resulting certificate on the server: . vault write -field=signed_key ssh-host-signer/sign/hostrole \\ cert_type=host \\ public_key=@/etc/ssh/ssh_host_ed25519_key.pub &gt; /etc/ssh/ssh_host_ed25519_key-cert.pub . Optionally set permission on the certificate: . sudo chmod 0640 /etc/ssh/ssh_host_ed25519_key-cert.pub . If the host key doesn’t exist, create one using ssh-keygen. Update /etc/ssh/sshd_config: . HostKey /etc/ssh/ssh_ed25519_key HostCertificate /etc/ssh/ssh_ed25519_key-cert.pub . Save host CA public key on client . From the client, get the public key of ssh-host-signer CA: . # If using API endpoint curl &lt;vault-api-url&gt;/v1/ssh-host-signer/public-key . # If client has direct access to the Vault server vault read -field=public_key ssh-host-signer/config/ca . Save the result to client’s ~/.ssh/known_hosts: . @cert-authority *.example.com,*.something.com ssh-ed25519 ... If you have already logged in to the server before the host certificate was set up, remove the corresponding fingerprint in known_hosts. Try SSHing to the server with a password or a regular public key. Assuming you have never SSHed to the server before or have removed the previous fingerprint, SSH should not show you the Are you sure you want to continue prompt. If it does, the host certificate is not set up correctly. ",
    "url": "/docs/security/ssh/cert.html#host-certificate",
    "relUrl": "/docs/security/ssh/cert.html#host-certificate"
  },"144": {
    "doc": "SSH Certificates",
    "title": "Client certificate",
    "content": "Instead of having the client’s public key saved to host’s authorized_keys, we will have the client use a certificate to authenticate. Some of the process is actually very similar to above. Mostly the only difference is to use client or user instead of host. Configure a client CA . Create a key pair for client CA: . ssh-keygen -t ed25519 -C \"clientca\" -f clientca . Which produces clientca and clientca.pub. You can actually use the same pair of keys you used for host CA. Mount the SSH secrets engine on Vault: . vault secrets enable -path=ssh-client-signer ssh . Upload the CA key pair: . vault write ssh-client-signer/config/ca \\ private_key=@clientca \\ public_key=@clientca.pub . Create a role to sign client keys: . vault write ssh-client-signer/roles/clientrole -&lt;&lt;\"EOH\" { \"allow_user_certificates\": true, \"allowed_users\": \"my-user\", \"allowed_extensions\": \"permit-pty,permit-port-forwarding,permit-x11-forwarding,permit-agent-forwarding,permit-user-rc\", \"default_extensions\": [ { \"permit-pty\": \"\" } ], \"key_type\": \"ca\", \"default_user\": \"my-user\", \"ttl\": \"30m0s\" } EOH . | allowed_users: Comma separated list of allowed username | allowed_extensions: Comma separated list of extensions that client can request in their certificate | default_extensions: Default extensions given when this role signs a certificate | default_user: Username to use when one isn’t specified | ttl: Client certificate expires after ttl. | . Sign the client key with CA . Create an SSH key if one doesn’t already exist: . ssh-keygen -t ed25519 -C \"user@example.com\" -f client_key . Sign and save the resulting certificate on the client: . | To accept default | . vault write -field=signed_key ssh-client-signer/sign/clientrole \\ public_key=@client_key.pub &gt; client_key-cert.pub . | To customize | . vault write ssh-client-signer/sign/my-role -&lt;&lt;\"EOH\" { \"public_key\": \"ssh-ed25519 ...\", \"valid_principals\": \"my-user\", \"extensions\": { \"permit-pty\": \"\", \"permit-port-forwarding\": \"\" } } EOH . Then copy and paste the certificate to client_key-cert.pub. If your certificate ends in the &lt;same_base&gt;-cert.pub suffix, OpenSSH will automatically detect it so you won’t have to pass in your certificate as an identity file in addition to the private key. Save client CA public key on host . From the host, get the public key of ssh-client-signer CA and save it to /etc/ssh/trusted-user-ca-keys.pem: . # If using API endpoint curl -o /etc/ssh/trusted-user-ca-keys.pem http://127.0.0.1:8200/v1/ssh-client-signer/public_key . # If host has direct access to the Vault server vault read -field=public_key ssh-client-signer/config/ca &gt; /etc/ssh/trusted-user-ca-keys.pem . Now modify /etc/ssh/sshd_config on host: . # /etc/ssh/sshd_config TrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem . ",
    "url": "/docs/security/ssh/cert.html#client-certificate",
    "relUrl": "/docs/security/ssh/cert.html#client-certificate"
  },"145": {
    "doc": "SSH Certificates",
    "title": "Host sshd_config settings",
    "content": "To disable SSH password authentication, . # /etc/ssh/sshd_config PasswordAuthentication no ChallengeResponseAuthentication no . Recap: . HostKey /etc/ssh/ssh_ed25519_key HostCertificate /etc/ssh/ssh_ed25519_key-cert.pub . # /etc/ssh/sshd_config TrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem . ",
    "url": "/docs/security/ssh/cert.html#host-sshd_config-settings",
    "relUrl": "/docs/security/ssh/cert.html#host-sshd_config-settings"
  },"146": {
    "doc": "SSH Certificates",
    "title": "Test connection",
    "content": "Now the client should be able to authenticate to the server with certificates: . # If the certificate ends in '-cert.pub' with the same base name ssh -i ~/.ssh/client_key my-user@example.com . # If the certificate has a different naming scheme ssh -i ~/.ssh/client-certificate.pub -i ~/.ssh/client_key my-user@example.com . Connection is a success if you don’t see any fingerprint validation prompt and was able to connect without adding client_key.pub to host’s authorized_keys. ",
    "url": "/docs/security/ssh/cert.html#test-connection",
    "relUrl": "/docs/security/ssh/cert.html#test-connection"
  },"147": {
    "doc": "SSH Certificates",
    "title": "Debugging",
    "content": "From the client side . Add -vvv to get a verbose log output: . ssh -vvv -i ~/.ssh/client_key my-user@example.com . From the host side . Linux . Set the LogLevel in /etc/ssh/sshd_config to VERBOSE. Then inspect /var/log/auth.log. macOS . log show --process sshd --last &lt;num&gt; --debug --info . See log show help for details. To check certificate metadata . ssh-keygen -Lf ssh_key-cert.pub . ",
    "url": "/docs/security/ssh/cert.html#debugging",
    "relUrl": "/docs/security/ssh/cert.html#debugging"
  },"148": {
    "doc": "SSH Certificates",
    "title": "SSH certificate extensions",
    "content": "Some of the basic extensions (names are self explanatory): . | permit-pty: Allow interactive shell | permit-port-forwarding: Allow SSH tunnels | permit-x11-forwarding | permit-agent-forwarding | permit-user-rc | . References: . | Vault: Signed SSH Certificates | . ",
    "url": "/docs/security/ssh/cert.html#ssh-certificate-extensions",
    "relUrl": "/docs/security/ssh/cert.html#ssh-certificate-extensions"
  },"149": {
    "doc": "Chalice",
    "title": "Chalice",
    "content": ". | What is Chalice? | Install Chalice | Create a new project | Deploy / Delete | Multifiles | Configuration File . | Environment Variables | . | Deploying with Terraform | . ",
    "url": "/docs/aws/chalice.html",
    "relUrl": "/docs/aws/chalice.html"
  },"150": {
    "doc": "Chalice",
    "title": "What is Chalice?",
    "content": "It is a python serverless microframework. What it essentially does is combine AWS API Gateway and associated Lambda functions to help you quickly deploy a microservice. Everything you can do in Chalice you can do in the AWS console, but it is easier to manage via code. The syntax and the concept is very much similar to Flask if you’re familiar with it. ",
    "url": "/docs/aws/chalice.html#what-is-chalice",
    "relUrl": "/docs/aws/chalice.html#what-is-chalice"
  },"151": {
    "doc": "Chalice",
    "title": "Install Chalice",
    "content": "pip3 install chalice . As of now (2021-05), chalice best supports Python 3.8. As of now (2022) chalice supports Python 3.9. ",
    "url": "/docs/aws/chalice.html#install-chalice",
    "relUrl": "/docs/aws/chalice.html#install-chalice"
  },"152": {
    "doc": "Chalice",
    "title": "Create a new project",
    "content": "To create a new project, . chalice new-project myproj . This will create a myproj directory . myproj ├── .chalice ├── app.py └── requirements.txt . ",
    "url": "/docs/aws/chalice.html#create-a-new-project",
    "relUrl": "/docs/aws/chalice.html#create-a-new-project"
  },"153": {
    "doc": "Chalice",
    "title": "Deploy / Delete",
    "content": "The AWS credentials must already be set in ~/.aws/config. To deploy, simply . chalice deploy chalice deploy --stage ${stage} --profile ${profile} . To delete, . chalice delete chalice delete --stage ${stage} --profile ${profile} . ",
    "url": "/docs/aws/chalice.html#deploy--delete",
    "relUrl": "/docs/aws/chalice.html#deploy--delete"
  },"154": {
    "doc": "Chalice",
    "title": "Multifiles",
    "content": "If you want to have multiple .py files apart from the app.py(which you will), place all the lib or utils related file in a folder called chalicelib. Anything you add to this directory is recursively added to the deployment. myproj ├── .chalice ├── app.py ├── chalicelib └── requirements.txt . ",
    "url": "/docs/aws/chalice.html#multifiles",
    "relUrl": "/docs/aws/chalice.html#multifiles"
  },"155": {
    "doc": "Chalice",
    "title": "Configuration File",
    "content": "In .chalice, there is a file called config.json. This folder contains all the configurations related to this package. You can set app name, deploment stages, environment variables, etc. Environment Variables . For general environment variables, add the following syntax to .chalice/config.json . { \"environment_variables\": { \"ENV_VAR\": \"value\", \"ENV_VAR2\": \"value2\" } } . You can also set stage specific environment variables by, . { \"stages\": { \"dev\": { \"environment_variables\": { \"MY_ENV\": \"value\" } }, \"prod\": { \"environment_variables\": { \"MY_PROD_ENV\": \"value\" } } } } . ",
    "url": "/docs/aws/chalice.html#configuration-file",
    "relUrl": "/docs/aws/chalice.html#configuration-file"
  },"156": {
    "doc": "Chalice",
    "title": "Deploying with Terraform",
    "content": "# Will generate deployment.zip and chalice.tf.json chalice package --pkg-format terraform output_dir . chalice package will generate the Lambda deployments and Terraform configuration files. You can then use Terraform CLI to deploy. See here for details. References: . | Chalice: Quickstart | Chalice: Terraform Support | Chalice: Multifile | Chalice: Configuration File | . ",
    "url": "/docs/aws/chalice.html#deploying-with-terraform",
    "relUrl": "/docs/aws/chalice.html#deploying-with-terraform"
  },"157": {
    "doc": "CLI Setup",
    "title": "IntelliJ CLI Setup",
    "content": ". | Launch IntelliJ from CLI . | Create a launcher script | . | . ",
    "url": "/docs/others/intellij/cli.html#intellij-cli-setup",
    "relUrl": "/docs/others/intellij/cli.html#intellij-cli-setup"
  },"158": {
    "doc": "CLI Setup",
    "title": "Launch IntelliJ from CLI",
    "content": "By default, IntelliJ IDEA does not provide a CLI launcher. Create a launcher script . First add the following script to /usr/local/bin (or any other directory in your $PATH). Give the script a name like idea or any other name you prefer. | 1 2 . | #!/bin/zsh open -na \"IntelliJ IDEA.app\" --args \"$@\" . | . Then make the script executable, . chmod 755 /usr/local/bin/idea . Now you can launch IntelliJ IDEA from the command line, . idea . References: . | IDEA CLI Setup | IDEA CLI Open Files | . ",
    "url": "/docs/others/intellij/cli.html#launch-intellij-from-cli",
    "relUrl": "/docs/others/intellij/cli.html#launch-intellij-from-cli"
  },"159": {
    "doc": "CLI Setup",
    "title": "CLI Setup",
    "content": " ",
    "url": "/docs/others/intellij/cli.html",
    "relUrl": "/docs/others/intellij/cli.html"
  },"160": {
    "doc": "MySQL Client",
    "title": "MySQL Client",
    "content": "If you only need to connect to a MySQL database, you can use the lighter weight MySQL client. | MySQL Client . | Installation . | Adding to PATH | . | Cloning a database . | Export query to recreate empty database | . | . | . ",
    "url": "/docs/mysql/client.html",
    "relUrl": "/docs/mysql/client.html"
  },"161": {
    "doc": "MySQL Client",
    "title": "Installation",
    "content": "Using Homebrew: . brew install mysql-client . Adding to PATH . You will notice that the client is not immediately available in your terminal: . | 1 2 3 4 5 6 7 8 9 10 11 12 13 14 . | ==&gt; Caveats ==&gt; mysql-client mysql-client is keg-only, which means it was not symlinked into /opt/homebrew, because it conflicts with mysql (which contains client libraries). If you need to have mysql-client first in your PATH, run: echo 'export PATH=\"/opt/homebrew/opt/mysql-client/bin:$PATH\"' &gt;&gt; ~/.zshrc For compilers to find mysql-client you may need to set: export LDFLAGS=\"-L/opt/homebrew/opt/mysql-client/lib\" export CPPFLAGS=\"-I/opt/homebrew/opt/mysql-client/include\" For pkg-config to find mysql-client you may need to set: export PKG_CONFIG_PATH=\"/opt/homebrew/opt/mysql-client/lib/pkgconfig\" . | . You can follow the instruction on line number 7 to add the client binaries to PATH. Doing so will create a conflict with the MySQL server binaries if you happen to install them later on. If you do, you will need to remove the client binaries from PATH. Or you could just create an alias to some of the binaries you may need, for e.g.: . alias mysqldump=\"/opt/homebrew/opt/mysql-client/bin/mysqldump\" . I personally prefer the latter. ",
    "url": "/docs/mysql/client.html#installation",
    "relUrl": "/docs/mysql/client.html#installation"
  },"162": {
    "doc": "MySQL Client",
    "title": "Cloning a database",
    "content": "Once you’ve downloaded the client, you can use mysqldump to selectively clone a database. Refer to mysqldump --help for a comprehensive list. Export query to recreate empty database . To export queries to recreate schemas, views, etc. use the --no-data flag: . mysqldump \\ --host 127.0.0.1 \\ --user root \\ --port 3306 \\ --databases mydb \\ --password --no-data --routines --single-transaction \\ &gt; schema.sql # OR mysqldump \\ -h 127.0.0.1 \\ -u root \\ -P 3306 \\ -B mydb \\ -p -d -R --single-transaction \\ &gt; schema.sql . --single-transaction can be used to avoid locking the database while performing the dump or if you do not have the LOCK TABLES privilege. Use 127.0.0.1 instead of localhost if you want to enforce connection via TCP/IP instead of sockets. MySQL 8.0.32’s mysqldump currently has an issue with AWS RDS when using the --single-transaction flag. See this issue for more details. ",
    "url": "/docs/mysql/client.html#cloning-a-database",
    "relUrl": "/docs/mysql/client.html#cloning-a-database"
  },"163": {
    "doc": "Cognito",
    "title": "AWS Cognito",
    "content": ". | Token: Hosted UI / AWS SDK . | Hosted UI | AWS SDK | . | How to use a custom domain | . ",
    "url": "/docs/aws/cognito.html#aws-cognito",
    "relUrl": "/docs/aws/cognito.html#aws-cognito"
  },"164": {
    "doc": "Cognito",
    "title": "Token: Hosted UI / AWS SDK",
    "content": "Hosted UI . Cognito hosts a login portal and an authorization server by default. This UI is hosted on the /login enpoint. After user types in their credentials, a request is automatically made to the /oauth2/authorize endpoint. Upon successful authentication, client is redirected to a URL configured for the user pool client. If you’re using an implicit flow (not recommended), you will be redirected with a token directly. If you’re using an authorization code flow, you will be redirected with a code parameter which you can exchange later to a token at the /oauth2/token endpoint. AWS SDK . Although the hosted UI option is convenient, one downside of it is that customization is limited. ",
    "url": "/docs/aws/cognito.html#token-hosted-ui--aws-sdk",
    "relUrl": "/docs/aws/cognito.html#token-hosted-ui--aws-sdk"
  },"165": {
    "doc": "Cognito",
    "title": "How to use a custom domain",
    "content": "To be added . ",
    "url": "/docs/aws/cognito.html#how-to-use-a-custom-domain",
    "relUrl": "/docs/aws/cognito.html#how-to-use-a-custom-domain"
  },"166": {
    "doc": "Cognito",
    "title": "Cognito",
    "content": " ",
    "url": "/docs/aws/cognito.html",
    "relUrl": "/docs/aws/cognito.html"
  },"167": {
    "doc": "Basic Combinatorics",
    "title": "Basic Combinatorics",
    "content": ". | Permutation . | Permutation without repetition | Permutation with repetition | . | Combination . | Combination without repetition | Combination with repetition | . | . ",
    "url": "/docs/statistics/notes/combinatorics.html",
    "relUrl": "/docs/statistics/notes/combinatorics.html"
  },"168": {
    "doc": "Basic Combinatorics",
    "title": "Permutation",
    "content": "A permutation is an arrangement of objects in a specific order. ",
    "url": "/docs/statistics/notes/combinatorics.html#permutation",
    "relUrl": "/docs/statistics/notes/combinatorics.html#permutation"
  },"169": {
    "doc": "Basic Combinatorics",
    "title": "Permutation without repetition",
    "content": "If repetition is not allowed, then the number of permutations of $n$ objects taken $r$ at a time is . $$ _nP_r = \\frac{n!}{(n-r)!} $$ . ",
    "url": "/docs/statistics/notes/combinatorics.html#permutation-without-repetition",
    "relUrl": "/docs/statistics/notes/combinatorics.html#permutation-without-repetition"
  },"170": {
    "doc": "Basic Combinatorics",
    "title": "Permutation with repetition",
    "content": "If repetition is allowed, then the number of permutations of $n$ objects taken $r$ at a time is $n^r$. ",
    "url": "/docs/statistics/notes/combinatorics.html#permutation-with-repetition",
    "relUrl": "/docs/statistics/notes/combinatorics.html#permutation-with-repetition"
  },"171": {
    "doc": "Basic Combinatorics",
    "title": "Combination",
    "content": "A combination is a selection of objects where order does not matter. ",
    "url": "/docs/statistics/notes/combinatorics.html#combination",
    "relUrl": "/docs/statistics/notes/combinatorics.html#combination"
  },"172": {
    "doc": "Basic Combinatorics",
    "title": "Combination without repetition",
    "content": "If repetition is not allowed, then the number of combinations of $n$ objects taken $r$ at a time is . $$ _nC_r = \\frac{n!}{r!(n-r)!} $$ . ",
    "url": "/docs/statistics/notes/combinatorics.html#combination-without-repetition",
    "relUrl": "/docs/statistics/notes/combinatorics.html#combination-without-repetition"
  },"173": {
    "doc": "Basic Combinatorics",
    "title": "Combination with repetition",
    "content": "If repetition is allowed, then the number of combinations of $n$ objects taken $r$ at a time is . $$ _{r+n-1}C_r = \\frac{(r+n-1)!}{r!(n-1)!} $$ . ",
    "url": "/docs/statistics/notes/combinatorics.html#combination-with-repetition",
    "relUrl": "/docs/statistics/notes/combinatorics.html#combination-with-repetition"
  },"174": {
    "doc": "Complexity Analysis Examples",
    "title": "Complexity Analysis Examples",
    "content": ". | Substitution method | Single while loop | For loop conditions . | Index square condition | Index increase with multiply | . | Master theorem | Nested for loops with dependence | Solving recurrence with change of variable | . ",
    "url": "/docs/compsci/algo/complexity-example.html",
    "relUrl": "/docs/compsci/algo/complexity-example.html"
  },"175": {
    "doc": "Complexity Analysis Examples",
    "title": "Substitution method",
    "content": "Given the recurrence relation, we can find find the complexity by substituting the complexity of the subproblems. \\[T(n) = \\begin{cases} 1 &amp; \\text{if } n = 0 \\\\ 2T(n-1) - 1 &amp; \\text{if } n &gt; 0 \\end{cases}\\] We want to solve . \\[T(n) = 2T(n-1) - 1\\] Usually, when there is negative work (i.e. $-1$), the Master Theorem idea does not work. We know that $T(n-1) = 2T(n-2) - 1$ so we can substitute it in. \\[\\begin{align*} T(n) &amp;= 2(2T(n-2) - 1) - 1 \\\\ &amp;= 2^2T(n-2) - 2^1 - 2^0 \\end{align*}\\] We can continue substituting until we get to $T(0)$, . \\[\\begin{align*} T(n) &amp;= 2^2T(n-2) - 2^1 - 2^0 \\\\ &amp;= 2^3T(n-3) - 2^2 - 2^1 - 2^0 \\\\ ...&amp;= 2^nT(n-n) - 2^{n-1} - 2^{n-2} - ... - 2^1 - 2^0 \\\\ &amp;= 2^n + 2^n - 2^n - 2^{n-1} - 2^{n-2} - ... - 2^1 - 2^0 \\\\ &amp;= 2^{n+1} - (2^0 + 2^1 + ... + 2^n) \\\\ &amp;= 2^{n+1} - \\frac{2^{n+1} - 1}{2 - 1} \\\\ &amp;= 2^{n+1} - 2^{n+1} + 1 \\\\ &amp;= 1 \\end{align*}\\] Therefore, $T(n) = O(1)$. ",
    "url": "/docs/compsci/algo/complexity-example.html#substitution-method",
    "relUrl": "/docs/compsci/algo/complexity-example.html#substitution-method"
  },"176": {
    "doc": "Complexity Analysis Examples",
    "title": "Single while loop",
    "content": "def f(n): i, x = 1, 1 while x &lt;= n: i += 1 x += i . The loop runs while $x &lt;= n$. What is $x$? It is the sum of the first $i$ integers. Let $k$ be the number of iterations. We want to solve for $k$. \\[\\begin{gather*} 1 + 2 + ... + k \\le n \\\\ \\frac{k(k+1)}{2} \\le n \\tag*{Arithmetic series} \\\\ k \\approx O(\\sqrt{n}) \\end{gather*}\\] . ",
    "url": "/docs/compsci/algo/complexity-example.html#single-while-loop",
    "relUrl": "/docs/compsci/algo/complexity-example.html#single-while-loop"
  },"177": {
    "doc": "Complexity Analysis Examples",
    "title": "For loop conditions",
    "content": "Assumes that the work of loop is $O(1)$, unless otherwise stated. ",
    "url": "/docs/compsci/algo/complexity-example.html#for-loop-conditions",
    "relUrl": "/docs/compsci/algo/complexity-example.html#for-loop-conditions"
  },"178": {
    "doc": "Complexity Analysis Examples",
    "title": "Index square condition",
    "content": "... for (i = 1; i*i &lt;= n; i++) ... The loop runs while $i^2 &lt;= n$. Let $k$ be the number of iterations. We want to solve for $k$. \\[k^2 \\le n \\implies k \\approx O(\\sqrt{n})\\] ",
    "url": "/docs/compsci/algo/complexity-example.html#index-square-condition",
    "relUrl": "/docs/compsci/algo/complexity-example.html#index-square-condition"
  },"179": {
    "doc": "Complexity Analysis Examples",
    "title": "Index increase with multiply",
    "content": "... for (i = 1; i &lt;= n; i *= 2) ... The loop runs while $i &lt;= n$. Let $k$ be the number of iterations. We want to solve for $k$. \\[2^k \\le n \\implies k \\approx O(\\log n)\\] . ",
    "url": "/docs/compsci/algo/complexity-example.html#index-increase-with-multiply",
    "relUrl": "/docs/compsci/algo/complexity-example.html#index-increase-with-multiply"
  },"180": {
    "doc": "Complexity Analysis Examples",
    "title": "Master theorem",
    "content": ". | Example 1: . \\[T(n) = T(n-2) + O(n^2)\\] Since the branching factor of recursion is $1$, each level of recursion does $O(n^2)$ work. Depth of recursion is $O(n)$ (since $n-2$). Therefore total work is $O(n) \\cdot O(n^2) = O(n^3)$. | . | Example 2: . \\[T(n) = 2T(\\frac{n}{2}) + O(nlog n)\\] Branching factor is $2 = 2^1$, so each level of recursion does $O(nlog n)$ work. Depth of recursion is $O(\\log n)$ (since $\\frac{n}{2}$). Therefore total work is $O(nlog n) \\cdot O(\\log n) = O(nlog^2 n)$. | . | Example 3: . \\[T(n) = T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + O(n)\\] We can tweak the recurrence to . \\[T(n) \\le aT(\\frac{n}{2}) + O(n)\\] We can see that $a &lt; 2^1$, because $\\frac{1}{2} &gt; \\frac{1}{4} + \\frac{1}{8}$. Work is dominated at the root, so $O(n)$. | . | Example 3: . \\[T(n) = T(\\frac{n}{2}) + O(1)\\] Since $1 = 2^0$, work is distributed among the tree. The depth of the tree is $O(\\log n)$ (since $\\frac{n}{2}$). Therefore total work is $O(\\log n) \\cdot O(1) = O(\\log n)$. | Example 4: . \\[T(n) = T(n-1) + T(n-2) + O(1)\\] We can tweak the recurrence to . \\[T(n) \\le 2T(n-1) + O(1)\\] Since $2 &gt; 1$, work is dominated by leaves. The depth of the tree is $O(n)$ (since $n-1$). At depth $n$, there are $2^n$ leaves (since $2T(n-1)$). Therefore total work is $O(1) \\cdot O(2^n) = O(2^n)$. | . ",
    "url": "/docs/compsci/algo/complexity-example.html#master-theorem",
    "relUrl": "/docs/compsci/algo/complexity-example.html#master-theorem"
  },"181": {
    "doc": "Complexity Analysis Examples",
    "title": "Nested for loops with dependence",
    "content": "Usually with nested loops, we can multiply the complexities. However, if the inner loop depends on the outer loop, we try to find the number of times the inner loop runs in total... for (i = 1; i &lt;= n; i++) for (j = 1; j &lt;= n; j += i) ... As the inner loop condition increases by $i$ each time, we cannot simply multiply the complexities. We want to solve how many times the inner loop runs in total. | When $i = 1$, the inner loop runs $n$ times. | When $i = 2$, the inner loop runs $\\frac{n}{2}$ times. | When $i = 3$, the inner loop runs $\\frac{n}{3}$ times. | When $i = n$, the inner loop runs $\\frac{n}{n} = 1$ time. | . Then, the inner loop runs in total . \\[\\begin{align*} \\sum_{i=1}^n \\frac{n}{i} &amp;= n \\sum_{i=1}^n \\frac{1}{i} \\\\ &amp;\\approx n \\cdot O(\\log n) \\tag{Harmonic series} \\\\ &amp;\\implies O(n \\log n) \\end{align*}\\] . ",
    "url": "/docs/compsci/algo/complexity-example.html#nested-for-loops-with-dependence",
    "relUrl": "/docs/compsci/algo/complexity-example.html#nested-for-loops-with-dependence"
  },"182": {
    "doc": "Complexity Analysis Examples",
    "title": "Solving recurrence with change of variable",
    "content": "\\[T(n) = 2T(\\sqrt{n}) + O(\\log n)\\] Let $n = 2^m$. Then, . \\[\\begin{align*} T(n) &amp;= T(2^m) \\\\ &amp;= 2T(2^{\\frac{m}{2}}) + O(m) \\end{align*}\\] Define $S(m) = T(2^m)$. Then $S(\\frac{m}{2}) = T(2^{\\frac{m}{2}})$. Therefore, . \\[S(m) = 2S(\\frac{m}{2}) + O(m)\\] Since $2 = 2^1$, work is distributed among the tree. The depth of the tree is $O(\\log m)$ (since $\\frac{m}{2}$). Thus work is $O(m) \\cdot O(\\log m) = O(m \\log m)$. Since $2^m = n \\implies m = \\log n$, it is $O(\\log n \\log \\log n)$. ",
    "url": "/docs/compsci/algo/complexity-example.html#solving-recurrence-with-change-of-variable",
    "relUrl": "/docs/compsci/algo/complexity-example.html#solving-recurrence-with-change-of-variable"
  },"183": {
    "doc": "Complexity of Algorithms",
    "title": "Complexity of Algorithms",
    "content": ". | Commonly used complexity classes | Asymptotic notation . | Big-O notation | Big-Omega notation | Big-Theta notation | . | Master theorem . | Divide-and-conquer recurrence relations . | Simple form | Advanced form | . | Decreasing recurrence relations | . | Amortized analysis | Examples | . ",
    "url": "/docs/compsci/algo/complexity.html",
    "relUrl": "/docs/compsci/algo/complexity.html"
  },"184": {
    "doc": "Complexity of Algorithms",
    "title": "Commonly used complexity classes",
    "content": ". | $1$: constant | $\\log\\log(n)$ | $\\sqrt{\\log(n)}$ | $log(n)$: logarithmic | $n$: linear | $\\log(n!)$ | $n\\log(n)$: loglinear | $n^c$: polynomial | $c^n$: exponential | $n!$: factorial | . Commonly used definitions in complexity analysis ",
    "url": "/docs/compsci/algo/complexity.html#commonly-used-complexity-classes",
    "relUrl": "/docs/compsci/algo/complexity.html#commonly-used-complexity-classes"
  },"185": {
    "doc": "Complexity of Algorithms",
    "title": "Properties of logarithms",
    "content": "\\[\\log(x^y) = y\\log(x)\\] \\[a^{\\log_b(x)} = x^{\\log_b(a)}\\] \\[\\log(xy) = \\log(x) + \\log(y)\\] \\[\\log(\\frac{x}{y}) = \\log(x) - \\log(y)\\] \\[\\log_b(x) = \\frac{\\log_a(x)}{\\log_a(b)}\\] \\[\\sum_{k=1}^n \\log(k) \\approx n\\log(n)\\] ",
    "url": "/docs/compsci/algo/complexity.html#properties-of-logarithms",
    "relUrl": "/docs/compsci/algo/complexity.html#properties-of-logarithms"
  },"186": {
    "doc": "Complexity of Algorithms",
    "title": "Arithmetic series",
    "content": "\\[\\sum_{k=1}^n k = 1 + 2 + \\cdots + n = \\frac{n(n+1)}{2}\\] ",
    "url": "/docs/compsci/algo/complexity.html#arithmetic-series",
    "relUrl": "/docs/compsci/algo/complexity.html#arithmetic-series"
  },"187": {
    "doc": "Complexity of Algorithms",
    "title": "Geometric series",
    "content": "\\[\\sum_{k=0}^n ar^k = a + ar + ar^2 + \\cdots + ar^n = \\frac{a(1-r^{n+1})}{1-r}\\] ",
    "url": "/docs/compsci/algo/complexity.html#geometric-series",
    "relUrl": "/docs/compsci/algo/complexity.html#geometric-series"
  },"188": {
    "doc": "Complexity of Algorithms",
    "title": "Harmonic series",
    "content": "\\[\\sum_{k=1}^n \\frac{1}{k} = 1 + \\frac{1}{2} + \\cdots + \\frac{1}{n} \\approx \\log(n)\\] ",
    "url": "/docs/compsci/algo/complexity.html#harmonic-series",
    "relUrl": "/docs/compsci/algo/complexity.html#harmonic-series"
  },"189": {
    "doc": "Complexity of Algorithms",
    "title": "Faulhaber’s formula",
    "content": "Special case of power sum. \\[\\sum_{k=1}^n k^p = 1^p + 2^p + \\cdots + n^p \\approx \\frac{n^{p+1}}{p+1}\\] . ",
    "url": "/docs/compsci/algo/complexity.html#faulhabers-formula",
    "relUrl": "/docs/compsci/algo/complexity.html#faulhabers-formula"
  },"190": {
    "doc": "Complexity of Algorithms",
    "title": "Asymptotic notation",
    "content": " ",
    "url": "/docs/compsci/algo/complexity.html#asymptotic-notation",
    "relUrl": "/docs/compsci/algo/complexity.html#asymptotic-notation"
  },"191": {
    "doc": "Complexity of Algorithms",
    "title": "Big-O notation",
    "content": "Big-O $O$ is for the upper bound (worst case). Not necessarily tight. For some function $f$ and $g$, . $$ \\exists n_0, c &gt; 0 \\text{ s.t. } \\forall n \\geq n_0,\\; f(n) \\leq c g(n) \\implies f(n) = O(g(n)) $$ . | Transitivity: $f(n) = O(g(n)) \\land g(n) = O(h(n)) \\implies f(n) = O(h(n))$ | Reflexivity: $f(n) = O(f(n))$ | . ",
    "url": "/docs/compsci/algo/complexity.html#big-o-notation",
    "relUrl": "/docs/compsci/algo/complexity.html#big-o-notation"
  },"192": {
    "doc": "Complexity of Algorithms",
    "title": "Big-Omega notation",
    "content": "Big-Omega $\\Omega$ is for the lower bound (best case). Not necessarily tight. For some function $f$ and $g$, . $$ \\exists n_0, c &gt; 0 \\text{ s.t. } \\forall n \\geq n_0,\\; f(n) \\geq c g(n) \\implies f(n) = \\Omega(g(n)) $$ . | Transitivity | Reflexivity | Transpose symmetry: $f(n) = \\Omega(g(n)) \\iff g(n) = O(f(n))$ | . ",
    "url": "/docs/compsci/algo/complexity.html#big-omega-notation",
    "relUrl": "/docs/compsci/algo/complexity.html#big-omega-notation"
  },"193": {
    "doc": "Complexity of Algorithms",
    "title": "Big-Theta notation",
    "content": "Big-Theta $\\Theta$ is for the tight bound (average case). For some function $f$ and $g$, . $$ \\exists n_0, c_1, c_2 &gt; 0 \\text{ s.t. } \\forall n \\geq n_0,\\; c_1 g(n) \\leq f(n) \\leq c_2 g(n) \\implies f(n) = \\Theta(g(n)) $$ . | Transitivity | Reflexivity | Symmetry: $f(n) = \\Theta(g(n)) \\iff g(n) = \\Theta(f(n))$ | . ",
    "url": "/docs/compsci/algo/complexity.html#big-theta-notation",
    "relUrl": "/docs/compsci/algo/complexity.html#big-theta-notation"
  },"194": {
    "doc": "Complexity of Algorithms",
    "title": "Master theorem",
    "content": " ",
    "url": "/docs/compsci/algo/complexity.html#master-theorem",
    "relUrl": "/docs/compsci/algo/complexity.html#master-theorem"
  },"195": {
    "doc": "Complexity of Algorithms",
    "title": "Divide-and-conquer recurrence relations",
    "content": "For recursive, divide-and-conquer algorithms, the master theorem can be used to determine the asymptotic complexity. Simple form . If the algorithm has the following recurrence relation, . $$ T(n) = aT(\\frac{n}{b}) + O(n^d) $$ . where $a &gt; 0$, $b &gt; 1$, and $d \\geq 0$, you may use the following rules of Master theorem. | If $a &lt; b^d$ (dominated by the work at root), $$ T(n) = O(n^d) $$ . | If $a = b^d$ (work is evenly distributed in each layer), $$ T(n) = O(n^d \\log(n)) $$ . | If $a &gt; b^d$ (dominated by the work at leaves), $$ T(n) = O(n^{\\log_b(a)}) $$ . | . Advanced form . Advanced form of Master theorem If the algorithm has the following recurrence relation, . $$ T(n) = aT(\\frac{n}{b}) + \\Theta(n^k \\log^p(n)) $$ . where $a \\geq 1$, $b &gt; 1$, $k \\geq 0$, and $p \\in \\mathbb{R}$, you may use the following rules of Master theorem. | If $a &gt; b^k$, $$ T(n) = \\Theta(n^{\\log_b(a)}) $$ . | If $a = b^k$, . | If $p &gt; -1$, $$ T(n) = \\Theta(n^{\\log_b(a)} \\log^{p+1}(n)) $$ . | If $p = -1$, $$ T(n) = \\Theta(n^{\\log_b(a)} \\log\\log(n)) $$ . | If $p &lt; -1$, $$ T(n) = \\Theta(n^{\\log_b(a)}) $$ . | . | If $a &lt; b^k$, . | If $p \\geq 0$, $$ T(n) = \\Theta(n^k \\log^p(n)) $$ . | If $p &lt; 0$, $$ T(n) = O(n^k) $$ . | . | . ",
    "url": "/docs/compsci/algo/complexity.html#divide-and-conquer-recurrence-relations",
    "relUrl": "/docs/compsci/algo/complexity.html#divide-and-conquer-recurrence-relations"
  },"196": {
    "doc": "Complexity of Algorithms",
    "title": "Decreasing recurrence relations",
    "content": "If the algorithm has the following recurrence relation, . $$ T(n) = aT(n-b) + O(n^d) $$ . where $a &gt; 0$, $b &gt; 0$, you may use the following rules of Master theorem. | If $a &lt; 1$ (dominated by the work at root), $$ T(n) = O(n^d) $$ . | If $a = 1$ (work is evenly distributed in each layer), $$ T(n) = O(n^{d+1}) $$ . | If $a &gt; 1$ (dominated by the work at leaves), $$ T(n) = O(n^d a^{n/b}) $$ . | . ",
    "url": "/docs/compsci/algo/complexity.html#decreasing-recurrence-relations",
    "relUrl": "/docs/compsci/algo/complexity.html#decreasing-recurrence-relations"
  },"197": {
    "doc": "Complexity of Algorithms",
    "title": "Amortized analysis",
    "content": "Amortized analysis is a method for analyzing a sequence of operations. It is often used to show that a sequence of operations is efficient. Even if a single operation is expensive, the amortized cost of each operation can be small if the expensive operation is performed rarely. Amortized analysis is used when the worst-case asymptotic analysis over a sequence of operations is unfairly pessimistic. ",
    "url": "/docs/compsci/algo/complexity.html#amortized-analysis",
    "relUrl": "/docs/compsci/algo/complexity.html#amortized-analysis"
  },"198": {
    "doc": "Complexity of Algorithms",
    "title": "Examples",
    "content": "Go to page . ",
    "url": "/docs/compsci/algo/complexity.html#examples",
    "relUrl": "/docs/compsci/algo/complexity.html#examples"
  },"199": {
    "doc": "Docker Compose",
    "title": "Docker Compose",
    "content": ". | Why use Docker Compose? | Dockerfile / docker-compose.yml . | Dockerfile | docker-compose.yml | . | Example usage | Useful commands | Itty Bitties . | Build context | Default network | . | . ",
    "url": "/docs/docker/compose.html",
    "relUrl": "/docs/docker/compose.html"
  },"200": {
    "doc": "Docker Compose",
    "title": "Why use Docker Compose?",
    "content": "Compose is a tool to help define and run containers/services. It basically packs all those docker bulid ..., docker run ... commands into a single yaml. ",
    "url": "/docs/docker/compose.html#why-use-docker-compose",
    "relUrl": "/docs/docker/compose.html#why-use-docker-compose"
  },"201": {
    "doc": "Docker Compose",
    "title": "Dockerfile / docker-compose.yml",
    "content": "Dockerfile . Dockerfile defines the recipe to create an image. docker-compose.yml . docker-compose.yml defines services or containers that run images. ",
    "url": "/docs/docker/compose.html#dockerfile--docker-composeyml",
    "relUrl": "/docs/docker/compose.html#dockerfile--docker-composeyml"
  },"202": {
    "doc": "Docker Compose",
    "title": "Example usage",
    "content": "version: '3.9' services: my-service: container_name: my-container build: context: ./src dockerfile: Dockerfile ports: - 1234:1234 volumes: - ./src:/www . For ports and volumes the order of syntax is &lt;host&gt;:&lt;container&gt;. ",
    "url": "/docs/docker/compose.html#example-usage",
    "relUrl": "/docs/docker/compose.html#example-usage"
  },"203": {
    "doc": "Docker Compose",
    "title": "Useful commands",
    "content": "# Create and start containers docker-compose up # Build images and start containers docker-compose up --build # Start the containers in detached mode docker-compose up --d # Stop and remove containers and default networks docker-compose down # Lists containers (even the ones that are exited) docker-compose ps . ",
    "url": "/docs/docker/compose.html#useful-commands",
    "relUrl": "/docs/docker/compose.html#useful-commands"
  },"204": {
    "doc": "Docker Compose",
    "title": "Itty Bitties",
    "content": "Suppose the project root directory is called proj. proj ├── docker-compose.yml └── src ├── Dockerfile └── ... # docker-compose.yml services: my-service: container_name: my-container build: context: ./src dockerfile: Dockerfile . Build context . With docker build -f ../Dockerfile ., it is possible for the Dockerfile to be outside of the build context. However, it seems that with Compose, the Dockerfile must be within the build context. So in the example case Dockerfile must be under src, otherwise it will produce an error: . failed to solve: rpc error: code = Unknown desc = failed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/your-build-context/Dockerfile: no such file or directory . Default network . Compose automatically creates a bridge network of name proj_default, and adds all service containers to it. Check that is is true by, . # Locate the created default network docker network ls # Inspect the containers in it docker inspect proj_default . Then you will see my-container listed under network proj_default. ",
    "url": "/docs/docker/compose.html#itty-bitties",
    "relUrl": "/docs/docker/compose.html#itty-bitties"
  },"205": {
    "doc": "Conda",
    "title": "Conda",
    "content": ". | Advantage to other virtual environments | Install Conda (miniconda) | Typical usage | Create environment | Activate / Deactivate | Install packages | List and export dependencies | Clone environment | . ",
    "url": "/docs/python/envs/conda.html",
    "relUrl": "/docs/python/envs/conda.html"
  },"206": {
    "doc": "Conda",
    "title": "Advantage to other virtual environments",
    "content": "Unlike some other virtual environments that are dependent on a preinstalled Python, conda is both a Python version manager and a virtual environment manager. conda makes using different Python versions in different environments easier. ",
    "url": "/docs/python/envs/conda.html#advantage-to-other-virtual-environments",
    "relUrl": "/docs/python/envs/conda.html#advantage-to-other-virtual-environments"
  },"207": {
    "doc": "Conda",
    "title": "Install Conda (miniconda)",
    "content": "Conda can be installed through installers. Just follow the prompt to install. Whatever you do, don’t forget to run conda init zsh. If you don’t want the base environment activated all the time, . conda config --set auto_activate_base false . ",
    "url": "/docs/python/envs/conda.html#install-conda-miniconda",
    "relUrl": "/docs/python/envs/conda.html#install-conda-miniconda"
  },"208": {
    "doc": "Conda",
    "title": "Typical usage",
    "content": "To create an environment for a project: . conda create -n myenv python=3.x conda activate myenv . ",
    "url": "/docs/python/envs/conda.html#typical-usage",
    "relUrl": "/docs/python/envs/conda.html#typical-usage"
  },"209": {
    "doc": "Conda",
    "title": "Create environment",
    "content": "Simplest method is: . conda create -n myenv . To use a specific Python version: . conda create --name myenv python=3.8 . Created environments are located in ~/anaconda3/env or ~/miniconda3/env. If you installed conda via GUI installer, the conda folder may be in /opt. Confirm environment creation via . conda env list # OR conda info --envs . ",
    "url": "/docs/python/envs/conda.html#create-environment",
    "relUrl": "/docs/python/envs/conda.html#create-environment"
  },"210": {
    "doc": "Conda",
    "title": "Activate / Deactivate",
    "content": "conda activate myenv . conda deactivate . ",
    "url": "/docs/python/envs/conda.html#activate--deactivate",
    "relUrl": "/docs/python/envs/conda.html#activate--deactivate"
  },"211": {
    "doc": "Conda",
    "title": "Install packages",
    "content": "To install packages in current active environment, . conda install pkg-name # OR for a specific version conda install pkg-name=1.0.0 . To install packages in another environment, . conda install pkg-name -n myenv . ",
    "url": "/docs/python/envs/conda.html#install-packages",
    "relUrl": "/docs/python/envs/conda.html#install-packages"
  },"212": {
    "doc": "Conda",
    "title": "List and export dependencies",
    "content": "conda list . To export dependencies (like pip3 freeze &gt; requirements.txt), . conda list --export &gt; requirements.txt . To create an environment with given requirements (like pip3 install -r requirements.txt), . conda create -n myenv --file requirements.txt . ",
    "url": "/docs/python/envs/conda.html#list-and-export-dependencies",
    "relUrl": "/docs/python/envs/conda.html#list-and-export-dependencies"
  },"213": {
    "doc": "Conda",
    "title": "Clone environment",
    "content": "To clone an existing environment, . conda create --name &lt;new-env&gt; --clone &lt;existing-env&gt; . References: . | Conda: Managing Environments | Conda: Install Packages | . ",
    "url": "/docs/python/envs/conda.html#clone-environment",
    "relUrl": "/docs/python/envs/conda.html#clone-environment"
  },"214": {
    "doc": "SSH Config",
    "title": "SSH Config",
    "content": ". | Useful SSH CLI Flags | SSH (Client) Config | SSHD (SSH Daemon) Config | . ",
    "url": "/docs/security/ssh/config.html",
    "relUrl": "/docs/security/ssh/config.html"
  },"215": {
    "doc": "SSH Config",
    "title": "Useful SSH CLI Flags",
    "content": "Man page . -i: Identity file -F: Specify a config file (if omitted, default is ~/.ssh/config) -J [user@]host[:port]: Proxy jump -p: Port -T: Disable pseudo-tty allocation -L local_socket:remote_host:remote_port: Local forwarding . Flags are case sensitive . ",
    "url": "/docs/security/ssh/config.html#useful-ssh-cli-flags",
    "relUrl": "/docs/security/ssh/config.html#useful-ssh-cli-flags"
  },"216": {
    "doc": "SSH Config",
    "title": "SSH (Client) Config",
    "content": "Client configuration man page . Host DECLARATION_SCOPE_NAME User USERNAME Hostname HOSTNAME/IP Port SSH_PORT IdentityFile PRIV_KEY LocalForward LOCAL_PORT REMOTE_HOST:REMOTE_PORT ProxyJump JUMP_SERVER_HOST RequestTTY yes/no ForwardX11 yes/no . | Blank lines are ignored. Use # for comments. | To use spaces in a value, surround it with \". | . ",
    "url": "/docs/security/ssh/config.html#ssh-client-config",
    "relUrl": "/docs/security/ssh/config.html#ssh-client-config"
  },"217": {
    "doc": "SSH Config",
    "title": "SSHD (SSH Daemon) Config",
    "content": "To be added . Daemon configuration man page . References: . | SSH Academy | . ",
    "url": "/docs/security/ssh/config.html#sshd-ssh-daemon-config",
    "relUrl": "/docs/security/ssh/config.html#sshd-ssh-daemon-config"
  },"218": {
    "doc": "Terraform Configuration",
    "title": "Terraform Configuration",
    "content": "With AWS (As of now) . | Terraform Block | Provider | Resource | Using variables | . ",
    "url": "/docs/terraform/config.html",
    "relUrl": "/docs/terraform/config.html"
  },"219": {
    "doc": "Terraform Configuration",
    "title": "Terraform Block",
    "content": "It contains the Terraform settings and has the basic structure of the following . terraform { required_providers { mylocalname = { source = \"source/address\" version = \"~&gt; 1.0\" } } required_version = \"&gt;= 0.14.9\" } . Throughout the module, Terraform refers to providers using a local name. Here I’ve given it a name of mylocalname. Source address takes the form of [Hostname/]Namespace/Type. If Hostname is ommitted, it defaults to registry.terraform.io which is Terraform’s default provider install source. hashicorp/aws is a shorthand for registry.terraform.io/hashicorp/aws. For the version constraint syntax, refer to Version Constraint Syntax. ",
    "url": "/docs/terraform/config.html#terraform-block",
    "relUrl": "/docs/terraform/config.html#terraform-block"
  },"220": {
    "doc": "Terraform Configuration",
    "title": "Provider",
    "content": "You can configure each provider using the local name you have provided in the required_providers of the Terraform block. For example, . provider \"mylocalname\" { # ... } . Reference Provider Configuration for details. ",
    "url": "/docs/terraform/config.html#provider",
    "relUrl": "/docs/terraform/config.html#provider"
  },"221": {
    "doc": "Terraform Configuration",
    "title": "Resource",
    "content": "Basic syntax is as follows, . resource \"aws_instance\" \"my_server\" { ami = \"ami-a1b2c3d4\" instance_type = \"t2.micro\" } . The example block above declares a resource type \"aws_instance\" and gives it a local name of \"my_server\". Just like the provider local name, resource local name is used to refer to this resource throughout the module. In addition, the unique ID for the resource becomes aws_instance.my_server. The resource configuration arguments within the block body are specific to each resource type. For an example, refer to documentation here for aws_instance. ",
    "url": "/docs/terraform/config.html#resource",
    "relUrl": "/docs/terraform/config.html#resource"
  },"222": {
    "doc": "Terraform Configuration",
    "title": "Using variables",
    "content": "To avoid using hard-coded values in configuration, create a new file variables.tf (name of the file can be anything you want) with the following: . variable \"variable_name\" { description = \"Some description of what this is\" type = string default = \"This is the value of the variable\" } . You can then use the variables in other .tf files as, . var.variable_name . You can also pass in a new variable value for testing by . terraform apply -var 'variable_name=SomeOtherValue' . It will modify the state so that all the variables use the new value. This does not update the original variable declaration. If you run terraform apply again without the -var flag, the state will be modified using the original value. References: . | Terraform: Terraform Settings | Terraform: Providers | Terraform: Resources | Terraform: Version Constraint Syntax | . ",
    "url": "/docs/terraform/config.html#using-variables",
    "relUrl": "/docs/terraform/config.html#using-variables"
  },"223": {
    "doc": "Confusion Matrix",
    "title": "Confusion Matrix",
    "content": ". | What is a confusion matrix? | Terminology . | Condition Positive (P) | Condition Negative (N) | True Positive (TP) | True Negative (TN) | False Positive (FP) | False Negative (FN) | . | Types of Errors . | Type I Error | Type II Error | . | Confusion metrics . | True Positive Rate (TPR) | True Negative Rate (TNR) | Positive Predictive Value (PPV) | Accuracy (ACC) | False Negative Rate (FNR) | False Positive Rate (FPR) | . | . ",
    "url": "/docs/statistics/notes/confusion.html#confusion-matrix",
    "relUrl": "/docs/statistics/notes/confusion.html#confusion-matrix"
  },"224": {
    "doc": "Confusion Matrix",
    "title": "What is a confusion matrix?",
    "content": "| Actual\\Prediction | Positive | Negative | . | Positive | True Positive | False Negative | . | Negative | False Positive | True Negative | . It is a performance measure for machine learning classification. It is also known as the error matrix. As you can see from the table, because the test result is compared against a condition label, it is usually used for measuring the performance of a supervised learning. ",
    "url": "/docs/statistics/notes/confusion.html#what-is-a-confusion-matrix",
    "relUrl": "/docs/statistics/notes/confusion.html#what-is-a-confusion-matrix"
  },"225": {
    "doc": "Confusion Matrix",
    "title": "Terminology",
    "content": "Condition Positive (P) . Actually positive . Condition Negative (N) . Actually negative . True Positive (TP) . Actually positive &amp; Tested positive . True Negative (TN) . Actually negative &amp; Tested negative . False Positive (FP) . Actually negative &amp; Tested positive . False Negative (FN) . Actually positive &amp; Tested negative . For TP, TN, FP, and FN, the last positive/negative indicates the result of the test. The True or False are affirmation or rejection of the test result. Memorize FP as “actually not positive” and FN as “actually not negative”. ",
    "url": "/docs/statistics/notes/confusion.html#terminology",
    "relUrl": "/docs/statistics/notes/confusion.html#terminology"
  },"226": {
    "doc": "Confusion Matrix",
    "title": "Types of Errors",
    "content": "The non-errors are True Positive (TP) and True Negative (TN). Type I Error . False Positive (FP) is a Type I error. Type II Error . False Negative (FN) is a Type II error. ",
    "url": "/docs/statistics/notes/confusion.html#types-of-errors",
    "relUrl": "/docs/statistics/notes/confusion.html#types-of-errors"
  },"227": {
    "doc": "Confusion Matrix",
    "title": "Confusion metrics",
    "content": "Listing some of the basic ones, . True Positive Rate (TPR) . Also called: sensitivity, recall, hit rate . \\[TPR = \\frac{TP}{P} = \\frac{TP}{TP+FN} = 1 - FNR\\] True Negative Rate (TNR) . Also called: specificity, selectivity . \\[TNR = \\frac{TN}{N} = \\frac{TN}{TN+FP} = 1 - FPR\\] Positive Predictive Value (PPV) . Also called: precision . \\[PPV = \\frac{TP}{TP+FP}\\] Accuracy (ACC) . \\[ACC = \\frac{TP+TN}{P+N} = \\frac{TP+TN}{TP+FN+TN+FP}\\] False Negative Rate (FNR) . Also called: miss rate . \\[FNR = \\frac{FN}{P} = \\frac{FN}{FN+TP} = 1 - TPR\\] False Positive Rate (FPR) . Also called: fall-out . \\[FPR = \\frac{FP}{N} = \\frac{FP}{FP+TN} = 1 - TNR\\] ",
    "url": "/docs/statistics/notes/confusion.html#confusion-metrics",
    "relUrl": "/docs/statistics/notes/confusion.html#confusion-metrics"
  },"228": {
    "doc": "Confusion Matrix",
    "title": "Confusion Matrix",
    "content": ". ",
    "url": "/docs/statistics/notes/confusion.html",
    "relUrl": "/docs/statistics/notes/confusion.html"
  },"229": {
    "doc": "Docker Container",
    "title": "Docker Container",
    "content": ". | Show running containers | Execute command in container . | Open a container shell | . | Start an existing container in background (detached) | Attach container | . ",
    "url": "/docs/docker/container.html",
    "relUrl": "/docs/docker/container.html"
  },"230": {
    "doc": "Docker Container",
    "title": "Show running containers",
    "content": "docker ps . ",
    "url": "/docs/docker/container.html#show-running-containers",
    "relUrl": "/docs/docker/container.html#show-running-containers"
  },"231": {
    "doc": "Docker Container",
    "title": "Execute command in container",
    "content": "docker exec -it my-container &lt;command&gt; . Open a container shell . docker exec -it my-container sh . ",
    "url": "/docs/docker/container.html#execute-command-in-container",
    "relUrl": "/docs/docker/container.html#execute-command-in-container"
  },"232": {
    "doc": "Docker Container",
    "title": "Start an existing container in background (detached)",
    "content": "The container will run in background and you will not see its stdout/stderr . docker start my-container . ",
    "url": "/docs/docker/container.html#start-an-existing-container-in-background-detached",
    "relUrl": "/docs/docker/container.html#start-an-existing-container-in-background-detached"
  },"233": {
    "doc": "Docker Container",
    "title": "Attach container",
    "content": "If you want to see outputs from the container in your terminal (ie. logging), you would want to run the container in attached mode. You can either run it in attached mode to begin with by . docker start my-container --attach # OR docker start my-container -a . Or you can attach a running container later . docker attach my-container . ",
    "url": "/docs/docker/container.html#attach-container",
    "relUrl": "/docs/docker/container.html#attach-container"
  },"234": {
    "doc": "Correlation",
    "title": "Correlation",
    "content": "Correlation is a measure of how two variables are related to each other (i.e. how they trend or move together). It is important to note that correlation does not imply causality. Quite obviously, correlation is high for variables that are interdependent. So you should make sure that variables are independent before you make any conclusions based on correlation. | Pearson’s correlation coefficient $r$ . | Pearson’s $r$ measures linear relationship | Not a linear regresion | Assumes bivariate normal distribution | Very sensitive to outliers | . | Non-parametric correlation coefficients . | Spearman’s rank correlation coefficient $\\rho$ | Kendall rank correlation coefficient $\\tau$ | . | . ",
    "url": "/docs/statistics/notes/correlation.html",
    "relUrl": "/docs/statistics/notes/correlation.html"
  },"235": {
    "doc": "Correlation",
    "title": "Pearson’s correlation coefficient $r$",
    "content": "Pearson’s corrleation corefficient, commonly denoted as $r$, measures the linear relationship between two quantitative variables. It is the most common measure of correlation. Pearson’s correlation coefficient $r$ is defined as: . $$ r = \\frac{\\Cov[X, Y]}{\\sigma_X \\sigma_Y} $$ . It is basically the covariance divided by the standard deviations of $X$ and $Y$, which just confines the value of $r$ to the range $[-1, 1]$. We say that the correlation is stronger when $\\lvert r\\rvert$ is closer to $1$, and weaker when $\\lvert r\\rvert$ is closer to $0$. ",
    "url": "/docs/statistics/notes/correlation.html#pearsons-correlation-coefficient-r",
    "relUrl": "/docs/statistics/notes/correlation.html#pearsons-correlation-coefficient-r"
  },"236": {
    "doc": "Correlation",
    "title": "Pearson’s $r$ measures linear relationship",
    "content": "As mentioned above, Pearson’s $r$ is a measure of linear relationship. Therefore it is not suitable to measure a non-linear relationship with $r$. You cannot assume a positive linear relationship just because you have some positive $r$ value, because it may not be linear in the first place. You should always use the scatter plot first to check. ",
    "url": "/docs/statistics/notes/correlation.html#pearsons-r-measures-linear-relationship",
    "relUrl": "/docs/statistics/notes/correlation.html#pearsons-r-measures-linear-relationship"
  },"237": {
    "doc": "Correlation",
    "title": "Not a linear regresion",
    "content": "Even though Pearson’s $r$ is a measure of linear relationship, it is not the same as the slope of the regression line. Whether the linear regression on the scatter plot of $X$ and $Y$ has a slope of $0$ or $10$ does not affect the value of $r$. ",
    "url": "/docs/statistics/notes/correlation.html#not-a-linear-regresion",
    "relUrl": "/docs/statistics/notes/correlation.html#not-a-linear-regresion"
  },"238": {
    "doc": "Correlation",
    "title": "Assumes bivariate normal distribution",
    "content": "Because Pearson’s $r$ is a parametric measure, it assumes that both $X$ and $Y$ have normality. If not, it is not a good measure of correlation. ",
    "url": "/docs/statistics/notes/correlation.html#assumes-bivariate-normal-distribution",
    "relUrl": "/docs/statistics/notes/correlation.html#assumes-bivariate-normal-distribution"
  },"239": {
    "doc": "Correlation",
    "title": "Very sensitive to outliers",
    "content": "Pearson’s $r$ will change drastically in the presence of outliers. You should always check for outliers before using $r$. ",
    "url": "/docs/statistics/notes/correlation.html#very-sensitive-to-outliers",
    "relUrl": "/docs/statistics/notes/correlation.html#very-sensitive-to-outliers"
  },"240": {
    "doc": "Correlation",
    "title": "Non-parametric correlation coefficients",
    "content": "If any one of $X$ or $Y$ is not normally distributed, then Pearson’s $r$ is not a good measure of correlation. In this case, we can use non-parametric correlation coefficients. ",
    "url": "/docs/statistics/notes/correlation.html#non-parametric-correlation-coefficients",
    "relUrl": "/docs/statistics/notes/correlation.html#non-parametric-correlation-coefficients"
  },"241": {
    "doc": "Correlation",
    "title": "Spearman’s rank correlation coefficient $\\rho$",
    "content": "To be added . ",
    "url": "/docs/statistics/notes/correlation.html#spearmans-rank-correlation-coefficient-rho",
    "relUrl": "/docs/statistics/notes/correlation.html#spearmans-rank-correlation-coefficient-rho"
  },"242": {
    "doc": "Correlation",
    "title": "Kendall rank correlation coefficient $\\tau$",
    "content": "To be added . ",
    "url": "/docs/statistics/notes/correlation.html#kendall-rank-correlation-coefficient-tau",
    "relUrl": "/docs/statistics/notes/correlation.html#kendall-rank-correlation-coefficient-tau"
  },"243": {
    "doc": "Covariance",
    "title": "Covariance",
    "content": ". | Understanding covariance | . ",
    "url": "/docs/statistics/notes/covariance.html",
    "relUrl": "/docs/statistics/notes/covariance.html"
  },"244": {
    "doc": "Covariance",
    "title": "Understanding covariance",
    "content": "Covariance is a measure of how two variables vary together. It is defined as follows: . $$ \\begin{align*} \\Cov[X, Y] &amp;= \\E[(X - \\E[X])(Y - \\E[Y])] \\\\[1em] &amp;= \\E[XY] - \\E[X]\\E[Y] \\end{align*} $$ . As you can see from first line of the definition, covariance is determined by the product of the deviations of $X$ and $Y$ from their respective means. If they are both above the mean (increasing trend) or both below the mean (decreasing trend), then the product will be positive, resulting in a positive covariance. If they go opposite ways from their means, then the product will be negative, resulting in a negative covariance. ",
    "url": "/docs/statistics/notes/covariance.html#understanding-covariance",
    "relUrl": "/docs/statistics/notes/covariance.html#understanding-covariance"
  },"245": {
    "doc": "CSR",
    "title": "Certificate Signing Request (CSR)",
    "content": ". | Generate using Java Keytool . | Generate new keystore | Generate CSR | (Optional) Extract PEM file from keystore . | Create p12 file | Create pem file | . | . | . ",
    "url": "/docs/security/ssl/csr.html#certificate-signing-request-csr",
    "relUrl": "/docs/security/ssl/csr.html#certificate-signing-request-csr"
  },"246": {
    "doc": "CSR",
    "title": "Generate using Java Keytool",
    "content": "Generate new keystore . keytool -genkey -keyalg rsa -keysize 2048 # To use basic info keytool -genkey -keyalg rsa -keysize 2048 \\ -dname \"cn=${yourdomain}, o=default, c=us\" \\ -keystore ${keystore_name}.keystore . ${yourdomain} must match exactly the domain name written in the SSL. Change keyalg in all commands if you want to use a different algorithm. Generate CSR . keytool -certreq -keyalg rsa -file ${csr_name}.csr -keystore ${keystore_name}.keystore . (Optional) Extract PEM file from keystore . Create p12 file . keytool -importkeystore -srckeystore ${keystore_name}.keystore \\ -destkeystore ${keystore_name}.p12 -deststoretype PKCS12 . Create pem file . openssl pkcs12 -in ${keystore_name}.p12 -nodes -nocerts -out ${keystore_name}.pem . ",
    "url": "/docs/security/ssl/csr.html#generate-using-java-keytool",
    "relUrl": "/docs/security/ssl/csr.html#generate-using-java-keytool"
  },"247": {
    "doc": "CSR",
    "title": "CSR",
    "content": " ",
    "url": "/docs/security/ssl/csr.html",
    "relUrl": "/docs/security/ssl/csr.html"
  },"248": {
    "doc": "Decision Tree / Random Forest",
    "title": "Decision Tree / Random Forest",
    "content": ". | Decision tree . | Feature selection | Advantages | Disadvantages | . | Random forest . | Ensemble methods | . | . ",
    "url": "/docs/statistics/notes/decision-tree.html",
    "relUrl": "/docs/statistics/notes/decision-tree.html"
  },"249": {
    "doc": "Decision Tree / Random Forest",
    "title": "Decision tree",
    "content": "Decision tree (DT) is a non-parametric supervised learning method. It can be used for both classification and regression. The general idea is, given a bunch of data points with features and labels, we want to build a tree that can predict the label of a new data point. We start with the root node and split the data into two groups based on a feature. We then repeat this process for each of the two groups until we reach a leaf node, which is the prediction. ",
    "url": "/docs/statistics/notes/decision-tree.html#decision-tree",
    "relUrl": "/docs/statistics/notes/decision-tree.html#decision-tree"
  },"250": {
    "doc": "Decision Tree / Random Forest",
    "title": "Feature selection",
    "content": "While decision trees are very intuitive, question remains: . How do we select which feature to split on in each layer of the tree? . We want to pick the feature that gives us the most information about the label. For that we need an understanding of entropy and mutual information. In short, entropy is the expected amount of information needed, and mutual information is the amount of information gain on one variable when we are given another, measured in terms of entropy. Let $Y$ be the label and $X_i$ be the $i$-th feature. For each feature $X_i$, we can calculate the mutual information between $X_i$ and $Y$. Higher mutual information means that $X_i$ is more informative about $Y$, which is what we want. \\[\\underset{X_i}{\\arg\\max} \\; I(Y; X_i) = \\underset{X_i}{\\arg\\max} \\; H(Y) - H(Y \\mid X_i)\\] Caveat One caveat is that mutual information is biased towards features with more partition. For two features ID and gender, while there are many distinct values that ID can take on, gender only has so many. So when you split on a feature with many distinct values (quantitative or not), we need to penalize by the number of splits and use a metric called information gain ratio, or just partition on a reasonable range. ",
    "url": "/docs/statistics/notes/decision-tree.html#feature-selection",
    "relUrl": "/docs/statistics/notes/decision-tree.html#feature-selection"
  },"251": {
    "doc": "Decision Tree / Random Forest",
    "title": "Advantages",
    "content": ". | Simple to understand and interpret | Does not require much data preprocessing (such as scaling) | Robust to outliers | Able to handle both qualitative and quantitative features and labels | . ",
    "url": "/docs/statistics/notes/decision-tree.html#advantages",
    "relUrl": "/docs/statistics/notes/decision-tree.html#advantages"
  },"252": {
    "doc": "Decision Tree / Random Forest",
    "title": "Disadvantages",
    "content": ". | Prone to overfitting . | “Pruning” unnecessary branches can help | . | Feature value partitions can cause bias (see “Caveat” above) | Because tree nodes are decided by probability calculation, you may end up with completely different trees for “inherently” similar data when you slightly change the data . | This unstable nature can be mitigated by using random forests | . | . ",
    "url": "/docs/statistics/notes/decision-tree.html#disadvantages",
    "relUrl": "/docs/statistics/notes/decision-tree.html#disadvantages"
  },"253": {
    "doc": "Decision Tree / Random Forest",
    "title": "Random forest",
    "content": "Random forest is an ensemble learning that combines several decision trees to mitigate overfitting and improve model stability. ",
    "url": "/docs/statistics/notes/decision-tree.html#random-forest",
    "relUrl": "/docs/statistics/notes/decision-tree.html#random-forest"
  },"254": {
    "doc": "Decision Tree / Random Forest",
    "title": "Ensemble methods",
    "content": "As described above, decision trees tend to overfit and are unstable upon slightly varying data. To mitigate these issues, random forest uses an ensemble of decision trees. Random forest uses bootstrap aggregating (bagging) and random subspace method. Instead of building a single tree using all the observation and features all at once, we create multiple smaller training samples with limited features. Hyperparameters would be the number of trees, the number of bootstrap samples, and the number of features to use for each tree. ",
    "url": "/docs/statistics/notes/decision-tree.html#ensemble-methods",
    "relUrl": "/docs/statistics/notes/decision-tree.html#ensemble-methods"
  },"255": {
    "doc": "Determinant",
    "title": "Determinant of a Square Matrix",
    "content": ". | Determinant . | Singular matrix | Properties of the determinant | . | Laplace expansion . | Singleton matrix case | General case . | Expansion along a row | Expansion along a column | . | 2x2 matrix application | . | Leibniz formula | Rule of Sarrus | Geometrical interpretation | . ",
    "url": "/docs/linalg/notes/determinant.html#determinant-of-a-square-matrix",
    "relUrl": "/docs/linalg/notes/determinant.html#determinant-of-a-square-matrix"
  },"256": {
    "doc": "Determinant",
    "title": "Determinant",
    "content": "Determinant of a $n \\times n$ square matrix $A$ is denoted: . $$ \\det(A) \\quad \\text{or} \\quad |A| $$ . ",
    "url": "/docs/linalg/notes/determinant.html",
    "relUrl": "/docs/linalg/notes/determinant.html"
  },"257": {
    "doc": "Determinant",
    "title": "Singular matrix",
    "content": "The following is an important property of the determinant. For a square matrix $A$, . $$ \\det(A) = 0 \\quad \\iff \\quad A \\text{ is singular} \\quad \\iff \\nexists A^{-1} $$ . Thus a singular matrix is not invertible, and the system of linear equations $Ax = b$ has no unique solution. ",
    "url": "/docs/linalg/notes/determinant.html#singular-matrix",
    "relUrl": "/docs/linalg/notes/determinant.html#singular-matrix"
  },"258": {
    "doc": "Determinant",
    "title": "Properties of the determinant",
    "content": "Let $A$, $B$, $C$ be square matrices of same size and $k$ a scalar: . | $\\det(kA) = k^n \\det(A)$ . | Multiplying a single row or column by $k$ multiplies the determinant by $k$. | Multiplying all rows or columns by $k$ multiplies the determinant by $k^n$. | . | If $A$, $B$, $C$ only differ by a single (same) row or column $i$, and the different row or column of $C_i = A_i + B_i$, then $\\det(C) = \\det(A) + \\det(B)$. | If $A$ is obtained by swapping two rows or columns of $B$, then $\\det(A) = -\\det(B)$. | If $A$ has two identical rows or columns, then $\\det(A) = 0$. | Adding a multiple of one row or column to another row or column does not change the determinant. | $\\det(A) = \\det(A^T)$ | $\\det(I) = 1$ | If any row or column of $A$ is all zeros, then $\\det(A) = 0$. | $\\det(AB) = \\det(A) \\det(B)$ | If $A$ is triangular or diagonal, then $\\det(A) = \\prod_{i=1}^n a_{ii}$. | $\\det(A^{-1}) = \\frac{1}{\\det(A)}$ | $\\det(\\operatorname{adj}(A)) = \\det(A)^{n-1}$ | . ",
    "url": "/docs/linalg/notes/determinant.html#properties-of-the-determinant",
    "relUrl": "/docs/linalg/notes/determinant.html#properties-of-the-determinant"
  },"259": {
    "doc": "Determinant",
    "title": "Laplace expansion",
    "content": "You need to understand minor (determinants) and cofactor before continuing. Laplace expansion is a recursive method to calculate the determinant of a matrix. ",
    "url": "/docs/linalg/notes/determinant.html#laplace-expansion",
    "relUrl": "/docs/linalg/notes/determinant.html#laplace-expansion"
  },"260": {
    "doc": "Determinant",
    "title": "Singleton matrix case",
    "content": "For a $1 \\times 1$ matrix $A = [a]$, the determinant is defined as simply: . $$ a $$ . ",
    "url": "/docs/linalg/notes/determinant.html#singleton-matrix-case",
    "relUrl": "/docs/linalg/notes/determinant.html#singleton-matrix-case"
  },"261": {
    "doc": "Determinant",
    "title": "General case",
    "content": "With the base case in mind, we can recursively calculate the determinant of a matrix by expanding along a row or a column. Expansion along a row . For a $n \\times n$ matrix $A$, we can fix a row $i$ and expand along that row. Usually, we expand along the first row $i = 1$. The determinant of $A$ is the sum of the products of the elements on the row $i$ and their cofactors. $$ \\det(A) = \\sum_{j=1}^n a_{ij} \\cdot (-1)^{i+j} M_{ij} $$ . Expansion along the row is more common than expansion along the column. Expansion along a column . Same idea, we can fix a column $j$ and expand along that column. The determinant of $A$ is the sum of the products of the elements on the column $j$ and their cofactors. \\[\\det(A) = \\sum_{i=1}^n a_{ij} \\cdot (-1)^{i+j} M_{ij}\\] ",
    "url": "/docs/linalg/notes/determinant.html#general-case",
    "relUrl": "/docs/linalg/notes/determinant.html#general-case"
  },"262": {
    "doc": "Determinant",
    "title": "2x2 matrix application",
    "content": "Let’s apply the Laplace expansion to a $2 \\times 2$ matrix $A$. \\[A = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\] Expansion along the first row $i = 1$. \\[\\begin{align*} \\det(A) &amp;= \\sum_{j=1}^2 a_{1j} \\cdot (-1)^{1+j} M_{1j} \\\\ &amp;= a \\cdot (-1)^{1+1} M_{11} + b \\cdot (-1)^{1+2} M_{12} \\\\ &amp;= a \\cdot M_{11} - b \\cdot M_{12} \\\\ &amp;= a \\cdot d - b \\cdot c \\end{align*}\\] The determinant of a $2 \\times 2$ matrix $A$ is thus defined as: . $$ \\det(A) = ad - bc $$ . ",
    "url": "/docs/linalg/notes/determinant.html#2x2-matrix-application",
    "relUrl": "/docs/linalg/notes/determinant.html#2x2-matrix-application"
  },"263": {
    "doc": "Determinant",
    "title": "Leibniz formula",
    "content": "Leibniz formula is another way to calculate the determinant of a square matrix. It comes from the observation that the full determinant expansion can be written as a special sum of products of elements. Explanation assumes expansion along row $i$ for simplicity. The key takeaway is that each recursive product in the sum ends up looking like the following: . \\[\\prod_{i=1}^n a_{i\\sigma(i)}\\] That is, while the row $i$ increases from $1$ to $n$ (or is an identity permutation), the column is actually permuted by some $\\sigma$. Like $a_{12} \\cdot a_{23} \\cdot a_{31}$, where the column $j$ is permuted to $2, 3, 1$. In addition, the sign of the recursive product ends up equal to the parity of the permutation $\\sigma$. Therefore the determinant of $A$ can be written as: . $$ \\det(A) = \\sum_{\\sigma \\in S_n} \\text{sgn}(\\sigma) \\prod_{i=1}^n a_{i\\sigma(i)} $$ . Or equivalently, . \\[\\det(A) = \\sum_{\\sigma \\in S_n} \\text{sgn}(\\sigma) \\prod_{i=1}^n a_{\\sigma(i)i}\\] . ",
    "url": "/docs/linalg/notes/determinant.html#leibniz-formula",
    "relUrl": "/docs/linalg/notes/determinant.html#leibniz-formula"
  },"264": {
    "doc": "Determinant",
    "title": "Rule of Sarrus",
    "content": "This is a special method for the determinant of a $3 \\times 3$ matrix. Go to Wikipedia . ",
    "url": "/docs/linalg/notes/determinant.html#rule-of-sarrus",
    "relUrl": "/docs/linalg/notes/determinant.html#rule-of-sarrus"
  },"265": {
    "doc": "Determinant",
    "title": "Geometrical interpretation",
    "content": "In 2D, the determinant of a $2 \\times 2$ matrix can be interpreted as the signed area of the parallelogram formed by the two column vectors. In 3D, the determinant of a $3 \\times 3$ matrix can be interpreted as the signed volume of the parallelepiped formed by the three column vectors. A parallelepiped is a 3D generalization of a parallelogram (i.e. a linearly transformed cube). ",
    "url": "/docs/linalg/notes/determinant.html#geometrical-interpretation",
    "relUrl": "/docs/linalg/notes/determinant.html#geometrical-interpretation"
  },"266": {
    "doc": "Diagonal of a Matrix",
    "title": "Diagonal of a Matrix",
    "content": ". | Principal diagonal . | Trace | . | Diagonal matrix . | Properties of a diagonal matrix | . | Triangular matrix . | Upper triangular matrix | Lower triangular matrix | . | . ",
    "url": "/docs/linalg/notes/diagonal.html",
    "relUrl": "/docs/linalg/notes/diagonal.html"
  },"267": {
    "doc": "Diagonal of a Matrix",
    "title": "Principal diagonal",
    "content": "The principal diagonal of a square matrix is the diagonal from the upper left to the lower right. $$ \\{a_{ij} \\mid i = j\\} $$ . Also called the main diagonal. Each element $a_{ii}$ is called a principal diagonal element. ",
    "url": "/docs/linalg/notes/diagonal.html#principal-diagonal",
    "relUrl": "/docs/linalg/notes/diagonal.html#principal-diagonal"
  },"268": {
    "doc": "Diagonal of a Matrix",
    "title": "Trace",
    "content": "A trace of a square matrix $A$ is the sum of its principal diagonal elements: . $$ tr(A) = \\sum_{i=1}^n a_{ii} $$ . ",
    "url": "/docs/linalg/notes/diagonal.html#trace",
    "relUrl": "/docs/linalg/notes/diagonal.html#trace"
  },"269": {
    "doc": "Diagonal of a Matrix",
    "title": "Diagonal matrix",
    "content": "A diagonal matrix is a square matrix whose non-principal diagonal elements are all zero. \\[A = \\begin{bmatrix} a_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; a_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; a_{nn} \\end{bmatrix}\\] Often denoted by: . $$ A = diag(a_{11}, a_{22}, \\cdots, a_{nn}) $$ . ",
    "url": "/docs/linalg/notes/diagonal.html#diagonal-matrix",
    "relUrl": "/docs/linalg/notes/diagonal.html#diagonal-matrix"
  },"270": {
    "doc": "Diagonal of a Matrix",
    "title": "Properties of a diagonal matrix",
    "content": "For diagonal matrices $A = diag(a_{11}\\cdots, a_{nn})$ and $B = diag(b_{11}\\cdots, b_{nn})$: . | $A + B = diag(a_{11} + b_{11}, \\cdots, a_{nn} + b_{nn})$ | $A B = diag(a_{11} b_{11}, \\cdots, a_{nn} b_{nn})$ | . ",
    "url": "/docs/linalg/notes/diagonal.html#properties-of-a-diagonal-matrix",
    "relUrl": "/docs/linalg/notes/diagonal.html#properties-of-a-diagonal-matrix"
  },"271": {
    "doc": "Diagonal of a Matrix",
    "title": "Triangular matrix",
    "content": "For both upper and lower triangular matrices, if the principal diagonal elements are all 1, then the matrix is called a unit (upper/lower) triangular matrix. ",
    "url": "/docs/linalg/notes/diagonal.html#triangular-matrix",
    "relUrl": "/docs/linalg/notes/diagonal.html#triangular-matrix"
  },"272": {
    "doc": "Diagonal of a Matrix",
    "title": "Upper triangular matrix",
    "content": "An upper triangular matrix is a square matrix whose non-principal diagonal elements below the principal diagonal are all zero. Upper triangular matrix, often denoted by $U_n$ is: . $$ U_n = [u_{ij}]_{n \\times n} \\quad \\text{where} \\quad \\forall i &gt; j,\\; u_{ij} = 0 $$ . Example: . \\[\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 4 &amp; 5 \\\\ 0 &amp; 0 &amp; 6 \\end{bmatrix}\\] ",
    "url": "/docs/linalg/notes/diagonal.html#upper-triangular-matrix",
    "relUrl": "/docs/linalg/notes/diagonal.html#upper-triangular-matrix"
  },"273": {
    "doc": "Diagonal of a Matrix",
    "title": "Lower triangular matrix",
    "content": "An lower triangular matrix is a square matrix whose non-principal diagonal elements above the principal diagonal are all zero. Lower triangular matrix, often denoted by $L_n$ is: . $$ L_n = [l_{ij}]_{n \\times n} \\quad \\text{where} \\quad \\forall i &lt; j,\\; l_{ij} = 0 $$ . Example: . \\[\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 3 &amp; 0 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix}\\] ",
    "url": "/docs/linalg/notes/diagonal.html#lower-triangular-matrix",
    "relUrl": "/docs/linalg/notes/diagonal.html#lower-triangular-matrix"
  },"274": {
    "doc": "Dockerized Jenkins",
    "title": "Dockerized Jenkins",
    "content": ". | Docker Installation . | Disble built-in node executors on initialization | . | Configure Docker node . | TLS credentials for Docker | Configure Docker . | Docker Host URI | Server credentials | Enabled | . | Configure Docker Agent . | Label, Name | Docker Image | Remote File System Root | Usage | Connect method | . | . | Use Docker agent in jobs | . ",
    "url": "/docs/jenkins/docker.html",
    "relUrl": "/docs/jenkins/docker.html"
  },"275": {
    "doc": "Dockerized Jenkins",
    "title": "Docker Installation",
    "content": "Consider using docker-compose to manage the containers. It makes your life easier. Official Documentation . The details are well documented in the link above. The above documentation describes the need for two containers, one is a DinD (Docker in Docker) container and the other is the master Jenkins container. The DinD container is used to enable Docker in the Jenkins container. This is accomplished by placing both containers in the same network and exposing the DinD TLS port (2376). It is recommended to map both volumes jenkins-docker-certs and jenkins-data to the host filesystem to persist the data between container restarts. For the Jenkins container, the documentation recommends two ports to be exposed: 8080 for the web UI and 50000 for the slave agent. However, if you are planning to use SSH agents you do not need to expose the 50000 port. Disble built-in node executors on initialization . This is a recommendation described in the Github documentation. Because you generally want to use agents to run your builds, it is advised to set number of executors in the built-in node to zero. You can do so by creating executors.groovy, . // executors.groovy import jenkins.model.* Jenkins.instance.setNumExecutors(0) . And a Dockerfile extending the jenkins/jenkins image, . # Dockerfile FROM jenkins/jenkins:lts-jdk11 COPY --chown=jenkins:jenkins executors.groovy /usr/share/jenkins/ref/init.groovy.d/executors.groovy . This Dockerfile can be further extended to initialize plugins and configure Jenkins. ",
    "url": "/docs/jenkins/docker.html#docker-installation",
    "relUrl": "/docs/jenkins/docker.html#docker-installation"
  },"276": {
    "doc": "Dockerized Jenkins",
    "title": "Configure Docker node",
    "content": "TLS credentials for Docker . First navigate to Manage Jenkins &gt; Manage Credentials, . Then create a global-scope credential of type X.509 Certificate. This is the TLS certificate used by the DinD container to authenticate with the Jenkins container. Assuming the name of the DinD container is jenkins-docker, you can get the required values by, . # Client Key docker exec jenkins-docker cat /certs/key.pem | pbcopy # Client Certificate docker exec jenkins-docker cat /certs/cert.pem | pbcopy # Server CA Certificate docker exec jenkins-docker cat /certs/ca.pem | pbcopy . It is useful to set the credential ID to something you can recognize, like docker-tls. Configure Docker . Now navigate to Manage Jenkins &gt; Manage Nodes and Clouds &gt; Configure Clouds, . Then create a new cloud of type Docker, . Docker Host URI . tcp://docker:2376 or leave blank if you’ve already set the DOCKER_HOST environment variable. Server credentials . Select the credential you created above, i.e. docker-tls. Enabled . Make sure it is checked. Configure Docker Agent . Then add an agent with Docker Agent Template, . Label, Name . Set Label to something you can recognize because it will be used in your pipeline, i.e. test-agent . Just give it a matching name. Docker Image . You need to have a Docker image pushed to a remote repository for this option, or prepare local image beforehand. Image can be from a public or private repository, but if you’re using a private one, remeber to generate an access token from Docker Hub and add it to Jenkins via Registry Authentication. The Docker image should be based on the functionalities of either jenkins/agent or jenkins/ssh-agent image. The former uses JNLP for connection to agents, while the latter uses SSH. Enter the image name and tag. Remote File System Root . Set it to /home/jenkins. Usage . I prefer Only build jobs with label expressions matching this node because you can specify types of jobs to run on this agent in the pipeline configuration. Connect method . This depends on the Docker image you configured above. If your image is Jenkins agent executable/JNLP based, select Connect with JNLP. If your image is SSH based, select Connect with SSH. Following example is for SSH agents. ",
    "url": "/docs/jenkins/docker.html#configure-docker-node",
    "relUrl": "/docs/jenkins/docker.html#configure-docker-node"
  },"277": {
    "doc": "Dockerized Jenkins",
    "title": "Use Docker agent in jobs",
    "content": "Create a new FreeStyle project for testing purposes. In General, set the following to a label you configured above. References: . | Jenkins Docker GitHub Documentation | . ",
    "url": "/docs/jenkins/docker.html#use-docker-agent-in-jobs",
    "relUrl": "/docs/jenkins/docker.html#use-docker-agent-in-jobs"
  },"278": {
    "doc": "DynamoDB",
    "title": "DynamoDB",
    "content": ". | Local Setup | NoSQL Workbench for DynamoDB | Key design / Data model . | Primary Key | Design | . | Itty Bitties | . ",
    "url": "/docs/aws/dynamodb.html",
    "relUrl": "/docs/aws/dynamodb.html"
  },"279": {
    "doc": "DynamoDB",
    "title": "Local Setup",
    "content": "Detailed documentation is provided here. Docker option is available as well. ",
    "url": "/docs/aws/dynamodb.html#local-setup",
    "relUrl": "/docs/aws/dynamodb.html#local-setup"
  },"280": {
    "doc": "DynamoDB",
    "title": "NoSQL Workbench for DynamoDB",
    "content": "This will be a great lifesaver while designing data models and testing connection. You can download it here. ",
    "url": "/docs/aws/dynamodb.html#nosql-workbench-for-dynamodb",
    "relUrl": "/docs/aws/dynamodb.html#nosql-workbench-for-dynamodb"
  },"281": {
    "doc": "DynamoDB",
    "title": "Key design / Data model",
    "content": "If you are used to relational database schemas, it is easy to end up designing your database to use multiple tables, to structure logical joins using foreign key-like attribute and what not, or to use multi-level nested structure. However, in NoSQL, all these familiar patterns are not only inefficient, but also almost impossible to manage. There really is no such thing as a schema design in DynamoDB but a careful design of primary key is useful. Primary Key . There are two types of keys that can consist a primary key in DynamoDB: partition (hash) key and sort (range) key. A primary key could just consist of a partition key or be a compound of partition and sort key. Because each item is identified by a unique primary key, you must use a unique partition key if your primary key only consists of it. However, if you also use the sort key, the partition key may overlap but the sort key must be unique. Partition key and sort key are also called hash and range keys. The naming indicates that the partition key serves as a hashed index to a physical storage internal unit called a partition. The sort key sorts the items within a partition into groups of similar items, effectively providing an efficient way to query for a range. Hence, design of primary key has an impact on the performance of the DB. Design . In relational databases, primary keys are usually a single attribute (like StudentID) of a homogeneous type. However, in DynamoDB it is common to use a multi-purpose (or heterogeneous) key attributes. Typically, every item is given an attribute called PK and SK for partition and sort key. This way the key attributes may contain any information without restriction. ",
    "url": "/docs/aws/dynamodb.html#key-design--data-model",
    "relUrl": "/docs/aws/dynamodb.html#key-design--data-model"
  },"282": {
    "doc": "DynamoDB",
    "title": "Itty Bitties",
    "content": ". | Compared to SQL statements, querying in DynamoDB can be a real pain in the ass… | . ",
    "url": "/docs/aws/dynamodb.html#itty-bitties",
    "relUrl": "/docs/aws/dynamodb.html#itty-bitties"
  },"283": {
    "doc": "ECR",
    "title": "AWS Elatic Container Registry",
    "content": ". | Pushing image to ECR . | Authenticate Docker | Build or tag image | Push image | . | . ",
    "url": "/docs/aws/ecr.html#aws-elatic-container-registry",
    "relUrl": "/docs/aws/ecr.html#aws-elatic-container-registry"
  },"284": {
    "doc": "ECR",
    "title": "Pushing image to ECR",
    "content": "Authenticate Docker . First you must authenticate Docker to push to your ECR registry. You must first configure AWS CLI with your credentials. aws ecr get-login-password --profile ${profile} | docker login --username AWS --password-stdin ${account_id}.dkr.ecr.${region}.amazonaws.com . &lt;account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com is the URI of your ECR registry. Authentication is only valid for 12 hours. Build or tag image . Your image must have a tag that matches the URI of your ECR registry. docker build -t ${account_id}.dkr.ecr.${region}.amazonaws.com/${repo_name}:${tag} . -f Dockerfile # OR docker tag ${image_id} ${account_id}.dkr.ecr.${region}.amazonaws.com/${repo_name}:${tag} . Push image . docker push ${account_id}.dkr.ecr.${region}.amazonaws.com/${repo_name}:${tag} . References: . | AWS ECR: Pushing a Docker image | . ",
    "url": "/docs/aws/ecr.html#pushing-image-to-ecr",
    "relUrl": "/docs/aws/ecr.html#pushing-image-to-ecr"
  },"285": {
    "doc": "ECR",
    "title": "ECR",
    "content": " ",
    "url": "/docs/aws/ecr.html",
    "relUrl": "/docs/aws/ecr.html"
  },"286": {
    "doc": "Ensemble Learning",
    "title": "Ensemble Learning",
    "content": ". | Ensemble methods . | Bootstrap aggregating . | Bootstrap sampling | Aggregating | . | Random subspace method | . | . ",
    "url": "/docs/statistics/notes/ensemble.html",
    "relUrl": "/docs/statistics/notes/ensemble.html"
  },"287": {
    "doc": "Ensemble Learning",
    "title": "Ensemble methods",
    "content": "Ensemble methods are learning algorithms that combine several models and then classify new data points by taking a (weighted) vote of their predictions. The idea is that the ensemble model will be more robust and accurate than any individual model. ",
    "url": "/docs/statistics/notes/ensemble.html#ensemble-methods",
    "relUrl": "/docs/statistics/notes/ensemble.html#ensemble-methods"
  },"288": {
    "doc": "Ensemble Learning",
    "title": "Bootstrap aggregating",
    "content": "Bootstrap aggregating (or bagging) is a technique that improves stability by training each model on a different subset of the data. As the name suggests, it involves two steps: . Bootstrap sampling . Given $n$ observations, we randomly sample a subset with replacement. We repeat this process $k$ times to get $k$ independent subsets. Allowing replacement is critical to make each sample independent. Aggregating . We train a model on each of the $k$ subsets and then combine the result. ",
    "url": "/docs/statistics/notes/ensemble.html#bootstrap-aggregating",
    "relUrl": "/docs/statistics/notes/ensemble.html#bootstrap-aggregating"
  },"289": {
    "doc": "Ensemble Learning",
    "title": "Random subspace method",
    "content": "Random subspace method is another technique in ensemble learning that reduces correlation between each base model. They are also called attribute bagging or feature bagging. The idea is almost the same as bagging, except that while bagging is a sampling on the training data, random subspace method is a sampling on the predictors. By disallowing each model to see the full set of predictors, we reduce the correlation between them and improve generalization. ",
    "url": "/docs/statistics/notes/ensemble.html#random-subspace-method",
    "relUrl": "/docs/statistics/notes/ensemble.html#random-subspace-method"
  },"290": {
    "doc": "Entropy / KL Divergence / Cross Entropy",
    "title": "Entropy / KL Divergence / Cross Entropy",
    "content": ". | Informational value (surprise) | Entropy | KL Divergence . | Some properties . | Another way to write KL divergence | KL divergence is not symmetric | . | . | Cross Entropy | Conditional entropy . | Mutual information | . | . ",
    "url": "/docs/statistics/notes/entropy.html",
    "relUrl": "/docs/statistics/notes/entropy.html"
  },"291": {
    "doc": "Entropy / KL Divergence / Cross Entropy",
    "title": "Informational value (surprise)",
    "content": "If a variable has a higher “surprise”, we say that it needs more information to describe it due to its uncertainty. A surprising event is one that has a low probability of occurring. Then it is clear that probability $p$ has an inverse relationship with surprise: . \\[\\text{surprise} \\propto \\log\\left(\\frac{1}{p}\\right)\\] Why the log? Intuitively, it makes sense that surprise should be infinitely large when $p = 0$. Infinitely surprised when something impossible happens. On the other hand, surprise should be $0$ when $p = 1$. However with just $\\frac{1}{p}$, we get a value of $1$ when $p = 1$. We put a $\\log$ in front so that: . \\[\\text{surprise} = \\log\\left(\\frac{1}{1}\\right) = 0\\] We can do this because $\\log$ is a monotonic function. Also $\\log$ makes math easier. Define $p_X$ as the probability function of random variable $X$. Therefore we define surprise of $x \\in X$ as: . $$ -\\log(p_X(x)) $$ . ",
    "url": "/docs/statistics/notes/entropy.html#informational-value-surprise",
    "relUrl": "/docs/statistics/notes/entropy.html#informational-value-surprise"
  },"292": {
    "doc": "Entropy / KL Divergence / Cross Entropy",
    "title": "Entropy",
    "content": "Entropy is a measure of surprise/uncertainty in a random variable. It is actually the expected value of surprise: . $$ H(X) = E[-\\log(p_X)] = -\\sum_{x \\in X} p_X(x) \\log(p_X(x)) $$ . For continuous random variables, just use the integral. ",
    "url": "/docs/statistics/notes/entropy.html#entropy",
    "relUrl": "/docs/statistics/notes/entropy.html#entropy"
  },"293": {
    "doc": "Entropy / KL Divergence / Cross Entropy",
    "title": "KL Divergence",
    "content": "Kullback-Leibler (KL) divergence is the statistical difference between two distributions. $$ D_{KL}(P\\mathbin{||}Q) = \\sum_{x \\in X} p_P(x) \\log\\left(\\frac{p(x)}{q(x)}\\right) $$ . Also known as relative entropy of $P$ with respect to $Q$, or how much extra surprise we get if we use $Q$ instead of $P$. Most common scenario is to compare the difference between the empirical distribution of the observations and the theoretical model distribution. Let $P$ be the empirical distribution and $Q$ be the theoretical model distribution. Intuitive way to remember this is to calculate the ratio of the probabilities of observing random variable $X$ under $P$ and $Q$: . \\[\\frac{p(x)}{q(x)}\\] Then we take the log of this ratio: . \\[\\log\\left(\\frac{p(x)}{q(x)}\\right)\\] This aligns with our intuition that we want to measure the statistical difference between the two distributions, because it is equal to the logarithmic difference $\\log(p(x)) - \\log(q(x))$. Then KL divergence is the expected value of this logarithmic difference under $P$: . \\[E_p\\left[\\log\\left(\\frac{p(X)}{q(X)}\\right)\\right]\\] ",
    "url": "/docs/statistics/notes/entropy.html#kl-divergence",
    "relUrl": "/docs/statistics/notes/entropy.html#kl-divergence"
  },"294": {
    "doc": "Entropy / KL Divergence / Cross Entropy",
    "title": "Some properties",
    "content": "Another way to write KL divergence . By logarithmic properties, . \\[D_{KL}(P\\mathbin{||}Q) = \\sum_{x \\in X} p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right) = - \\sum_{x \\in X} p(x) \\log\\left(\\frac{q(x)}{p(x)}\\right)\\] KL divergence is not symmetric . Algebraically, it is easy to see that $D_{KL}(P\\mathbin{||}Q) \\neq D_{KL}(Q\\mathbin{||}P)$. Intuitive way to remember Remeber that we can think of $P$ as the observed distribution and $Q$ as the theoretical model distribution. We usually want to estimate a model parameter that can minimize the difference with the observed distribution. But a model cannot always perfectly fit the observed distribution, but it will try its best. But two distributions may have different ideas on what counts as “best”. Maybe $P$ has a lot of bumps or outbursts, which may be important because they are caused by relevant events. However, a model distribution might have a tendency to flatten out the bumps and outbursts because it wants to generalize. For $D_{KL}(P\\mathbin{||}Q)$, we are looking at the difference from the empirical distribution perspective. When $P$ looks at $Q$, it may think the difference is big because they did not capture the important information, like the bump. For $D_{KL}(Q\\mathbin{||}P)$, we are looking at the difference from the model perspective. When $Q$ looks at $P$, it may think the difference is small because the mean is similar and the bumps are not important. ",
    "url": "/docs/statistics/notes/entropy.html#some-properties",
    "relUrl": "/docs/statistics/notes/entropy.html#some-properties"
  },"295": {
    "doc": "Entropy / KL Divergence / Cross Entropy",
    "title": "Cross Entropy",
    "content": "Cross entropy is a measure of the surprise we have if we use the estimated distribution $Q$ instead of true distribution $P$. Definition of cross entropy using KL divergence is an intuitive way to remember: . $$ H(P, Q) = H(P) + D_{KL}(P\\mathbin{||}Q) $$ . Information we need to describe $P$ when we use $Q$ to explain is equal to information needed for $P$ plus the extra information we need for using $Q$ instead. It is very easy to show that it leads to the following definition: . $$ H(P, Q) = -\\sum_{x \\in X} p(x) \\log(q(x)) $$ . ",
    "url": "/docs/statistics/notes/entropy.html#cross-entropy",
    "relUrl": "/docs/statistics/notes/entropy.html#cross-entropy"
  },"296": {
    "doc": "Entropy / KL Divergence / Cross Entropy",
    "title": "Conditional entropy",
    "content": "Conditional entropy of random variable $X$ given $Y$ is the amount of extra information needed to describe $X$ given $Y$. $$ H(X \\mid Y) = -\\sum_{x \\in X} \\sum_{y \\in Y} p_{XY}(x, y) \\log(p_{X \\mid Y}(x \\mid y)) $$ . ",
    "url": "/docs/statistics/notes/entropy.html#conditional-entropy",
    "relUrl": "/docs/statistics/notes/entropy.html#conditional-entropy"
  },"297": {
    "doc": "Entropy / KL Divergence / Cross Entropy",
    "title": "Mutual information",
    "content": "Mutual information is the amount of information that random variable $X$ and $Y$ share. $$ I(X; Y) = H(X) - H(X \\mid Y) = H(Y) - H(Y \\mid X) $$ . Also called “information gain of $\\mathbf{X}$” given $Y$. Intuition We need $H(X)$ much information to describe $X$. $H(X \\mid Y)$ is the amount of extra work we have to do from $Y$ in order to describe $X$. So $H(X) - H(X \\mid Y)$ is the amount of free info that $Y$ already gives us about $X$. So $I(X; Y)$ is the amount of information that $X$ and $Y$ already had in common, thus mutual information. It is also the “information gained about $X$” by knowing $Y$. ",
    "url": "/docs/statistics/notes/entropy.html#mutual-information",
    "relUrl": "/docs/statistics/notes/entropy.html#mutual-information"
  },"298": {
    "doc": "F-Score",
    "title": "F-Score",
    "content": ". | Confusion matrix | Precision and Recall . | Precision | Recall | . | F1-score | . ",
    "url": "/docs/statistics/notes/f1-score.html#f-score",
    "relUrl": "/docs/statistics/notes/f1-score.html#f-score"
  },"299": {
    "doc": "F-Score",
    "title": "Confusion matrix",
    "content": "| Actual\\Prediction | Positive | Negative | . | Positive | True Positive | False Negative | . | Negative | False Positive | True Negative | . ",
    "url": "/docs/statistics/notes/f1-score.html#confusion-matrix",
    "relUrl": "/docs/statistics/notes/f1-score.html#confusion-matrix"
  },"300": {
    "doc": "F-Score",
    "title": "Precision and Recall",
    "content": ". Walber, CC BY-SA 4.0, via Wikimedia Commons Precision . Also called the positive predict value (PPV), . \\[precision = \\frac{TP}{TP + FP}\\] Recall . Also called the sensitivity, . \\[recall = \\frac{TP}{TP + FN}\\] . ",
    "url": "/docs/statistics/notes/f1-score.html#precision-and-recall",
    "relUrl": "/docs/statistics/notes/f1-score.html#precision-and-recall"
  },"301": {
    "doc": "F-Score",
    "title": "F1-score",
    "content": "An F-score is a measure of a binary classification’s accuracy. There exists a general F-score of $F_\\beta$, where the $\\beta$ acts as a weight of importance for recall. However, the balanced F-score (or $F_1$ score), where precision and recall are considered equally, is the harmonic mean of precision and recall: . \\[F_1 = \\frac{2}{recall^{-1} + precision^{-1}} = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} = \\frac{TP}{TP + \\frac{1}{2}(FP + FN)}\\] Where $0 \\le F_1 \\le 1$. $F_1$ of $1.0$ means perfect precision and recall, while $0$ means either one of them was $0$. Always remeber that F1 score (and the whole TP, TN …) is dependent on which threshold was selected (by examining ROC, etc.). Low F1 score does not represent the performance of the entire model, but the performance at a certain classification threshold. ",
    "url": "/docs/statistics/notes/f1-score.html#f1-score",
    "relUrl": "/docs/statistics/notes/f1-score.html#f1-score"
  },"302": {
    "doc": "F-Score",
    "title": "F-Score",
    "content": ". ",
    "url": "/docs/statistics/notes/f1-score.html",
    "relUrl": "/docs/statistics/notes/f1-score.html"
  },"303": {
    "doc": "filter-repo",
    "title": "git filter-repo",
    "content": ". | Installation | Set subdirectory as root of its own repo . | Clone a fresh copy of the repo | Set the contents of subdirectory as the content of new repo | Create a new remote and push | . | Move multiple subdirectories to a new repo | Delete files from all commit history | . ",
    "url": "/docs/git-hub/git/filter-repo.html#git-filter-repo",
    "relUrl": "/docs/git-hub/git/filter-repo.html#git-filter-repo"
  },"304": {
    "doc": "filter-repo",
    "title": "Installation",
    "content": "Install via Homebrew: . brew install git-filter-repo . ",
    "url": "/docs/git-hub/git/filter-repo.html#installation",
    "relUrl": "/docs/git-hub/git/filter-repo.html#installation"
  },"305": {
    "doc": "filter-repo",
    "title": "Set subdirectory as root of its own repo",
    "content": "You could technically just copy over the directory and create a new repo. However, if you’d like to carry over commits that are relevant to the subdirectory to the new reoo, you can use git filter-repo to do so. Suppose you have a repo named test-repo with the following structure: . ├── dir1 ├── dir2 └── dir3 . And you want to move the contents of dir1 into a new repo named dir1-repo. Clone a fresh copy of the repo . Cloning a fresh copy before running git filter-repo is a recommended practice. git clone ${https_or_ssh_to_test_repo} dir1-repo . Set the contents of subdirectory as the content of new repo . cd dir1-repo git filter-repo --subdirectory-filter dir1 . Relevant commits should have been cherry picked as well. Create a new remote and push . Create a new repo on GitHub. Suppose its name is dir1-repo. First check if you still have the remote pointing to the original test-repo: . git remote -v . If you do, modify it: . Assuming origin is the name of the main upstream remote. git remote set-url origin ${https_or_ssh_to_dir1_repo} # OR if the remote settings were already purged git add remote origin ${https_or_ssh_to_dir1_repo} . Verify that a new remote origin has been set and push: . git push -u origin ${branch} . ",
    "url": "/docs/git-hub/git/filter-repo.html#set-subdirectory-as-root-of-its-own-repo",
    "relUrl": "/docs/git-hub/git/filter-repo.html#set-subdirectory-as-root-of-its-own-repo"
  },"306": {
    "doc": "filter-repo",
    "title": "Move multiple subdirectories to a new repo",
    "content": "Suppose you have a repo named test-repo with the following structure: . ├── dir1 ├── dir2 ├── dir3 └── dir4 . And you want to move dir1 and dir2 to a new repo named new-repo: . ├── dir1 └── dir2 . Steps are similar to above, except for the git filter-repo command: . git filter-repo --path dir1 --path dir2 . ",
    "url": "/docs/git-hub/git/filter-repo.html#move-multiple-subdirectories-to-a-new-repo",
    "relUrl": "/docs/git-hub/git/filter-repo.html#move-multiple-subdirectories-to-a-new-repo"
  },"307": {
    "doc": "filter-repo",
    "title": "Delete files from all commit history",
    "content": "To remove certain files from all commit history, use the following command: . git filter-repo --invert-paths --path &lt;file1&gt; --path &lt;file2&gt; . References . | git-filter-repo | GitHub: Splitting a subfolder out into a new repository | . ",
    "url": "/docs/git-hub/git/filter-repo.html#delete-files-from-all-commit-history",
    "relUrl": "/docs/git-hub/git/filter-repo.html#delete-files-from-all-commit-history"
  },"308": {
    "doc": "filter-repo",
    "title": "filter-repo",
    "content": " ",
    "url": "/docs/git-hub/git/filter-repo.html",
    "relUrl": "/docs/git-hub/git/filter-repo.html"
  },"309": {
    "doc": "Frontend Web",
    "title": "Frontend Web",
    "content": ". | SPA vs SSR vs Static Site . | Single-Page Application (SPA) | Server-Side Rendering | Static Site | . | . ",
    "url": "/docs/learned/frontend-web.html",
    "relUrl": "/docs/learned/frontend-web.html"
  },"310": {
    "doc": "Frontend Web",
    "title": "SPA vs SSR vs Static Site",
    "content": "Here is an attempt to understand the exact differences between the three. Single-Page Application (SPA) . An SPA uses CSR (client-side rendering). Just by that I can already see the glaring difference to SSR (server-side rendering). In CSR, as the name suggests the client (browser) dynamically renders the web app. All the HTML, CSS, and Javascript are loaded in the beginning of the app’s lifecycle. Script makes AJAX calls to the API when it needs new data and dynamically integrates it. So technically, there really is only one page that is being presented to the user, it’s just that the contents within the page change to meet your needs. One advantage of SPA is that it provides better UX, because there is little to no lag time during navigation within the app. This comes from the fact that SPA does not require duplicate resources again and again after each click unlike MPA (Multiple-Page Application)/SSR. Things that always stay static on a website, like the general frame or style can stay as is and only new data are fetched from server. One disadvantage is that it is generally considered to have poor SEO (Search-Engine Optimization) compared to server-side apps. This is because without JS rendering, the HTML of an SPA is pretty much empty. If you check the source code of an SPA (not from the console), you will see that it does not contain much other than all the scripts that are sitting and waiting to execute upon interaction. In addition, SPAs might not have unique URLs for each content delivered. In many cases the URL stays the same throughout the entire site. Therefore crawling and indexing becomes slow and difficult. Server-Side Rendering . With all that being said about SPA, SSR is easier to understand. When navigation happens (e.g via click), the server builds the page and hands it over to the browser. Within a browser, you will only see the resources that consist the current page that you are on. You can already see why this could be slow, since it’s like asking the chef to dip your nachos every single bite when you could’ve just had the chips and cheese in front of you and dip it yourself. Due to this nature of SSR, you will see the page flicker upon navigation unlike the smooth UX of SPA. However, the benefit of SSR compared to SPA is that it is more secure, less heavy on the browser (and no memory leaks), and better SEO. Static Site . Static sites do not have dynamic content and consist of only the static files (HTML, CSS, JS). You could think of this as if the SSR had already rendered every single page that the client might request and had it prepared for you. There is no backend component to static sites and no rendering is involved. ",
    "url": "/docs/learned/frontend-web.html#spa-vs-ssr-vs-static-site",
    "relUrl": "/docs/learned/frontend-web.html#spa-vs-ssr-vs-static-site"
  },"311": {
    "doc": "GitHub Integration",
    "title": "Github Integration",
    "content": ". | Setup | Deployment key for Github repo . | Generate a key pair in Jenkins container | Add private key to Jenkins credentials | Add public key to Github repo | . | GitHub webhook for Jenkins | Create a Jenkins item . | No ECDSA error | . | . ",
    "url": "/docs/jenkins/github.html#github-integration",
    "relUrl": "/docs/jenkins/github.html#github-integration"
  },"312": {
    "doc": "GitHub Integration",
    "title": "Setup",
    "content": "One way to install and experiment with Jenkins locally is to use Docker. Necessary steps are well documented and thoroughly explained in the official documentation. It is easier to manage the containers if you transcribe the commands in the docs to a docker-compose file. ",
    "url": "/docs/jenkins/github.html#setup",
    "relUrl": "/docs/jenkins/github.html#setup"
  },"313": {
    "doc": "GitHub Integration",
    "title": "Deployment key for Github repo",
    "content": "Generate a key pair in Jenkins container . Suppose you have a Jenkins container named jenkins running locally, . exec a shell in the container and generate a key pair in /var/jenkins_home/.ssh: . docker exec -it jenkins bash mkdir -p /var/jenkins_home/.ssh ssh-keygen -t ed25519 -f /var/jenkins_home/.ssh/jenkins_github . You will now have a key pair named jenkins_github in /var/jenkins_home/.ssh. Add private key to Jenkins credentials . Now log in to Jenkins Dashboard and navigate to Manage Jenkins &gt; Manage Credentials. Click on Add Credentials and select SSH Username with private key from the dropdown. Copy the contents of jenkins_github and paste it in the Private Key field. Add public key to Github repo . Go to your Github repo and navigate to Settings &gt; Deploy keys. Copy the contents of jenkins_github.pub and paste it in the Key field. ",
    "url": "/docs/jenkins/github.html#deployment-key-for-github-repo",
    "relUrl": "/docs/jenkins/github.html#deployment-key-for-github-repo"
  },"314": {
    "doc": "GitHub Integration",
    "title": "GitHub webhook for Jenkins",
    "content": "If you have not chosen to Install suggested plugins during the Jenkins setup, you may need to install Git plugin and GitHub plugin manually. To create a GitHub webhook, you need a working public URL. If you do not have one, you can use ngrok, etc. to create a forwarding URL for your Jenkins exposed port. Go to your Github repo and navigate to Settings &gt; Webhooks &gt; Add webhook. | Payload URL: Must be appended with /github-webhook/ to work with the GitHub plugin. | Content type: Must be set to application/json. | . ",
    "url": "/docs/jenkins/github.html#github-webhook-for-jenkins",
    "relUrl": "/docs/jenkins/github.html#github-webhook-for-jenkins"
  },"315": {
    "doc": "GitHub Integration",
    "title": "Create a Jenkins item",
    "content": "TBA . No ECDSA error . If you encounter the following error while configuring the URL for the repo, . No ECDSA host key is known for github.com and you have requested strict checking. Navigate to Manage Jenkins &gt; Configure Global Security, . Find Git Host Key Verification Configuration and set Host Key Verification Strategy to Accept first connection. ",
    "url": "/docs/jenkins/github.html#create-a-jenkins-item",
    "relUrl": "/docs/jenkins/github.html#create-a-jenkins-item"
  },"316": {
    "doc": "GitHub Integration",
    "title": "GitHub Integration",
    "content": " ",
    "url": "/docs/jenkins/github.html",
    "relUrl": "/docs/jenkins/github.html"
  },"317": {
    "doc": "Hyperparameters",
    "title": "Hyperparameters",
    "content": ". | Grid search | . ",
    "url": "/docs/data-science/notes/hyperparameters.html",
    "relUrl": "/docs/data-science/notes/hyperparameters.html"
  },"318": {
    "doc": "Hyperparameters",
    "title": "Grid search",
    "content": "To be added . ",
    "url": "/docs/data-science/notes/hyperparameters.html#grid-search",
    "relUrl": "/docs/data-science/notes/hyperparameters.html#grid-search"
  },"319": {
    "doc": "Type I and Type II Errors",
    "title": "Type I and Type II Errors",
    "content": ". | Truth Table | Type I Error | Type II Error . | Power of Test | . | Effect Size | Relationship between $\\alpha$ and $\\beta$ | Sample Size Planning | . ",
    "url": "/docs/statistics/basics/hypothesis-testing-errors.html",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-errors.html"
  },"320": {
    "doc": "Type I and Type II Errors",
    "title": "Truth Table",
    "content": "In hypothesis testing, we have two truths: . | Null hypothesis ($H_0$) is true | Alternative hypothesis ($H_a$) is true | . Based on the results of the test, we make two decisions: . | Reject the null hypothesis | Fail to reject the null hypothesis | . Then we have a truth table of the following: . ",
    "url": "/docs/statistics/basics/hypothesis-testing-errors.html#truth-table",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-errors.html#truth-table"
  },"321": {
    "doc": "Type I and Type II Errors",
    "title": "Type I Error",
    "content": "Type I error is when we reject the null hypothesis when it is actually true. Type I error is also called false positive. Why is Type I error called false positive? Remember that alternatitve hypothesis is what we want to prove. Try to see things from the perspective of the alternative hypothesis. By rejecting the null hypothesis, we’re basically saying yes (positive) to our alternative hypothesis that is actually false. Because we do not actually know the true state of the world, it is techinally impossible to know whether we have made a Type I error. However, we can control the probability of making a Type I error. Remember that we reject the null hypothesis when: . $$ p &lt; \\alpha $$ . Therefore, if we fix the value of $\\alpha$, we can control the probability of making a Type I error. So when we say $\\alpha = 0.05$, it means that we are willing to accept a 5% chance of making a Type I error. ",
    "url": "/docs/statistics/basics/hypothesis-testing-errors.html#type-i-error",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-errors.html#type-i-error"
  },"322": {
    "doc": "Type I and Type II Errors",
    "title": "Type II Error",
    "content": "Type II error is when we fail to reject the null hypothesis when it is actually false. Type II error is also called false negative. Why is Type II error called false negative? Our alternative hypothesis was actually true, but we fail to confirm it / end up saying “no” (negative). The probability of making a Type II error is denoted by $\\beta$. ",
    "url": "/docs/statistics/basics/hypothesis-testing-errors.html#type-ii-error",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-errors.html#type-ii-error"
  },"323": {
    "doc": "Type I and Type II Errors",
    "title": "Power of Test",
    "content": "The power of test is the probability of rejecting the null hypothesis when it is actually false. It is denoted by: . $$ 1 - \\beta $$ . The power of test is the probability of not making a Type II error. The most common power of test is $80\\%$. However, unlike $\\alpha$, we cannot control the value of $\\beta$ directly. There are a few factors that affect the power of test: . | Sample size: larger sample size $\\rightarrow$ higher power | Effect size: larger effect size $\\rightarrow$ higher power | . Keeping these factors in mind, we instead try to design our experiment so that we have a desired $1 - \\beta$. ",
    "url": "/docs/statistics/basics/hypothesis-testing-errors.html#power-of-test",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-errors.html#power-of-test"
  },"324": {
    "doc": "Type I and Type II Errors",
    "title": "Effect Size",
    "content": "Effect size measures the relationship between two variables. Calculating the effect size depends on the type of test we are performing. For a t-test on the difference between two means, the effect size becomes the standardized difference between the two means or the Cohen’s d: . $$ d = \\frac{\\mu_A - \\mu_B}{\\sigma} $$ . For other tests, we can use other measures of effect size. It is important to pre-determine a relevant effect size (i.e. the blood pressure must decrease by a certain amount) before starting an experiment. Otherwise, depending on how we design our experiment, we might end up with a positive result even though the effect is not significant (i.e. miniscule reduction in blood pressure). ",
    "url": "/docs/statistics/basics/hypothesis-testing-errors.html#effect-size",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-errors.html#effect-size"
  },"325": {
    "doc": "Type I and Type II Errors",
    "title": "Relationship between $\\alpha$ and $\\beta$",
    "content": "Ideally, we’d want to have both $\\alpha$ and $\\beta$ to be small. However, there is a trade-off between the two. If we try to reduce our false positive rate $\\alpha$, we will end up increasing our false negative rate $\\beta$. ",
    "url": "/docs/statistics/basics/hypothesis-testing-errors.html#relationship-between-alpha-and-beta",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-errors.html#relationship-between-alpha-and-beta"
  },"326": {
    "doc": "Type I and Type II Errors",
    "title": "Sample Size Planning",
    "content": "$\\alpha$, $\\beta$, sample size $n$, and effect size are all related. There is a property that, if we fix any three of the four, the last one is pre-determined. When we plan an experiment, we usually have a desired $\\alpha$, $\\beta$, and effect size in mind. Then we can calculate the required sample size $n$. ",
    "url": "/docs/statistics/basics/hypothesis-testing-errors.html#sample-size-planning",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-errors.html#sample-size-planning"
  },"327": {
    "doc": "Hypothesis Testing Methods",
    "title": "Hypothesis Testing Methods",
    "content": ". | Selecting the Right Method . | What happens when you choose an inappropriate test? . | Using a t-Test for Non-Normal Data | . | . | Combination of Data Types . | Qualitative vs. Quantitative | Qualitative vs. Qualitative . | Contingency Table | . | Quantitative vs. Quantitative | . | Distribution of Quantitative Variable . | Parametric Test . | Normality | . | Nonparametric Test | Assumption of Homogeneity of Variance | . | Number of Samples . | Multiple Comparison Problem | How to Avoid the MCP | . | Hypothesis Testing for Qualitative Variable | . ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html"
  },"328": {
    "doc": "Hypothesis Testing Methods",
    "title": "Selecting the Right Method",
    "content": "As seen in the previous section, during hypothesis testing, we: . | Set up a null hypothesis and assume it true | Calculate a test statistic from the real data to test against the null hypothesis | . How one would set up the null hypothesis and what test statistic one would use depends on the type of data, number of samples, and the teting method. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#selecting-the-right-method",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#selecting-the-right-method"
  },"329": {
    "doc": "Hypothesis Testing Methods",
    "title": "What happens when you choose an inappropriate test?",
    "content": "Using a t-Test for Non-Normal Data . Suppose we use a t-test on a population without normality (and a small sample size). What happens is, even if we set the significance level to 0.05, the probability of making a Type I error does not match the significance level. If the tails are much skinnier than the t-distribution, then the probability of making a Type I error is much lower than 0.05. This in turn has the effect of increasing the probability of making a Type II error. This case is sometimes called conservative testing, because you are much more reluctant to accept the alternative hypothesis. Some may prefer this idea, but it still poses a problem. If the tails are much fatter than the t-distribution, then the probability of making a Type I error is much higher than 0.05. This is a big problem because it means that we are more prone to claiming something insignificant to be significant. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#what-happens-when-you-choose-an-inappropriate-test",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#what-happens-when-you-choose-an-inappropriate-test"
  },"330": {
    "doc": "Hypothesis Testing Methods",
    "title": "Combination of Data Types",
    "content": "There are two different types of data: . | Quantitative | Qualitative | . Suppose we are comparing two different variables. Depending on the combination of data types, the way we interpret our null hypothesis and test statistic will change. So it is important to first determine the data that we’re working with before selecting a hypothesis testing method. The difference in characteristic can be best illustrated with some figures. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#combination-of-data-types",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#combination-of-data-types"
  },"331": {
    "doc": "Hypothesis Testing Methods",
    "title": "Qualitative vs. Quantitative",
    "content": "For instance, suppose we were testing the effectiveness of a new drug by comparing some measurement (i.e. blood pressure) of people who take the drug and people who take a placebo. Then, we’d have a bar plot like the following: . In this case, we have a qualitative variable (i.e. drug vs. placebo), and a quantitative variable (i.e. body fat). ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#qualitative-vs-quantitative",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#qualitative-vs-quantitative"
  },"332": {
    "doc": "Hypothesis Testing Methods",
    "title": "Qualitative vs. Qualitative",
    "content": "Contingency Table . When we have two qualitative variables, we can use a contingency table to visualize the data. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#qualitative-vs-qualitative",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#qualitative-vs-qualitative"
  },"333": {
    "doc": "Hypothesis Testing Methods",
    "title": "Quantitative vs. Quantitative",
    "content": "When we have two quantitative variables, we can use a scatter plot to visualize the data. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#quantitative-vs-quantitative",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#quantitative-vs-quantitative"
  },"334": {
    "doc": "Hypothesis Testing Methods",
    "title": "Distribution of Quantitative Variable",
    "content": "When we have a quantitative variable, it is important to understand the distribution of the variable before selecting a hypothesis testing method. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#distribution-of-quantitative-variable",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#distribution-of-quantitative-variable"
  },"335": {
    "doc": "Hypothesis Testing Methods",
    "title": "Parametric Test",
    "content": "Hypothesis testing that assumes a specific distribution for the population (mathematically defined by parameters) is called a parametric test. For instance, the t-distribution assumes that the population distribution is normal. Hence, any t-test that we perform will be based on the assumption that the population distribution is normal. | One-Sample t-Test | Paired t-Test | Two-Sample t-Test | Welch’s t-Test | . Normality . If a set of data has normality, then it means that it was sampled from a population that is normally distributed. The majority of parametric tests assume that the population distribution is normal. There are a few ways to check for normality: . | Quantile-Quantile (Q-Q) Plot: a graphical/visual method | Shapiro-Wilk Test: a statistical method using hypothesis testing | Kolmogorov-Smirnov (K-S) Test: a statistical method using hypothesis testing | . When using hypothesis testing to check for normality, we set the null hypothesis to be that the data is normally distributed. There is also the issue of test multiplicity, which is the problem of performing multiple hypothesis tests on the same data set, because we perform the normality test before performing the actual test. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#parametric-test",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#parametric-test"
  },"336": {
    "doc": "Hypothesis Testing Methods",
    "title": "Nonparametric Test",
    "content": "Not all data follow a certain mathematical distribution. For data with asymmetry or outliers, statistics such as the mean and standard deviation are not reliable. In such cases, parametric tests are not appropriate. Hypothesis testing that does not depend on parameters, such as mean and standard deviation, is called a nonparametric test. When the distributions in comparison are similar/identical: . | Wilcoxon Rank Sum Test | Mann-Whitney U Test | . When the distributions in comparison are different: . | Fligner-Policello Test | Brunner-Munzel Test | . Still not recommended for tests with extremely different distributions. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#nonparametric-test",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#nonparametric-test"
  },"337": {
    "doc": "Hypothesis Testing Methods",
    "title": "Assumption of Homogeneity of Variance",
    "content": "When we compare the means of groups of data, we often assume that all comparison populations have the same variance. If the variance of all populations are the same, then we say that the they have homogeneity of variance. Some of the hypothesis tests to test for homogeneity of variance are: . | Bartlett’s Test | Levene’s Test | . The null hypothesis would be that the populations have homogeneity of variance. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#assumption-of-homogeneity-of-variance",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#assumption-of-homogeneity-of-variance"
  },"338": {
    "doc": "Hypothesis Testing Methods",
    "title": "Number of Samples",
    "content": "Number of samples refers to the number of groups of data that we have. Do not confuse this with the sample size $n$, which refers to the number of observations in a single group. If we had a quantitative, single variable that approximates a normal distribution, then we would use a one-sample t-test. If we had two quantitative variables that approximates a normal distribution, then we would use a two-sample t-test. The most common case is when we have two groups of data, but there are cases where we have to make a comparison between more than two groups. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#number-of-samples",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#number-of-samples"
  },"339": {
    "doc": "Hypothesis Testing Methods",
    "title": "Multiple Comparison Problem",
    "content": "Why not just perform multiple pairwise t-tests? . Suppose we have $m$ pairs of groups. For each pair, the Type I error rate is $\\alpha$. When we perform $m$ two-sample t-tests, the probability of making at least one Type I error is equal to 1 minus the probability of never making a Type I error in any of the tests: . $$ 1 - (1 - \\alpha)^m $$ . This type of error rate is actually called the family-wise error rate (FWER). For example, if we have 3 pairs of groups, and we set $\\alpha = 0.05$, then the probability of making at least one Type I error is: . $$ 1 - (1 - 0.05)^3 = 0.1426 &gt; 0.05 $$ . Which is dangerously higher than the desired Type I error rate of $\\alpha$. This is called the multiple comparison problem (MCP). ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#multiple-comparison-problem",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#multiple-comparison-problem"
  },"340": {
    "doc": "Hypothesis Testing Methods",
    "title": "How to Avoid the MCP",
    "content": "When there are more than two groups of data, we generally perform an omnibus test to test for significant difference between at least one pair of groups. If the result of the omnibus test turns out to be significant, (as necessary to the purpose of your research) we perform a post-hoc test that either tries to correct our results from the previous test or performs an additional test to see exactly which pairs of groups are different. Even though the name suggest otherwise, not all post-hoc tests pre-requisite a test like ANOVA. Some post-hoc tests like Bonferroni, Tukey, Dunnet, Williams, etc. can be used without ANOVA. Post-hoc tests are sometimes just called multiple comparison analysis as well. In addition, even though the omnibus test turns out to be insignificant, a post-hoc test may report significance. This may occur if the statistics and distributions used for the pre and post tests are different. So it is worth performing a standalone multiple comparison analysis. ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#how-to-avoid-the-mcp",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#how-to-avoid-the-mcp"
  },"341": {
    "doc": "Hypothesis Testing Methods",
    "title": "Tests for Multiple Comparison",
    "content": "Omnibus Test: . | ANOVA: parametric test | Kruskal-Wallis Test: nonparametric test | . Multiple Comparison Analysis / Post-Hoc Test: . | Bonferroni Correction | Tukey’s HSD Test: when comparing all groups to each other | Steel-Dwass Test: nonparametric version of Tukey’s HSD Test | Dunnett’s Test: when comparing all groups to a control group | Steel Test: nonparametric version of Dunnett’s Test | Williams Test: when you can rank/sort the groups into a specific order | Scheffe’s Test | Newman-Keuls Test | . ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#tests-for-multiple-comparison",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#tests-for-multiple-comparison"
  },"342": {
    "doc": "Hypothesis Testing Methods",
    "title": "Hypothesis Testing for Qualitative Variable",
    "content": "When the population is quantitative, we often set up a hypothesis test about the population mean. When the population is qualitative, we often set up a hypothesis test about: . $$ P: \\text{probability of an event occurring in the population} $$ . | Binomial test | Chi-square test of goodness of fit | Chi-square test of independence | . ",
    "url": "/docs/statistics/basics/hypothesis-testing-methods.html#hypothesis-testing-for-qualitative-variable",
    "relUrl": "/docs/statistics/basics/hypothesis-testing-methods.html#hypothesis-testing-for-qualitative-variable"
  },"343": {
    "doc": "Hypothesis Testing",
    "title": "Statistical Hypothesis Testing",
    "content": ". | Different Types of Data Analysis . | Confirmatory Data Analysis | Exploratory Data Analysis | . | Setting Up a Hypothesis . | Null Hypothesis | Alternative Hypothesis | Proof by Contradiction | . | $p$-Value | Significance Level $\\alpha$ . | Rejection Region . | One-Tailed Test / One-Sided Test | Two-Tailed Test / Two-Sided Test | . | Statistically Significant | . | Different Methods for Hypothesis Testing | Hypothesis Testing and Confidence Interval | Plotting Graphs in Hypothesis Testing . | Error Bars | Indicating Statistical Significance | . | Errors in Hypothesis Testing | . ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#statistical-hypothesis-testing",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#statistical-hypothesis-testing"
  },"344": {
    "doc": "Hypothesis Testing",
    "title": "Different Types of Data Analysis",
    "content": " ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#different-types-of-data-analysis",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#different-types-of-data-analysis"
  },"345": {
    "doc": "Hypothesis Testing",
    "title": "Confirmatory Data Analysis",
    "content": "Confirmatory data analysis is when the researcher already has a clear hypothesis and tries to test whether the data confirms or rejects it. For example, a researcher may hypothesize that a new drug is more effective than a placebo. Then, the researcher would collect data and perform statistical tests to compare the two groups. This page focuses on confirmatory data analysis. ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#confirmatory-data-analysis",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#confirmatory-data-analysis"
  },"346": {
    "doc": "Hypothesis Testing",
    "title": "Exploratory Data Analysis",
    "content": "Exploratory data analysis is when the researcher does not have a clear hypothesis and tries to find patterns or trends in the data. May be used to generate hypotheses for confirmatory data analysis. ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#exploratory-data-analysis",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#exploratory-data-analysis"
  },"347": {
    "doc": "Hypothesis Testing",
    "title": "Setting Up a Hypothesis",
    "content": "Say we want to test the effectiveness of a new drug. Let $A$ be population of blood pressure of people who take the new drug, and $B$ be the population of blood pressure of people who take a placebo. For the drug to be effective, we want $\\mu_A \\ne \\mu_B$, where $\\mu_A$ and $\\mu_B$ are the population means of $A$ and $B$. Notice that the hypothesis is about the population, not the sample. In hypothesis testing, we set up two hypotheses: . ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#setting-up-a-hypothesis",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#setting-up-a-hypothesis"
  },"348": {
    "doc": "Hypothesis Testing",
    "title": "Null Hypothesis",
    "content": "The null hypothesis is the negation of the hypothesis we want to confirm. With our example, the null hypothesis is: . $$ \\begin{equation} \\label{eq:null-h} \\tag{Null Hypothesis} \\mu_A = \\mu_B \\end{equation} $$ . This would mean that the drug is not effective. ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#null-hypothesis",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#null-hypothesis"
  },"349": {
    "doc": "Hypothesis Testing",
    "title": "Alternative Hypothesis",
    "content": "The alternative hypothesis is the hypothesis we want to confirm. With our example, the alternative hypothesis is: . $$ \\begin{equation} \\label{eq:alt-h} \\tag{Alternative Hypothesis} \\mu_A \\ne \\mu_B \\end{equation} $$ . This would mean that the drug is effective. ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#alternative-hypothesis",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#alternative-hypothesis"
  },"350": {
    "doc": "Hypothesis Testing",
    "title": "Proof by Contradiction",
    "content": "Hypothesis testing takes the following approach: . | Set up the null hypothesis and alternative hypothesis | Assume a distribution where the null hypothesis is true | Calculate a test statistic from the real sample data | Test how this statistic fits into this assumed distribution | If the real data statistic is unlikely when the null hypothesis is true, then we reject the null hypothesis | By rejecting the null hypothesis, we accept the alternative hypothesis | Otherwise, we fail to reject the null hypothesis | . Notice the wording: fail to reject the null hypothesis. Failing to reject does not equal accepting the null hypothesis or rejecting the alternative hypothesis. ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#proof-by-contradiction",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#proof-by-contradiction"
  },"351": {
    "doc": "Hypothesis Testing",
    "title": "$p$-Value",
    "content": "The $p$-value is: . The probability of observing the data in a distribution that assumes the null hypothesis is true. So if the p-value of a real data is low, then the data is unlikely to occur when the null hypothesis is true. Which in turn means that the null hypothesis is unlikely to be true. So the next question is: . | What is considered a low $p$-value? | . ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#p-value",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#p-value"
  },"352": {
    "doc": "Hypothesis Testing",
    "title": "Significance Level $\\alpha$",
    "content": "Whether we reject the null hypothesis or not depends on the significance level, commonly denoted by $\\alpha$. Most commonly used significance level is $\\alpha = 0.05$. ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#significance-level-alpha",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#significance-level-alpha"
  },"353": {
    "doc": "Hypothesis Testing",
    "title": "Rejection Region",
    "content": "The rejection region refers to the area on the left and right tails of the distribution that we would reject the null hypothesis. For $\\alpha = 0.05$, the rejection region would be on each side of the distribution, 2.5% each. One-Tailed Test / One-Sided Test . One-tailed test is when we reject the null hypothesis by considering only one tail of the distribution, and thus $\\alpha / 2$. Two-Tailed Test / Two-Sided Test . Two-tailed test is when we reject the null hypothesis by considering both tails of the distribution, and thus the whole $\\alpha$. Unless there is a reason to use one-tailed test, it is much more common to use two-tailed test. ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#rejection-region",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#rejection-region"
  },"354": {
    "doc": "Hypothesis Testing",
    "title": "Statistically Significant",
    "content": "If the $p$-value is less than $\\alpha$, then the data is statistically significant. $$ p &lt; \\alpha $$ . Then we reject the null hypothesis and accept the alternative hypothesis. Otherwise, we have no grounds to reject the null hypothesis. ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#statistically-significant",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#statistically-significant"
  },"355": {
    "doc": "Hypothesis Testing",
    "title": "Different Methods for Hypothesis Testing",
    "content": "Go to page . ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#different-methods-for-hypothesis-testing",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#different-methods-for-hypothesis-testing"
  },"356": {
    "doc": "Hypothesis Testing",
    "title": "Hypothesis Testing and Confidence Interval",
    "content": "The relationship between confidence interval and hypothesis testing is like a mirror image. $$ \\text{Significance Level}\\; \\alpha = 1 - \\text{Confidence Level} $$ . With a 95 CI, we infer a likely (with 95% chance) range of the population statistic based on the sample statistic computed from real data. Then we test if our null hypothesis fits into this range. For instance, with our example null hypothesis, we can calculate a 95 CI for $\\mu_A - \\mu_B$, and see if it contains 0. If not, then we reject the null hypothesis. On the other hand, with hypothesis testing, we first assume that the null hypothesis is true, and then calculate a sample statistic from the real data. Then we test how this sample statistic fits into this assumed null hypothesis. For instance, with our example null hypothesis, we obtain a distribution of $\\mu_A - \\mu_B$ of which the mean is 0. Then we calculate the probability $p$ of observing the real data, and see if it is less than $\\alpha = 0.05$. If so, then we reject the null hypothesis. ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#hypothesis-testing-and-confidence-interval",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#hypothesis-testing-and-confidence-interval"
  },"357": {
    "doc": "Hypothesis Testing",
    "title": "Plotting Graphs in Hypothesis Testing",
    "content": " ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#plotting-graphs-in-hypothesis-testing",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#plotting-graphs-in-hypothesis-testing"
  },"358": {
    "doc": "Hypothesis Testing",
    "title": "Error Bars",
    "content": "With bar plots and scatter plots, you’ll often see error bars. It is important to describe in the legends which metric is used for the error bars, because it can mean different things. Suppose we have a bar plot with sample mean as the variable, . | Standard deviation . | The error bars represent the dispersion of the sample | Does not represent a probability | . | Standard error of the mean . | The error bars represent the probability of the sample mean | . | Confidence interval | . ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#error-bars",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#error-bars"
  },"359": {
    "doc": "Hypothesis Testing",
    "title": "Indicating Statistical Significance",
    "content": "In figures and charts, we commonly use asterisks ($*$) to indicate statistical significance. Although the figure below does not, make sure to indicate what the asterisks mean in the legends. Commonly used symbols are: . | $p &lt; 0.05$: $*$ | $p &lt; 0.01$: $**$ | $p &lt; 0.001$: $***$ | Non-significant: $\\text{N.S.}$ | . ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#indicating-statistical-significance",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#indicating-statistical-significance"
  },"360": {
    "doc": "Hypothesis Testing",
    "title": "Errors in Hypothesis Testing",
    "content": "Go to page . ",
    "url": "/docs/statistics/basics/hypothesis-testing.html#errors-in-hypothesis-testing",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html#errors-in-hypothesis-testing"
  },"361": {
    "doc": "Hypothesis Testing",
    "title": "Hypothesis Testing",
    "content": " ",
    "url": "/docs/statistics/basics/hypothesis-testing.html",
    "relUrl": "/docs/statistics/basics/hypothesis-testing.html"
  },"362": {
    "doc": "As IDE Extensions",
    "title": "As IDE Extensions",
    "content": "To be added . | Extensions | GitHub Copilot | . ",
    "url": "/docs/others/vim/ide.html",
    "relUrl": "/docs/others/vim/ide.html"
  },"363": {
    "doc": "As IDE Extensions",
    "title": "Extensions",
    "content": ". | VSCode: vscodevim.vim | IntelliJ: IdeaVim | . ",
    "url": "/docs/others/vim/ide.html#extensions",
    "relUrl": "/docs/others/vim/ide.html#extensions"
  },"364": {
    "doc": "As IDE Extensions",
    "title": "GitHub Copilot",
    "content": "If you’re using GitHub Copilot, the default key mapping for Dismiss an inline suggestion is Esc. However, since Esc is used to switch to normal mode in Vim, you’d probably want to map it to something else. | VSCode: editor.action.inlineSuggest.hide | IntelliJ: Copilot: Hide Completions in Editor | . References: . | Configure GitHub Copilot: Visual Studio Code | Configure GitHub Copilot: JetBrains | . ",
    "url": "/docs/others/vim/ide.html#github-copilot",
    "relUrl": "/docs/others/vim/ide.html#github-copilot"
  },"365": {
    "doc": "Independent and Identically Distributed (IID)",
    "title": "Independent and Identically Distributed (IID)",
    "content": "IID (aka iid, i.i.d.) is a commonly used assumption in statistical modeling, which simplifies the underlying mathematics of a complex system. When we have a set or sequence of i.i.d. random variables, it means: . | Independent: Each random variable is independent of each other | Identically Distributed: Each random variable has the same probability distribution . Do not mistake identical for uniform distribution. | . ",
    "url": "/docs/statistics/notes/iid.html",
    "relUrl": "/docs/statistics/notes/iid.html"
  },"366": {
    "doc": "Docker Images",
    "title": "Docker Images",
    "content": ". | Docker Image / Images . | Image | Images | . | Dangling images . | Remove dangling images | . | . ",
    "url": "/docs/docker/images.html",
    "relUrl": "/docs/docker/images.html"
  },"367": {
    "doc": "Docker Images",
    "title": "Docker Image / Images",
    "content": "You may have noticed that there are two Docker CLI commands that seem similar . | docker image | docker images | . There is a bit of a difference between the two. Image . Actually builds, pulls, and removes images. This command is used to physically manage the images. You can of course list images as well. docker image ls . Images . This has to do with displaying in a high-level fashion what kind of images exist. Primary purpose is to display image metadata. docker images . ",
    "url": "/docs/docker/images.html#docker-image--images",
    "relUrl": "/docs/docker/images.html#docker-image--images"
  },"368": {
    "doc": "Docker Images",
    "title": "Dangling images",
    "content": "When you do . docker images -a | grep '&lt;none&gt;' # OR docker image ls -a | grep '&lt;none&gt;' . Or check the Images tab in Docker Desktop, you may see a bunch of images with the name and tag of &lt;none&gt;. This is a residue / intermediate image created from previous image builds. It seems they exist as a cached layer for subsequent builds. But it is safe to delete them. Remove dangling images . You can remove these dangling images by . docker image prune . docker image prune -a not only removes dangling images but also any unused images. This can come in handy, but if you’re keeping any pulled Docker registry images (unused in containers at the moment) in your local storage for some reason, this is not what you want. ",
    "url": "/docs/docker/images.html#dangling-images",
    "relUrl": "/docs/docker/images.html#dangling-images"
  },"369": {
    "doc": "Home",
    "title": "Online Long-Term Memory",
    "content": "Personal documentation of itty bitties and all the hacky decisions I’ve made throughout my learning. ",
    "url": "/#online-long-term-memory",
    "relUrl": "/#online-long-term-memory"
  },"370": {
    "doc": "Home",
    "title": "Disclaimer",
    "content": "The information contained in this document is not necessarily correct or comprehensive. It will be biased in many ways and may contain naive and pitiful approaches made by a novice. Its sole purpose is to document my footsteps to reference in the future. ",
    "url": "/#disclaimer",
    "relUrl": "/#disclaimer"
  },"371": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"372": {
    "doc": "Demo",
    "title": "Demo",
    "content": " ",
    "url": "/docs/demo/",
    "relUrl": "/docs/demo/"
  },"373": {
    "doc": "Basic Stats for Data",
    "title": "Basic Statistics for Data Analysis",
    "content": "Summary of a book on basic statistics for data analysis. With my own interpretation and explanations… . Related Pages . | Hypothesis Testing . | Hypothesis Testing Methods | Type I and Type II Errors | Comparing Means in Parametric Tests | Welch’s t-Test | Comparing with Nonparametric Tests | ANOVA | Post-Hoc Test | Binomial Test | . | Basic Stats Vocabulary | . ",
    "url": "/docs/statistics/basics/#basic-statistics-for-data-analysis",
    "relUrl": "/docs/statistics/basics/#basic-statistics-for-data-analysis"
  },"374": {
    "doc": "Basic Stats for Data",
    "title": "Basic Stats for Data",
    "content": " ",
    "url": "/docs/statistics/basics/",
    "relUrl": "/docs/statistics/basics/"
  },"375": {
    "doc": "Stats/ML Quick Notes",
    "title": "Statistics / Machine Learning Quick Notes",
    "content": "To be added . Fragments of things I learned. They may be moved to a separate category later. ",
    "url": "/docs/statistics/notes/#statistics--machine-learning-quick-notes",
    "relUrl": "/docs/statistics/notes/#statistics--machine-learning-quick-notes"
  },"376": {
    "doc": "Stats/ML Quick Notes",
    "title": "Stats/ML Quick Notes",
    "content": " ",
    "url": "/docs/statistics/notes/",
    "relUrl": "/docs/statistics/notes/"
  },"377": {
    "doc": "Statistics",
    "title": "Statistics / Machine Learning",
    "content": "Statistics with most of the focus on how they apply to machine learning. ",
    "url": "/docs/statistics/#statistics--machine-learning",
    "relUrl": "/docs/statistics/#statistics--machine-learning"
  },"378": {
    "doc": "Statistics",
    "title": "Statistics",
    "content": " ",
    "url": "/docs/statistics/",
    "relUrl": "/docs/statistics/"
  },"379": {
    "doc": "Flask",
    "title": "Flask",
    "content": " ",
    "url": "/docs/flask/",
    "relUrl": "/docs/flask/"
  },"380": {
    "doc": "Terraform",
    "title": "Terraform",
    "content": " ",
    "url": "/docs/terraform/",
    "relUrl": "/docs/terraform/"
  },"381": {
    "doc": "Terraform",
    "title": "What is Terraform?",
    "content": "Infrastructure as Code (IaC): Terraform is a software tool that codes the infrastructure with a declarative configuration language. Your entire infrastructure is managed through a set of declarations. The benefit of IaC is that everything is collected within a single tool. This gets rid of the pain of having to jump to different tools every time you want to configure your resources. ",
    "url": "/docs/terraform/#what-is-terraform",
    "relUrl": "/docs/terraform/#what-is-terraform"
  },"382": {
    "doc": "Network",
    "title": "Network Basics",
    "content": ". | IP Address . | Private IP Address . | IPv4: RFC1918 | . | . | . ",
    "url": "/docs/learned/network/#network-basics",
    "relUrl": "/docs/learned/network/#network-basics"
  },"383": {
    "doc": "Network",
    "title": "IP Address",
    "content": "IP (Internet Protocol) address is a unique address that identifies a device on a network using an Internet Protocol. Private IP Address . Reserved range for private networks . IPv4: RFC1918 . | [24-bit block] CIDR: 10.0.0.0/8, Subnet mask: 255.0.0.0 | [20-bit block] CIDR: 172.16.0.0/12, Subnet mask: 255.240.0.0 | [16-bit block] CIDR: 192.168.0.0/16, Subnet mask: 255.255.0.0 | . ",
    "url": "/docs/learned/network/#ip-address",
    "relUrl": "/docs/learned/network/#ip-address"
  },"384": {
    "doc": "Network",
    "title": "Network",
    "content": " ",
    "url": "/docs/learned/network/",
    "relUrl": "/docs/learned/network/"
  },"385": {
    "doc": "OAuth 2.0",
    "title": "OAuth 2.0",
    "content": ". | What is an OAuth 2.0 protocol? . | OpenID | . | Client types . | Client secret | Public clients | Confidential clients | . | Authorization flow . | Implicit flow | Authorization code flow | Authorization code flow with PKCE | . | Authorization server API . | Authorize | Token | . | PKCE Code Challenge . | Code verifier | Code challenge | . | . ",
    "url": "/docs/learned/oauth2/",
    "relUrl": "/docs/learned/oauth2/"
  },"386": {
    "doc": "OAuth 2.0",
    "title": "What is an OAuth 2.0 protocol?",
    "content": "According to Google, it is an ‘open standard for access delegation’. While it sounds intimidating, it is essentially made to ‘let this application access my Google photos’, ‘let this site use my Facebook contacts’, etc. So it was developed as method for authorization to a 3rd party resource. Some terms: . | Resource owner: that’s the user (you) wanting to grant access | Resource server: the API you want to access | Client: application requesting access | User Agent: the thing user is using to talk to client (browser, mobile app) | Authorization server: authorizes and grants access tokens to client | . OpenID . One thing to note is the word authorization, and you shoud not to confuse it with authentication. When I first read about OAuth, I thought, “Well isn’t this the ‘Sign in with Google/Facebook’ button that I see quite a lot on websites these days?”. It sort of is, because the protocol behind that button is OpenID which is built on top of OAuth 2.0. So the way they operate are very similar, but it is good to know the difference that OAuth is for authorization and OpenID builds a layer on top of OAuth for authentication. ",
    "url": "/docs/learned/oauth2/#what-is-an-oauth-20-protocol",
    "relUrl": "/docs/learned/oauth2/#what-is-an-oauth-20-protocol"
  },"387": {
    "doc": "OAuth 2.0",
    "title": "Client types",
    "content": "There are two different types of clients in OAuth. One is a public client and the other is a confidential client. To understand the difference, you need to know the term client secret. Client secret . A client secret is nothing more than a random string generated. It is usually created by generating a secure random string of 256-bit (32 bytes) and then converting it to hex. This value should never be revealed to the outside except for the authorizing server and the client app. Hence the name ‘client secret’. Inside your code, client secret will be used to successfully authorize users. But the issue that arises is where should the client store this secret. Public clients . If the client cannot keep the client secret a secret, it is called a public client. For example, single-page apps that expose everything on the browser with no backend or mobile apps that can have their HTTPS request intercepted and revealed are considered public clients. In case of an SPA, everything is exposed on the browser. Chrome inspect will reveal the source code, local storage, session, and cookies. So storing client secret is infeasible. For a mobile app, apparently it is possible to provide a fake HTTPS certificate that goes to your own API. So you can catch an HTTPS leaving the phone, route it to a different API, have that API make a request to the initial intended API, and return the response to phone as if it would normally, while the proxy API in the middle can inspect all the requests (which may contain the client secret at some point). Confidential clients . This is typically a traditional web server or anything backed by a server where nobody can take a peek at the source code or have the requests intercepted. ",
    "url": "/docs/learned/oauth2/#client-types",
    "relUrl": "/docs/learned/oauth2/#client-types"
  },"388": {
    "doc": "OAuth 2.0",
    "title": "Authorization flow",
    "content": "There are a few different flows, but I will only document three of them: implicit flow, authorization code flow, authorization code flow with PKCE. The general process is as below: . | Client sends request to autorization server | Client gets an authorization code back | Client sends request to a token endpoint | Client gets an access token | Client places this token in a header when sending a request to resource server | . Implicit flow . Implicit flow is much more simplified. After step 1, implicit flow skips right to step 4. Because the access token is revealed on the browser url, this is considered an insecure lecay method. Authorization code flow . Client gets an authorization code back as a request parameter embedded in the url. The client then uses this code to exchange it for an access token. Usually secure random strings such as state and client secret are used to validate the process. Authorization code flow with PKCE . For public clients that cannot keep any secret strings, PKCE (Proof Key for Code Exchange) is implemented. This step includes an additional code challenge and verifying step. ",
    "url": "/docs/learned/oauth2/#authorization-flow",
    "relUrl": "/docs/learned/oauth2/#authorization-flow"
  },"389": {
    "doc": "OAuth 2.0",
    "title": "Authorization server API",
    "content": "Typically there are two endpoints during the process. Authorize . Typical request is an HTTPS GET to a path that often looks like oauth/authorize. Parameters . | response_type: code for authorization code flow and token for implicit flow | client_id: client app id | redirect_uri: absolute uri to be redirected after authorization | state: a random value that will be returned back in redirect. This is a protection against CSRF. | scope: the scope of resources you want to protect | code_challenge_method (PKCE only): the encryption used in code challenge; typically S256 for SHA256 | code_challenge (PKCE only): the generated challenge from code_verifier | . Token . After extracting the authorization code from the redirect url, you make an HTTPS POST request to oauth/token. Header: . | Authorization: Basic Base64_url_encode('client_id:client_secret') | Content-Type: application/x-www-form-urlencoded | . Body: . | grant_type: authorization_code, refresh_token, client_credentials | client_id | redirect_uri: should be the same as the one used for authorization request | scope | code: extracted from url | code_verifier: proof key for the code_challenge | . Response: . { \"id_token\": \"~\", \"access_token\": \"~\", \"refresh_token\": \"~\", \"token_type\": \"Bearer\", \"expires_in\": 10000 } . ",
    "url": "/docs/learned/oauth2/#authorization-server-api",
    "relUrl": "/docs/learned/oauth2/#authorization-server-api"
  },"390": {
    "doc": "OAuth 2.0",
    "title": "PKCE Code Challenge",
    "content": "Code verifier . According to here it is a ‘cryptographically random string using the characters A-Z, a-z, 0-9, and the punctuation characters -._~ (hyphen, period, underscore, and tilde), between 43 and 128 characters long’. Code challenge . Code challenge is created by hashing the code_verifier with SHA256 and then encoding as a BASE6-URL string. References: . | OAuth: PKCE | AWS Cognito: AUTHORIZE | AWS Cognito: TOKEN | Auth0: PKCE | . ",
    "url": "/docs/learned/oauth2/#pkce-code-challenge",
    "relUrl": "/docs/learned/oauth2/#pkce-code-challenge"
  },"391": {
    "doc": "Database",
    "title": "Database",
    "content": ". | ACID / BASE . | ACID | BASE | . | CAP Theory | ORM | . ",
    "url": "/docs/learned/db/",
    "relUrl": "/docs/learned/db/"
  },"392": {
    "doc": "Database",
    "title": "ACID / BASE",
    "content": "ACID . | Atomicity: All operations in a transaction are atomic, meaning they either all succeed or none happen (single failure rolls back the entire transaction). | Consistency: A transaction does not break any invariants set by DB. A DB must be in a valid state before and after a transaction. | Isolation: The end result of a DB after concurrent transactions and sequential transactions must be the same. All transactions must operate as if they were operating on an isolated DB. | Durability: Once a transaction is committed, the commit is retained even after a system failure. | . ACID transaction has been the norm for relational databases. It is more conservative in a sense and more suitable in domains where data safety is critical (financial institutes). However, it is generally considered to be slower due to heavy locking. BASE . | Basic Availiability: Reading and writing is available whenever possible. | Soft-state: State of the system do not ensure write consistency, and replica nodes are not guranteed to be consistent with each other. | Eventually consistent: Given time, system will eventually converge to a known state. | . Where throughput is deemed higher importance than immediate consistency, ACID may be too restrictive. BASE transaction is more optimistic in locking compared to ACID. Many NoSQL databases adhere to BASE when data safety is less of a risk. ",
    "url": "/docs/learned/db/#acid--base",
    "relUrl": "/docs/learned/db/#acid--base"
  },"393": {
    "doc": "Database",
    "title": "CAP Theory",
    "content": "TBA . ",
    "url": "/docs/learned/db/#cap-theory",
    "relUrl": "/docs/learned/db/#cap-theory"
  },"394": {
    "doc": "Database",
    "title": "ORM",
    "content": "TBA . References: . | Data Consistency Models | . ",
    "url": "/docs/learned/db/#orm",
    "relUrl": "/docs/learned/db/#orm"
  },"395": {
    "doc": "Things I Learned",
    "title": "Things I Learned",
    "content": "List of itty bitty things that I want to keep a note of, but couldn’t quite find a category to place yet. Contents listed here may be moved or grouped with other pages if more related contents are produced. ",
    "url": "/docs/learned/",
    "relUrl": "/docs/learned/"
  },"396": {
    "doc": "Docker",
    "title": "Docker",
    "content": " ",
    "url": "/docs/docker/",
    "relUrl": "/docs/docker/"
  },"397": {
    "doc": "Docker",
    "title": "Explained in a really dumb way",
    "content": "You build an image that contains all the resources that compose a project. This packaging makes porting really easy because all the resources that made your project run at one time is now completely captured in it. You could think of this as a snapshot of your project. This image can be run in a docker container. A container is basically a process isolated from your computer. Think of it as a mini sandbox that mimics your system. Inside a container resources will be downloaded, installed, and copied just as you would normally, but whatever that happened during a container execution will not meddle with your actual computer (unless you specifically configure it to). ",
    "url": "/docs/docker/#explained-in-a-really-dumb-way",
    "relUrl": "/docs/docker/#explained-in-a-really-dumb-way"
  },"398": {
    "doc": "LeetCode",
    "title": "LeetCode",
    "content": "LeetCode problems and solutions. ",
    "url": "/docs/compsci/leetcode/",
    "relUrl": "/docs/compsci/leetcode/"
  },"399": {
    "doc": "Algorithms",
    "title": "Algorithm Quick Notes",
    "content": "Random notes on algorithm. ",
    "url": "/docs/compsci/algo/#algorithm-quick-notes",
    "relUrl": "/docs/compsci/algo/#algorithm-quick-notes"
  },"400": {
    "doc": "Algorithms",
    "title": "Algorithms",
    "content": " ",
    "url": "/docs/compsci/algo/",
    "relUrl": "/docs/compsci/algo/"
  },"401": {
    "doc": "Math",
    "title": "Math Quick Notes",
    "content": "Random quick notes on mathematical concepts. ",
    "url": "/docs/compsci/math/#math-quick-notes",
    "relUrl": "/docs/compsci/math/#math-quick-notes"
  },"402": {
    "doc": "Math",
    "title": "Math",
    "content": " ",
    "url": "/docs/compsci/math/",
    "relUrl": "/docs/compsci/math/"
  },"403": {
    "doc": "Computer Science",
    "title": "Computer Science",
    "content": "Random notes on computer science. ",
    "url": "/docs/compsci/",
    "relUrl": "/docs/compsci/"
  },"404": {
    "doc": "Linear Algebra Quick Notes",
    "title": "Linear Algebra Quick Notes",
    "content": "To be added . Fragments of things I learned and recaps. ",
    "url": "/docs/linalg/notes/",
    "relUrl": "/docs/linalg/notes/"
  },"405": {
    "doc": "Linear Algebra",
    "title": "Linear Algebra",
    "content": " ",
    "url": "/docs/linalg/",
    "relUrl": "/docs/linalg/"
  },"406": {
    "doc": "Python Environments",
    "title": "Python Environments",
    "content": ". | Why do you need them? | Python version manager vs Virtual environments . | Typical use case | . | Notes / sanity check | . ",
    "url": "/docs/python/envs/",
    "relUrl": "/docs/python/envs/"
  },"407": {
    "doc": "Python Environments",
    "title": "Why do you need them?",
    "content": "Every Python project comes with different requirements. For example: . |   | Python | Library1 | Library2 | Library3 | . | Project A | 3.6 | x | 1.1.2 | 2.3.0 | . | Project B | 3.10 | x | 2.3.1 | 3.0.1 | . | Project C | 2.7 | 1.4.0 | 1.1.2 | x | . As the number of projects grow, managing different versions of Python and their packages are going to be increasingly difficult. Switching between them and resolving conflicts is one hassle, but removing them after use is also a pain. Environments, however, remembers the context of a project, and keeps them independent of other project’s context. Hence, project collaboration and management becomes much easier with environments. ",
    "url": "/docs/python/envs/#why-do-you-need-them",
    "relUrl": "/docs/python/envs/#why-do-you-need-them"
  },"408": {
    "doc": "Python Environments",
    "title": "Python version manager vs Virtual environments",
    "content": "It can be confusing because they all go by the name environment. Long story short, . | Version manager: manages Python versions | Virtual environment: manages libraries | . You’ll probably end up needing both. Typical use case . You install and select a Python version to use with a version manager. Then create a virtual environment for a project using that Python version. For example (not comprehensive): . | pyenv: version manager | Conda: version manager + virtual environment | venv: virtual environment | Pipenv: virtual environment | Poetry: virtual environment | . ",
    "url": "/docs/python/envs/#python-version-manager-vs-virtual-environments",
    "relUrl": "/docs/python/envs/#python-version-manager-vs-virtual-environments"
  },"409": {
    "doc": "Python Environments",
    "title": "Notes / sanity check",
    "content": ". | which python / which python3 will point to the python binary | which pip / which pip3 will point the pip binary | pip -V / pip3 -V will point to the site-packages | conda run which python / conda run python -V does the expected for the base conda env or the active env | pipenv run which python / pipenv run python -V does the expected for the current root directory env | However, pipenv run pip -V will create an env for the cwd and add pip to the Pipfile for cwd | . ",
    "url": "/docs/python/envs/#notes--sanity-check",
    "relUrl": "/docs/python/envs/#notes--sanity-check"
  },"410": {
    "doc": "Python",
    "title": "Python",
    "content": " ",
    "url": "/docs/python/",
    "relUrl": "/docs/python/"
  },"411": {
    "doc": "Time Series Analysis",
    "title": "Time Series Analysis",
    "content": "Includes summary of multiple books and online resources on time series analysis. With my own interpretation and explanations… . References: . | Practical Time Series Analysis: Prediction with Statistics and Machine Learning by Aileen Nielsen | . ",
    "url": "/docs/data-science/time-series/",
    "relUrl": "/docs/data-science/time-series/"
  },"412": {
    "doc": "DS Quick Notes",
    "title": "Data Science Quick Notes",
    "content": "To be added . Fragments of things I learned. They may be moved to a separate category later. ",
    "url": "/docs/data-science/notes/#data-science-quick-notes",
    "relUrl": "/docs/data-science/notes/#data-science-quick-notes"
  },"413": {
    "doc": "DS Quick Notes",
    "title": "DS Quick Notes",
    "content": " ",
    "url": "/docs/data-science/notes/",
    "relUrl": "/docs/data-science/notes/"
  },"414": {
    "doc": "Data Science",
    "title": "Data Science",
    "content": " ",
    "url": "/docs/data-science/#data-science",
    "relUrl": "/docs/data-science/#data-science"
  },"415": {
    "doc": "Data Science",
    "title": "Data Science",
    "content": ". ",
    "url": "/docs/data-science/",
    "relUrl": "/docs/data-science/"
  },"416": {
    "doc": "SSL",
    "title": "SSL",
    "content": " ",
    "url": "/docs/security/ssl/",
    "relUrl": "/docs/security/ssl/"
  },"417": {
    "doc": "SSH",
    "title": "SSH",
    "content": " ",
    "url": "/docs/security/ssh/",
    "relUrl": "/docs/security/ssh/"
  },"418": {
    "doc": "PGP Key",
    "title": "PGP Key",
    "content": ". | OpenPGP | GnuPG . | Basic usage | Editing a key | Encrypt and decrypt | Error | . | Key ID . | Fingerprint | Long key ID | Short key ID | . | . ",
    "url": "/docs/security/pgp/",
    "relUrl": "/docs/security/pgp/"
  },"419": {
    "doc": "PGP Key",
    "title": "OpenPGP",
    "content": "To be added . ",
    "url": "/docs/security/pgp/#openpgp",
    "relUrl": "/docs/security/pgp/#openpgp"
  },"420": {
    "doc": "PGP Key",
    "title": "GnuPG",
    "content": "Basic usage . # Follow prompt to create keys gpg --full-generate-key # List public keys gpg --list-keys # List secret keys gpg --list-secret-keys . Editing a key . gpg --edit-key &lt;key-id&gt; . To fix the expiration setting, for example, do: . gpg&gt; expire ...prompt... Then save the settings by: . gpg&gt; save . Related files will be stored in ~/.gnupg. Encrypt and decrypt . To be added . The last % of decrypted output is unused. Error . In case you get any error of the following: . $ gpg: public key decryption failed: Inappropriate ioctl for device $ gpg: decryption failed: Inappropriate ioctl for device . or . $ gpg: public key decryption failed: No such file or directory $ gpg: decryption failed: No such file or directory . Try: . echo $GPG_TTY . If it shows a not a tty error, set: . export GPG_TTY=$(tty) . You can place them in your shell configuration file. ",
    "url": "/docs/security/pgp/#gnupg",
    "relUrl": "/docs/security/pgp/#gnupg"
  },"421": {
    "doc": "PGP Key",
    "title": "Key ID",
    "content": "The key ID is calculated from your public key and the creation timestamp. Fingerprint . The long hex printed with gpg --list-keys is the fingerprint of the key. Long key ID . The last 16 hex of the fingerprint. Short key ID . The last 8 hex of the fingerprint. You can provide either one of the three for a key ID. ",
    "url": "/docs/security/pgp/#key-id",
    "relUrl": "/docs/security/pgp/#key-id"
  },"422": {
    "doc": "Vault",
    "title": "Vault",
    "content": ". | Installation . | With Homebrew | With Docker | . | Server configuration file . | storage | listener | log level | ttl (Time-To-Live) | ui | . | . ",
    "url": "/docs/security/vault/",
    "relUrl": "/docs/security/vault/"
  },"423": {
    "doc": "Vault",
    "title": "Installation",
    "content": "With Homebrew . brew tap hashicorp/tap . brew install hashicorp/tap/vault . With Docker . Official docker image is vault. Three volumes can be mounted. | /vault/logs to persist logs | /vault/file to persist data when file is the storage backnd for Vault | /vault/config for Vault server configuration file | . By default, Vault will run in container as a development server (vault server -dev). Vault entrypoint checks for a command and uses it as a subcommand to vault. If you do not wish to run in development mode, set command to server. To prevent memory leaking information to disk through swaps, container must be run with cap-add set to IPC_LOCK. To disable memory locking due to setcap issues, set SKIP_SETCAP environment variable to a non-empty value. In non-development environment, you must add disable_mlock: true to the configuration file to disable this functionality. Place a configuration file (either using .hcl or .json) for the Vault server in /vault/config. Vault will automatically read it. ",
    "url": "/docs/security/vault/#installation",
    "relUrl": "/docs/security/vault/#installation"
  },"424": {
    "doc": "Vault",
    "title": "Server configuration file",
    "content": "You can use either HCL or JSON, but I will use HCL because I prefer its syntax. The entire set of configuration can be found here. The following are some of the most basic configurations to run a Vault server. storage . The list of all storage backends can be found here. The simplest storage backend is the filesystem. Example: . storage \"file\" { path = \"/vault/file\" } . listener . listener configures where Vault should listen for requests. There is only one configuration right now which is TCP. listener \"tcp\" { # If you're using docker, and you want to access the web UI # Use address = \"0.0.0.0:8200\" address = \"127.0.0.1:8200\" # You must explicitly disable tls if you're not using it tls_disable = \"false\" | \"true\" (string) # Else tls_cert_file = \"...\" tls_key_file = \"...\" } . Make sure to secure your connection with tls (Let’s Encrypt or so) if you expect your Vault server to have non-local http requests, which usually is the case when being used for production. log level . Specifies log level. log_level = \"trace\" | \"debug\" | \"error\" | \"warn\" | \"info\" . ttl (Time-To-Live) . max_lease_ttl = \"768h\" (string) default_least_ttl = \"700h\" (string) . These set the lease expiration time for non-root tokens and secrets. default_least_ttl cannot be greater than max_lease_ttl. max_lease_ttl can be overriden later for different token lease methods. ui . ui = false | true (boolean) . Set to true to enable web UI. ",
    "url": "/docs/security/vault/#server-configuration-file",
    "relUrl": "/docs/security/vault/#server-configuration-file"
  },"425": {
    "doc": "Security",
    "title": "Security",
    "content": " ",
    "url": "/docs/security/",
    "relUrl": "/docs/security/"
  },"426": {
    "doc": "GitHub",
    "title": "GitHub",
    "content": " ",
    "url": "/docs/git-hub/github/",
    "relUrl": "/docs/git-hub/github/"
  },"427": {
    "doc": "Git",
    "title": "Git",
    "content": " ",
    "url": "/docs/git-hub/git/",
    "relUrl": "/docs/git-hub/git/"
  },"428": {
    "doc": "Git/GitHub",
    "title": "Git/GitHub",
    "content": " ",
    "url": "/docs/git-hub/",
    "relUrl": "/docs/git-hub/"
  },"429": {
    "doc": "Git/GitHub",
    "title": "You probably already know what this is",
    "content": "Version control system; awesome stuff. ",
    "url": "/docs/git-hub/#you-probably-already-know-what-this-is",
    "relUrl": "/docs/git-hub/#you-probably-already-know-what-this-is"
  },"430": {
    "doc": "Vue",
    "title": "Vue",
    "content": " ",
    "url": "/docs/vue/",
    "relUrl": "/docs/vue/"
  },"431": {
    "doc": "Vim",
    "title": "Vim",
    "content": "To be added . | Window | Key mapping | NerdTree | . ",
    "url": "/docs/others/vim/",
    "relUrl": "/docs/others/vim/"
  },"432": {
    "doc": "Vim",
    "title": "Window",
    "content": "To split, . :sp # Split window horizontally :vsp # Split window vertically . To navigate between windows, . &lt;Ctrl&gt; + w + w # Navigate between viewports &lt;Ctrl&gt; + w + h/j/k/l # Navigate to respective direction . ",
    "url": "/docs/others/vim/#window",
    "relUrl": "/docs/others/vim/#window"
  },"433": {
    "doc": "Vim",
    "title": "Key mapping",
    "content": "There are 6 types of mapping commands: . :map # Recursive / Works in normal, visual, select and operator modes :noremap # Non-recursive version :nmap # Recursive / Works in normal mode :nnoremap :vmap # Recursive / Works in visual mode :vnoremap . Type one of the above to list the current key mappings. To set new mappings, add them to ~/.vim/vimrc: . \"Example nnoremap &lt;C-n&gt; :&lt;NERDTreeToggle&lt;CR&gt; . ",
    "url": "/docs/others/vim/#key-mapping",
    "relUrl": "/docs/others/vim/#key-mapping"
  },"434": {
    "doc": "Vim",
    "title": "NerdTree",
    "content": "Set shortcut to toggle NerdTree: . nnoremap nerd :&lt;NERDTreeToggle&lt;CR&gt; . In NerdTree, type ? to toggle help. References: . | Key Mapping | . ",
    "url": "/docs/others/vim/#nerdtree",
    "relUrl": "/docs/others/vim/#nerdtree"
  },"435": {
    "doc": "ngrok",
    "title": "ngrok",
    "content": "Official Guide . | Installation | Setup | Forward a local port to a public URL | . ",
    "url": "/docs/others/ngrok/",
    "relUrl": "/docs/others/ngrok/"
  },"436": {
    "doc": "ngrok",
    "title": "Installation",
    "content": "brew install ngrok/ngrok/ngrok # OR brew install --cask ngrok . Check installation via: . ngrok -h . ",
    "url": "/docs/others/ngrok/#installation",
    "relUrl": "/docs/others/ngrok/#installation"
  },"437": {
    "doc": "ngrok",
    "title": "Setup",
    "content": "To use ngrok you must be signed up for an account. You can do this via the official website. Once you login, you will be given an authtoken to authenticate ngrok. Copy the value and authenticate in terminal: . ngrok config add-authtoken $AUTH_TOKEN . ",
    "url": "/docs/others/ngrok/#setup",
    "relUrl": "/docs/others/ngrok/#setup"
  },"438": {
    "doc": "ngrok",
    "title": "Forward a local port to a public URL",
    "content": "Run the following command: . ngrok http $PORT . The public URL will be displayed in the terminal: ... Forwarding https://some.url.given.ngrok.io -&gt; http://localhost:$PORT ... For free plans, the URL changes every time you restart ngrok. ",
    "url": "/docs/others/ngrok/#forward-a-local-port-to-a-public-url",
    "relUrl": "/docs/others/ngrok/#forward-a-local-port-to-a-public-url"
  },"439": {
    "doc": "Jekyll",
    "title": "Jekyll",
    "content": ". | Ruby installation with rbenv | Install Jekyll | Create a Jekyll blog | Bundler / Gemfile . | Install gems listed in Gemfile | Adding gems to project | Removing gems | Execute a command in the context of the bundle | . | . ",
    "url": "/docs/others/jekyll/",
    "relUrl": "/docs/others/jekyll/"
  },"440": {
    "doc": "Jekyll",
    "title": "Ruby installation with rbenv",
    "content": "I’ve decided to use rbenv only because I didn’t want to mess with the system ruby that comes with macOS (I am currently using Catalina). Assuming you have Homebrew installed. # Install rbenv and ruby-build brew install rbenv # Set up rbenv integration with your shell rbenv init # Then follow the instruction that appears on screen . # rbenv init will ask you to add the following to .zshrc eval \"$(rbenv init - zsh)\" . Now that you have installed rbenv, create a folder that will contain your Jekyll site. I will refer to the folder as blog. Once created, move into blog. cd blog # List latest stable versions rbenv install -l # I chose 3.0.2 rbenv install 3.0.2 rbenv rehash # Following creates .ruby-version in cwd rbenv local 3.0.2 # Confirm ruby version in folder ruby -v . All the ruby versions are installed in ~/.rbenv. ",
    "url": "/docs/others/jekyll/#ruby-installation-with-rbenv",
    "relUrl": "/docs/others/jekyll/#ruby-installation-with-rbenv"
  },"441": {
    "doc": "Jekyll",
    "title": "Install Jekyll",
    "content": "Before installing the gems, check where they are being installed via . # Refer to INSTALLATION DIRECTORY / GEM PATHS gem env # OR gem env home . Rest of the stuff are just my preferences/me being a clean freak. Now, the Jekyll documentation tells you to do a local install with the --user-install flag. If you’re not using rbenv this is indeed more desirable as it installs your gems to your home directory (like ~/.gem). However, for my purpose and with rbenv it was unnecessary. As you’ll notice by inspecting the gem env outputs, the global install directory (INSTALLATION DIRECTORY) is already in your home directory (~/.rbenv/versions/...). On the other hand, the user install directory (USER INSTALLATION DIRECTORY) is set to some local folder (~/.local/share/gem/ruby/...). I personally prefer having all the packages contained in ~/.rbenv, so I simply chose to omit --user-install and do: . End of me being a freak. gem install jekyll bundler . ",
    "url": "/docs/others/jekyll/#install-jekyll",
    "relUrl": "/docs/others/jekyll/#install-jekyll"
  },"442": {
    "doc": "Jekyll",
    "title": "Create a Jekyll blog",
    "content": "First create a new Jekyll project by . # Assuming you're still in the blog folder jekyll new . It will create a default website you can test locally. # Will generate a static html site in _site bundle exec jekyll serve # With live-reloading bundle exec jekyll serve --livereload . If you get any errors regarding webrick: cannot load such file -- webrick (LoadError), add webrick by bundle add webrick. This is due to ruby 3 excluding webrick as a default bundled gem. ",
    "url": "/docs/others/jekyll/#create-a-jekyll-blog",
    "relUrl": "/docs/others/jekyll/#create-a-jekyll-blog"
  },"443": {
    "doc": "Jekyll",
    "title": "Bundler / Gemfile",
    "content": "Think of the bundler as the npm/yarn of Ruby and Gemfile as the package.json of Node projects. When you create a new Jekyll project with jekyll new, a Gemfile is automatically created. This Gemfile will list the basic gem dependencies to create a basic Jekyll site. Install gems listed in Gemfile . If there already exists a Gemfile, you can download all the necessary gems for this project by: . bundle install . These gems are usually installed in the same place they would be when you call gem install. Exact location can be confirmed by bundle show &lt;gem-name&gt;. Adding gems to project . When you need to add another gem for your project, you can either: . | Type it out yourself in Gemfile | . # Gemfile gem \"just-the-docs\" . Then, . bundle install . OR . | Use bundle add | . bundle add just-the-docs . If the gems already exist in system, it’ll just use that. If they don’t already, it will download the gem for you. Removing gems . When you no longer need a gem for the project, . bundle remove just-the-docs . This doesn’t remove the gem from the system. Only removes it from your project’s Gemfile. Execute a command in the context of the bundle . Every bundled gem will be made available in the context of the command you wish to execute even if these gems are not in the executable path. bundle exec jekyll build . References: . | Markdown Supported Languages | . ",
    "url": "/docs/others/jekyll/#bundler--gemfile",
    "relUrl": "/docs/others/jekyll/#bundler--gemfile"
  },"444": {
    "doc": "DBeaver",
    "title": "DBeaver",
    "content": ". | Installation | Export settings and connections | Download Vim plugin | SSH Troubleshooting | . ",
    "url": "/docs/others/dbeaver/",
    "relUrl": "/docs/others/dbeaver/"
  },"445": {
    "doc": "DBeaver",
    "title": "Installation",
    "content": "brew install --cask dbeaver-community . ",
    "url": "/docs/others/dbeaver/#installation",
    "relUrl": "/docs/others/dbeaver/#installation"
  },"446": {
    "doc": "DBeaver",
    "title": "Export settings and connections",
    "content": "On MacOS, workspace configurations are stored in ~/Library/DBeaverData. Refer to link for different OS. To export the settings and connections, copy ~/Library/DBeaverData/workspace6/General/.dbeaver from source to target. ",
    "url": "/docs/others/dbeaver/#export-settings-and-connections",
    "relUrl": "/docs/others/dbeaver/#export-settings-and-connections"
  },"447": {
    "doc": "DBeaver",
    "title": "Download Vim plugin",
    "content": "On the top nagivation bar, click Help &gt; Install New Software.... Then type the following URL in the Work with field and click Add. http://vrapper.sourceforge.net/update-site/stable . Enter Vrapper for the Name field and click OK. Then select Vrapper and any other optional Vim plugins to install. Click Next to install the plugin. I personally find optional Surround.vim plugin to be very useful. After installation, restart DBeaver. ",
    "url": "/docs/others/dbeaver/#download-vim-plugin",
    "relUrl": "/docs/others/dbeaver/#download-vim-plugin"
  },"448": {
    "doc": "DBeaver",
    "title": "SSH Troubleshooting",
    "content": "While SSHing into a remote server, if you get the following error: . invalid privatekey: [B@540..... You are probably using a key algorithm incompatible for JSch implementation. To solve this navigate to Connection settings -&gt; SSH. Open Advanced settings, and change Implementation to SSHJ. ",
    "url": "/docs/others/dbeaver/#ssh-troubleshooting",
    "relUrl": "/docs/others/dbeaver/#ssh-troubleshooting"
  },"449": {
    "doc": "Chrome Extensions",
    "title": "Chrome Extensions",
    "content": "To be added . Official Documentation . | Manifest V3 / V2 | To view the contents of extension storage | . ",
    "url": "/docs/others/chrome-ext/",
    "relUrl": "/docs/others/chrome-ext/"
  },"450": {
    "doc": "Chrome Extensions",
    "title": "Manifest V3 / V2",
    "content": "To be added . ",
    "url": "/docs/others/chrome-ext/#manifest-v3--v2",
    "relUrl": "/docs/others/chrome-ext/#manifest-v3--v2"
  },"451": {
    "doc": "Chrome Extensions",
    "title": "To view the contents of extension storage",
    "content": "Open the background in inspect view mode. Then type the following: . chrome.storage.local.get(console.log) . ",
    "url": "/docs/others/chrome-ext/#to-view-the-contents-of-extension-storage",
    "relUrl": "/docs/others/chrome-ext/#to-view-the-contents-of-extension-storage"
  },"452": {
    "doc": "Homebrew",
    "title": "Homebrew",
    "content": "Package manager for macOS . Official Page . | Installation . | Opt-out of Homebrew analytics | . | Useful commands . | brew search | brew install | brew uninstall | brew list | brew deps | brew info | brew update | brew upgrade | brew doctor | brew autoremove | . | Installing other versions of Casks | Notes . | keg-only | . | . ",
    "url": "/docs/others/homebrew/",
    "relUrl": "/docs/others/homebrew/"
  },"453": {
    "doc": "Homebrew",
    "title": "Installation",
    "content": "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" . Then follow the instructions. In my case, I had to add /opt/homebrew/bin to PATH. echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; ~/.zprofile eval \"$(/opt/hombrew/bin/brew shellenv)\" # Or just open a new tab . Opt-out of Homebrew analytics . brew analytics off . ",
    "url": "/docs/others/homebrew/#installation",
    "relUrl": "/docs/others/homebrew/#installation"
  },"454": {
    "doc": "Homebrew",
    "title": "Useful commands",
    "content": "brew search . brew search ${package} . brew install . brew install ${package} brew install --cask ${package} . cask is an extension to Hombrew formulae, mainly for GUI applications . brew uninstall . brew uninstall ${package} . brew uninstall won’t let you remove a package if it is a dependency of another package. You could force uninstall, but you generally don’t wanna do this. brew list . brew list brew list --versions . brew deps . brew deps ${package} brew deps --installed brew deps --installed --tree . Shows dependencies. brew info . brew info ${package} . Shows a summary of information of a formula/cask. Summary includes dependencies, current stable version, install status, etc. brew update . brew update . Updates brew itself. brew upgrade . brew upgrade # Upgrade all brew upgrade ${package} . Upgrades installed packages that are outdated. brew doctor . brew doctor . Diagnoses problems or errors regarding, but not limited to, brew. Possible warnings or errors may include, interruption during brew install, failure to symlink binary, deprecated Xcode, etc. brew autoremove . brew autoremove . This removes dangling dependencies that were not removed with the parent package. ",
    "url": "/docs/others/homebrew/#useful-commands",
    "relUrl": "/docs/others/homebrew/#useful-commands"
  },"455": {
    "doc": "Homebrew",
    "title": "Installing other versions of Casks",
    "content": "If you want to install an older version of a cask, . brew tap homebrew/cask-versions . Then search for the version you want. brew search ${package} . ",
    "url": "/docs/others/homebrew/#installing-other-versions-of-casks",
    "relUrl": "/docs/others/homebrew/#installing-other-versions-of-casks"
  },"456": {
    "doc": "Homebrew",
    "title": "Notes",
    "content": "keg-only . By default, brew installed binaries are symlinked to /usr/local/bin, but keg-only formulae are not. This is usually due to the preexistence of an older OS shipped default version, typically in /usr/bin. Although not symlinked in /usr/local/bin, keg-only or not, every brew formula is kept in /usr/local/Cellar and every formula is symlinked in /usr/local/opt. Which means you can add /usr/local/opt/&lt;formula&gt;/bin to PATH (just making sure it goes in the front of /usr/bin so that it is found first). All of the information here can be found during install or brew info Caveats. ",
    "url": "/docs/others/homebrew/#notes",
    "relUrl": "/docs/others/homebrew/#notes"
  },"457": {
    "doc": "Volta",
    "title": "Volta",
    "content": "Javascript command-line tool manager . Official Guide . | Why Volta? | Installation . | Using Homebrew | Using installation script | . | Usage example . | Install | . | curl SSL certificate problem . | Workaround (Not Recommended) | Fix | . | . ",
    "url": "/docs/others/volta/",
    "relUrl": "/docs/others/volta/"
  },"458": {
    "doc": "Volta",
    "title": "Why Volta?",
    "content": "If you’ve ever tried uninstalling Node or installing a newer version of Node for a project, you may have found that it can get quite ugly. Volta keeps all of the binaries in your home directory, and makes it easy to install and uninstall different versions. You can also use Volta to pin a specified Node version for each project, much like the Python virtual environments. ",
    "url": "/docs/others/volta/#why-volta",
    "relUrl": "/docs/others/volta/#why-volta"
  },"459": {
    "doc": "Volta",
    "title": "Installation",
    "content": "Using Homebrew . brew install volta . Add to ~/.zshrc: . export VOLTA_HOME=\"$HOME/.volta\" export PATH=\"$VOLTA_HOME/bin:$PATH\" . Using installation script . curl https://get.volta.sh | bash . Necessary PATH will be added to .zshrc. If you get a curl: (60) SSL certificate problem: certificate has expired error, you may be using an old version of OpenSSL or LibreSSL. Workaround/Fix . ",
    "url": "/docs/others/volta/#installation",
    "relUrl": "/docs/others/volta/#installation"
  },"460": {
    "doc": "Volta",
    "title": "Usage example",
    "content": "Install . volta install node volta install yarn . ",
    "url": "/docs/others/volta/#usage-example",
    "relUrl": "/docs/others/volta/#usage-example"
  },"461": {
    "doc": "Volta",
    "title": "curl SSL certificate problem",
    "content": "This is a known issue (as of Sep. 2021). The issue is not due to Volta but is related to an older version of OpenSSL/LibreSSL. See here for details, but long story short: . | Update to OpenSSL 1.1 for secure connection using LetsEncrypt certificates. | . Workaround (Not Recommended) . As suggested by this comment, one hacky workaround is to just use an insecure (-k) curl: . curl -k https://get.volta.sh &gt; volta.sh . Then change line 10 of volta.sh to use an insecure curl as well: . 9| get_latest_release() { 10| curl -k --silent \"https://volta.sh/latest-version\" 11| } . Then run: . bash volta.sh . It works, but defeats the whole purpose of certificates. Fix . Better approach is to install the brew packaged curl, as it uses OpenSSL 1.1 while the shipped curl uses an older version of LibreSSL. ",
    "url": "/docs/others/volta/#curl-ssl-certificate-problem",
    "relUrl": "/docs/others/volta/#curl-ssl-certificate-problem"
  },"462": {
    "doc": "Scrapy",
    "title": "Scrapy",
    "content": "To be added . Python web scraper . Official Documentation . | Installation | Start a project | Create a spider . | Show available spider templates | Generate spider | Check generated spiders | . | . ",
    "url": "/docs/others/scrapy/",
    "relUrl": "/docs/others/scrapy/"
  },"463": {
    "doc": "Scrapy",
    "title": "Installation",
    "content": "pip install Scrapy # or poetry add Scrapy . ",
    "url": "/docs/others/scrapy/#installation",
    "relUrl": "/docs/others/scrapy/#installation"
  },"464": {
    "doc": "Scrapy",
    "title": "Start a project",
    "content": "cd &lt;proj-root&gt; scrapy startproject &lt;proj-name&gt; . ",
    "url": "/docs/others/scrapy/#start-a-project",
    "relUrl": "/docs/others/scrapy/#start-a-project"
  },"465": {
    "doc": "Scrapy",
    "title": "Create a spider",
    "content": "First navigate to a specific Scrapy project: . cd &lt;proj-name&gt; . Check that you are indeed in the right project by: . $ scrapy Scrapy x.x.x - project: &lt;proj-name&gt; . Show available spider templates . $ scrapy genspider -l basic crawl csvfeed xmlfeed . Generate spider . scrapy genspider -t crawl &lt;spider-name&gt; &lt;allowed-domain&gt; . Check generated spiders . scrapy list . ",
    "url": "/docs/others/scrapy/#create-a-spider",
    "relUrl": "/docs/others/scrapy/#create-a-spider"
  },"466": {
    "doc": "MongoDB",
    "title": "MongoDB",
    "content": "On-prem community edition . | Install MongoDB (locally) | Run and stop MongoDB (locally) | . ",
    "url": "/docs/others/mongodb/",
    "relUrl": "/docs/others/mongodb/"
  },"467": {
    "doc": "MongoDB",
    "title": "Install MongoDB (locally)",
    "content": "brew tap mongodb/brew brew install mongodb-community@4.4 . This installs . | mongod server | mongos sharded cluster query router | mongo shell | . And also . | /usr/local/etc/mongod.conf configuration file | /usr/local/var/log/mongodb log directory | /usr/local/var/mongodb data directory | . And finally MongoDB Database Tools . Location varies by system. Check with brew --prefix. ",
    "url": "/docs/others/mongodb/#install-mongodb-locally",
    "relUrl": "/docs/others/mongodb/#install-mongodb-locally"
  },"468": {
    "doc": "MongoDB",
    "title": "Run and stop MongoDB (locally)",
    "content": "Run MongoDB as a macOS service (recommended) . brew services start mongodb-community@4.4 # Verify it is running (should be in started status) brew service list | grep mongodb-community . You can then use the mongo shell via . mongo . Stop MongoDB . brew services stop mongodb-community@4.4 . References: . | MongoDB: Install | . ",
    "url": "/docs/others/mongodb/#run-and-stop-mongodb-locally",
    "relUrl": "/docs/others/mongodb/#run-and-stop-mongodb-locally"
  },"469": {
    "doc": "zsh",
    "title": "zsh &amp; Shell setup",
    "content": ". | Install zsh . | OS X | Ubuntu | . | Change to zsh | Install oh-my-zsh | Change Theme | Recommended plugins . | zsh-syntax-highlighting | zsh-autosuggestions | fzf | fasd | . | Preferred Iterm2/Gnome Terminal color schemes | . ",
    "url": "/docs/others/zsh/#zsh--shell-setup",
    "relUrl": "/docs/others/zsh/#zsh--shell-setup"
  },"470": {
    "doc": "zsh",
    "title": "Install zsh",
    "content": "OS X . brew install zsh . Ubuntu . sudo apt install zsh . ",
    "url": "/docs/others/zsh/#install-zsh",
    "relUrl": "/docs/others/zsh/#install-zsh"
  },"471": {
    "doc": "zsh",
    "title": "Change to zsh",
    "content": "Make zsh the default shell . chsh -s $(which zsh) . Confirm shell has changed . echo $SHELL . In Ubuntu, if echo $SHELL or echo $0 still shows bash, try logging out and log back in. Hopefully, shell would have been changed and zsh-newuser-install will pop up. ",
    "url": "/docs/others/zsh/#change-to-zsh",
    "relUrl": "/docs/others/zsh/#change-to-zsh"
  },"472": {
    "doc": "zsh",
    "title": "Install oh-my-zsh",
    "content": "Assuming you have curl installed, . sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" . In case of a change refer to here for a new link. ",
    "url": "/docs/others/zsh/#install-oh-my-zsh",
    "relUrl": "/docs/others/zsh/#install-oh-my-zsh"
  },"473": {
    "doc": "zsh",
    "title": "Change Theme",
    "content": "My preferred theme is Powerlevel10k. To install it as an Oh My Zsh theme, . git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k . Then in ~/.zshrc, set ZSH_THEME . ZSH_THEME=\"powerlevel10k/powerlevel10k\" . When using Iterm2, the recommended fonts are automatically installed. Otherwise, install the fonts from here. ",
    "url": "/docs/others/zsh/#change-theme",
    "relUrl": "/docs/others/zsh/#change-theme"
  },"474": {
    "doc": "zsh",
    "title": "Recommended plugins",
    "content": "zsh-syntax-highlighting . See here for details. git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting . To activate the plugin, go to .zshrc and add zsh-syntax-highlighting to plugins. zsh-autosuggestions . See here for details. git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions . To activate the plugin, go to .zshrc and add zsh-autosuggestions to plugins. fzf . See here for details. # OS X brew install fzf # Ubuntu sudo apt install fzf . Then activate the plugin in .zshrc. fasd . See here for details. # OS X brew install fasd # Ubuntu sudo apt install fasd . Then activate the plugin in .zshrc. ",
    "url": "/docs/others/zsh/#recommended-plugins",
    "relUrl": "/docs/others/zsh/#recommended-plugins"
  },"475": {
    "doc": "zsh",
    "title": "Preferred Iterm2/Gnome Terminal color schemes",
    "content": "Look for the following themes in Iterm2 / Gough: . | Snazzy | Tomorrow Night | . ",
    "url": "/docs/others/zsh/#preferred-iterm2gnome-terminal-color-schemes",
    "relUrl": "/docs/others/zsh/#preferred-iterm2gnome-terminal-color-schemes"
  },"476": {
    "doc": "zsh",
    "title": "zsh",
    "content": " ",
    "url": "/docs/others/zsh/",
    "relUrl": "/docs/others/zsh/"
  },"477": {
    "doc": "IntelliJ",
    "title": "IntelliJ",
    "content": " ",
    "url": "/docs/others/intellij/",
    "relUrl": "/docs/others/intellij/"
  },"478": {
    "doc": "Others",
    "title": "List of All Documentations",
    "content": " ",
    "url": "/docs/others/#list-of-all-documentations",
    "relUrl": "/docs/others/#list-of-all-documentations"
  },"479": {
    "doc": "Others",
    "title": "Others",
    "content": " ",
    "url": "/docs/others/",
    "relUrl": "/docs/others/"
  },"480": {
    "doc": "Java",
    "title": "Java",
    "content": " ",
    "url": "/docs/java/",
    "relUrl": "/docs/java/"
  },"481": {
    "doc": "Flutter",
    "title": "Flutter",
    "content": " ",
    "url": "/docs/flutter/",
    "relUrl": "/docs/flutter/"
  },"482": {
    "doc": "Linux",
    "title": "Linux",
    "content": " ",
    "url": "/docs/linux/",
    "relUrl": "/docs/linux/"
  },"483": {
    "doc": "MySQL/MariaDB",
    "title": "MySQL/MariaDB",
    "content": " ",
    "url": "/docs/mysql/",
    "relUrl": "/docs/mysql/"
  },"484": {
    "doc": "AWS",
    "title": "AWS",
    "content": " ",
    "url": "/docs/aws/",
    "relUrl": "/docs/aws/"
  },"485": {
    "doc": "Jenkins",
    "title": "Jenkins",
    "content": " ",
    "url": "/docs/jenkins/",
    "relUrl": "/docs/jenkins/"
  },"486": {
    "doc": "Kubernetes",
    "title": "Kubernetes",
    "content": " ",
    "url": "/docs/kubernetes/",
    "relUrl": "/docs/kubernetes/"
  },"487": {
    "doc": "Math and Stats Basics",
    "title": "Math and Stats Basics",
    "content": ". ",
    "url": "/docs/bootcamp/math-stats/",
    "relUrl": "/docs/bootcamp/math-stats/"
  },"488": {
    "doc": "Computing Basics",
    "title": "Computing Basics",
    "content": " ",
    "url": "/docs/bootcamp/computing/",
    "relUrl": "/docs/bootcamp/computing/"
  },"489": {
    "doc": "Inferential Statistics",
    "title": "Inferential Statistics",
    "content": ". | Recap: Why Inferential Statistics? . | Overview of the idea | Modeling | . | Random Sampling . | Simple Random Sampling | Stratified Sampling | Other Sampling Methods | . | Sampling Distribution . | Sample Statistic as a Random Variable | . | Sampling Distribution of the Sample Mean . | Law of Large Numbers | Central Limit Theorem | . | Estimators . | Consistent Estimator | Unbiased Estimator | . | Sampling Error . | Standard Error of the Mean | . | Confidence Interval . | Confidence Interval for the Mean | . | t-Distribution . | t-Score | t-Distribution vs Standard Normal Distribution | Degrees of Freedom | Confidence Interval Using t-Distribution . | Narrowing the Confidence Interval | . | . | . ",
    "url": "/docs/statistics/basics/inferencial-stats.html",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html"
  },"490": {
    "doc": "Inferential Statistics",
    "title": "Recap: Why Inferential Statistics?",
    "content": "Our goal is to understand the population. We have two main options: . | Complete enumeration | Sample survey | . Because complete enumeration is unrealistic, we need to figure out how to use samples to understand the population. Inferential statistics is used to analyze samples and infer the population. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#recap-why-inferential-statistics",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#recap-why-inferential-statistics"
  },"491": {
    "doc": "Inferential Statistics",
    "title": "Overview of the idea",
    "content": ". | Assume population is represented by a probability distribution, namely the population distribution. | The parameters define the shape of population distribution. | The samples are values drawn from the population distribution. | From the samples, we infer the parameters of the population distribution. | . ",
    "url": "/docs/statistics/basics/inferencial-stats.html#overview-of-the-idea",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#overview-of-the-idea"
  },"492": {
    "doc": "Inferential Statistics",
    "title": "Modeling",
    "content": "In a realistic sense, the population distribution cannot exactly match a certain mathematical probability distribution. Instead, we are merely modeling the population distribution with a certain probability distribution. Sounds obvious, but it is an important concept to keep in mind. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#modeling",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#modeling"
  },"493": {
    "doc": "Inferential Statistics",
    "title": "Random Sampling",
    "content": "Random sampling is essential if we want an unbiased inference. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#random-sampling",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#random-sampling"
  },"494": {
    "doc": "Inferential Statistics",
    "title": "Simple Random Sampling",
    "content": "This is equivalent to drawing papers from a jar. Results are ideal, but not realistic as it can be time-consuming and costly. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#simple-random-sampling",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#simple-random-sampling"
  },"495": {
    "doc": "Inferential Statistics",
    "title": "Stratified Sampling",
    "content": "This is equivalent to drawing papers from jars of different colors. We divide the population into strata, and draw random samples from each stratum. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#stratified-sampling",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#stratified-sampling"
  },"496": {
    "doc": "Inferential Statistics",
    "title": "Other Sampling Methods",
    "content": "To be added . | Systematic sampling | Cluster sampling | . ",
    "url": "/docs/statistics/basics/inferencial-stats.html#other-sampling-methods",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#other-sampling-methods"
  },"497": {
    "doc": "Inferential Statistics",
    "title": "Sampling Distribution",
    "content": "Sampling distribution is the probability distribution of a statistic obtained from a random sample of size $n$. It is the probability distribution you observe when you repeatedly draw samples of size $n$ from the population and calculate each sample’s statistic. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#sampling-distribution",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#sampling-distribution"
  },"498": {
    "doc": "Inferential Statistics",
    "title": "Sample Statistic as a Random Variable",
    "content": "Take the example of the sample mean $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$. Each $x_i$ is also a random variable since it is randomly drawn from the population. Then $\\bar{x}$ is also a random variable since it is a function of $x_i$. We generally use lower case letters to denote the realization of a random variable, and use upper case letters to denote the random variable itself. To make it more clear, let’s use capital letters to denote the random variables. $$ \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i $$ . Then we know that $\\bar{X}$ must have an associated probability distribution. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#sample-statistic-as-a-random-variable",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#sample-statistic-as-a-random-variable"
  },"499": {
    "doc": "Inferential Statistics",
    "title": "Sampling Distribution of the Sample Mean",
    "content": "Below are some of the useful concepts when inferencing the population mean from the sample mean. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#sampling-distribution-of-the-sample-mean",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#sampling-distribution-of-the-sample-mean"
  },"500": {
    "doc": "Inferential Statistics",
    "title": "Law of Large Numbers",
    "content": "The law of large numbers states that the sample mean $\\bar{x}$ converges to the population mean $\\mu$ as the sample size $n$ increases. $$ \\bar{x} \\to \\mu \\text{ as } n \\to \\infty $$ . This only holds for the mean, not for other statistics. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#law-of-large-numbers",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#law-of-large-numbers"
  },"501": {
    "doc": "Inferential Statistics",
    "title": "Central Limit Theorem",
    "content": "The central limit theorem (CLT) states that, regardless of whether the population distribution is normal or not, when the sample size $n$ is large enough, the sampling distribution of $\\bar{X}$ is approximately normal with parameters $\\mu$ and $\\frac{\\sigma^2}{n}$. Important assumption of CLT is that $X_i$ are i.i.d. $$ \\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n}) \\text{ as } n \\to \\infty $$ . So as $n$ increases, the sampling distribution of $\\bar{X}$ becomes a normal distribution where most of the values are close to $\\mu$ with a standard deviation of $\\frac{\\sigma}{\\sqrt{n}}$. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#central-limit-theorem",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#central-limit-theorem"
  },"502": {
    "doc": "Inferential Statistics",
    "title": "Estimators",
    "content": "An estimator is a statistic used to estimate a population parameter. Because it is a function of random variables, it is also a random variable. Realization of estimators are estimates of the parameter. Let’s say we have a population parameter $\\theta$ that we want to estimate. Let $X_i$ be the random variable and we have a statistic calculated from $X_i$. We will denote the statistic as $T_n$, where $n$ is the sample size. Then $T_n$ is an estimator of $\\theta$. $$ T_n = u(X_1, X_2, \\dots, X_n) $$ . ",
    "url": "/docs/statistics/basics/inferencial-stats.html#estimators",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#estimators"
  },"503": {
    "doc": "Inferential Statistics",
    "title": "Consistent Estimator",
    "content": "An estimator $T_n$ is consistent if $T_n$ converges in probability to $\\theta$ as $n \\to \\infty$. $$ \\plim_{n \\to \\infty} T_n = \\theta $$ . Convergence in probability $$ \\forall \\epsilon &gt; 0,\\; \\lim_{n \\to \\infty} Pr[|T_n - \\theta| &gt; \\epsilon] = 0 $$ . ",
    "url": "/docs/statistics/basics/inferencial-stats.html#consistent-estimator",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#consistent-estimator"
  },"504": {
    "doc": "Inferential Statistics",
    "title": "Unbiased Estimator",
    "content": "An estimator $T_n$ is unbiased if the expected value of $T_n$ is equal to $\\theta$. $$ E[T_n] = \\theta $$ . Therefore the sampling distribution of $T_n$ is centered at $\\theta$. This means that the estimator is not systematically over- or under-estimating. If the expected value of $T_n$ is not equal to $\\theta$, then the estimator is considered biased. One example of an unbiased estimator is the sample mean $\\bar{X}$. Proof that sample mean is an unbiased estimator of population mean $$ \\begin{align*} E[\\bar{X}] &amp;= E[\\frac{1}{n} \\sum_{i=1}^n X_i] \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n E[X_i] \\tag{linearity of expectation} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n \\mu \\tag{by definition} \\\\ &amp;= \\frac{1}{n} \\cdot n \\cdot \\mu \\\\ &amp;= \\mu \\end{align*} $$ . ",
    "url": "/docs/statistics/basics/inferencial-stats.html#unbiased-estimator",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#unbiased-estimator"
  },"505": {
    "doc": "Inferential Statistics",
    "title": "Sampling Error",
    "content": "Sampling error is the difference between the sample statistic and the population statistic. An example of sampling error would be $\\bar{X} - \\mu$. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#sampling-error",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#sampling-error"
  },"506": {
    "doc": "Inferential Statistics",
    "title": "Sampling Error as a Random Variable",
    "content": "Let’s take the example of $\\bar{X} - \\mu$. $\\bar{X} - \\mu$ is also a random variable since $\\mu$ is a constant. This means that $\\bar{X} - \\mu$ has an associated probability distribution. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#sampling-error-as-a-random-variable",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#sampling-error-as-a-random-variable"
  },"507": {
    "doc": "Inferential Statistics",
    "title": "Sampling Distribution",
    "content": "By the Central Limit Theorem, we know that the sampling distribution of $\\bar{X}$ is approximately normal with parameters $\\mu$ and $\\frac{\\sigma^2}{n}$. Since we’re merely shifting the distribution horizontally by $\\mu$, we know that the distribution of $\\bar{X} - \\mu$ is also approximately normal with parameters $0$ and $\\frac{\\sigma^2}{n}$: . $$ \\begin{equation*} \\label{eq:sa} \\end{equation*} \\bar{X} - \\mu \\sim N(0, \\frac{\\sigma^2}{n}) \\text{ as } n \\to \\infty $$ . ",
    "url": "/docs/statistics/basics/inferencial-stats.html#sampling-distribution-1",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#sampling-distribution-1"
  },"508": {
    "doc": "Inferential Statistics",
    "title": "Standard Error of the Mean",
    "content": "The standard error of the mean (SEM) is the standard deviation of the sampling distribution of the sampling mean $\\bar{X}$. $$ \\text{SEM} = \\frac{\\sigma}{\\sqrt{n}} $$ . However, we usually don’t know the population standard deviation $\\sigma$. So we use the sample standard deviation $s$ (or the unbiased estimator of the population standard deviation) instead: . $$ \\text{SEM} \\approx \\frac{s}{\\sqrt{n}} $$ . ",
    "url": "/docs/statistics/basics/inferencial-stats.html#standard-error-of-the-mean",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#standard-error-of-the-mean"
  },"509": {
    "doc": "Inferential Statistics",
    "title": "Confidence Interval",
    "content": "A confidence interval is a range of values that we are fairly confident contains the population parameter. So basically how confident can we be that the estimate obtained from the sample is close to the true population parameter. The smaller the confidence interval, the more precise the estimate is. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#confidence-interval",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#confidence-interval"
  },"510": {
    "doc": "Inferential Statistics",
    "title": "Confidence Interval for the Mean",
    "content": "Let’s say we want to estimate the population mean $\\mu$. We know that the sampling distribution of $\\bar{X} - \\mu$ is approximately . $$ \\bar{X} - \\mu \\sim N(0, \\frac{\\sigma^2}{n}) \\text{ as } n \\to \\infty $$ . Because the sampling distribution is normal, we know that approx. 95% of the values fall within approx. $2 \\cdot \\text{SEM}$ . Technically, 95% of the values fall within $1.96 \\cdot \\frac{s}{\\sqrt{n}}$. This 1.96 is the z-score that marks $0.025$ area (2.5%) on the right tail of the standard normal distribution. $$ \\begin{gather*} 0 - 1.96 \\cdot \\frac{s}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 0 + 1.96 \\cdot \\frac{s}{\\sqrt{n}} \\\\ \\downarrow \\\\ \\bar{X} - 1.96 \\cdot \\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} + 1.96 \\cdot \\frac{s}{\\sqrt{n}} \\end{gather*} $$ . Then we can say that we are 95% confident that the population mean $\\mu$ falls within the interval . $$ \\bar{X} \\pm 1.96 \\cdot \\frac{s}{\\sqrt{n}} $$ . Basically, when we perform the interval calculation for 100 samples, we expect the true population mean to fall within the interval 95 times out of the 100 times. This is the confidence interval at the 95% confidence level. 95% is the most common confidence level. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#confidence-interval-for-the-mean",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#confidence-interval-for-the-mean"
  },"511": {
    "doc": "Inferential Statistics",
    "title": "t-Distribution",
    "content": "The Central Limit Theorem only applies when the sample size $n$ is large enough. In reality, we don’t always have a large sample size during data analysis. Also, since we don’t know the population standard deviation $\\sigma$, we use the sample standard deviation $s$ as an estimator of $\\sigma$ instead. t-distribution is a probability distribution that is used when the sample size is small and the population standard deviation is unknown. The t-distribution assumes that the population from which the sample is drawn follows a normal distribution. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#t-distribution",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#t-distribution"
  },"512": {
    "doc": "Inferential Statistics",
    "title": "t-Score",
    "content": "The t-score is calculated by the following formula: . $$ T = \\frac{\\bar{X} - \\mu}{s/\\sqrt{n}} $$ . Just like the z-score, we are standardizing the sampling distribution of the sample mean $\\bar{X}$: . | Shifting the mean to 0 and standardizing variance to 1. | . The only difference is that we don’t know the population standard deviation $\\sigma$, so we estimate it with the sample standard deviation $s$. Because of this estimation, the t-score is not exactly a standard normal distribution. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#t-score",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#t-score"
  },"513": {
    "doc": "Inferential Statistics",
    "title": "t-Distribution vs Standard Normal Distribution",
    "content": "The t-distribution is similar to the normal distribution, but with heavier tails. The tails of a distribution are the regions that fall outside of 2 standard deviations from the mean. As the sample size $n$ increases, the t-distribution approaches $N(0, 1)$. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#t-distribution-vs-standard-normal-distribution",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#t-distribution-vs-standard-normal-distribution"
  },"514": {
    "doc": "Inferential Statistics",
    "title": "Degrees of Freedom",
    "content": "The degrees of freedom (typically denoted by $\\nu$ or d.f.) is the number of independent observations in a sample that are used to calculate an estimate of a population parameter. Typically, the degrees of freedom equals the number of sample size $n$ minus the number of parameters to estimate. When we’re estimating the mean, $\\nu = n - 1$. Degrees of freedom $\\nu$ is a parameter that determines the shape of the t-distribution. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#degrees-of-freedom",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#degrees-of-freedom"
  },"515": {
    "doc": "Inferential Statistics",
    "title": "Confidence Interval Using t-Distribution",
    "content": "The 95% confidence interval above assumes that $n$ is large enough and hence the sampling distribution of $\\bar{X}$ is approximately normal. When $n$ is small, realistically, we use the t-distribution instead. So the realistic confidence interval is calculated by the following formula: . $$ \\begin{equation} \\label{eq:ci-t} \\bar{X} \\pm t_{\\alpha/2, n-1} \\cdot \\frac{s}{\\sqrt{n}} \\end{equation} $$ . where $t_{\\alpha/2, \\nu}$ is the t-score such that . $$ Pr[-t_{\\alpha/2, \\nu} \\leq t \\leq t_{\\alpha/2, \\nu}] = 1 - \\alpha $$ . Setting $\\alpha = 0.05$ gives us the 95% confidence interval. We will discuss $\\alpha$ in more detail in the following sections. Narrowing the Confidence Interval . If we wanted to make our estimate more precise, we want to narrow down the confidence interval. If we take a look at equation $\\eqref{eq:ci-t}$, we notice that we can either: . | Increase the sample size $n$ | Decrease the sample standard deviation $s$ | . Increasing the sample size $n$ is not always realistic. Since it is $\\sqrt{n}$ in the denominator, in order to narrow down the confidence unit by $1/k$, we need to increase the sample size by $k^2$. Decreasing the sample standard deviation $s$ is also not always possible, because of the nature of the population. However, we can try to decrease the sample standard deviation $s$ by collecting our samples with a more precise measure. ",
    "url": "/docs/statistics/basics/inferencial-stats.html#confidence-interval-using-t-distribution",
    "relUrl": "/docs/statistics/basics/inferencial-stats.html#confidence-interval-using-t-distribution"
  },"516": {
    "doc": "Vue Project Setup",
    "title": "Vue Project Setup",
    "content": ". | Start project directory | Install Tailwind CSS . | Remove unused styles in production builds | Include Tailwind CSS | WindiCSS (optional) | . | Add path alias | Desktop App with Electron (Optional) | Install ESLint and Prettier (Optional) | . ",
    "url": "/docs/vue/init.html",
    "relUrl": "/docs/vue/init.html"
  },"517": {
    "doc": "Vue Project Setup",
    "title": "Start project directory",
    "content": "Details are listed here. yarn create vite &lt;app-name&gt; --template vue-ts # Init project cd &lt;app-name&gt; yarn # Install packages yarn dev # Check build . ",
    "url": "/docs/vue/init.html#start-project-directory",
    "relUrl": "/docs/vue/init.html#start-project-directory"
  },"518": {
    "doc": "Vue Project Setup",
    "title": "Install Tailwind CSS",
    "content": "Details are listed here. But for the brief summary: . yarn add tailwindcss@latest postcss@latest autoprefixer@latest --dev npx tailwindcss init -p . npx tailwind init -p generates two files tailwind.config.js and postcss.config.js. Remove unused styles in production builds . In tailwind.config.js, replace the purge to following line, . purge: ['./index.html', './src/**/*.{vue,js,ts,jsx,tsx}'] . Include Tailwind CSS . Create a file src/index.css. Then add the following to the file, . @tailwind base; @tailwind components; @tailwind utilities; . Then in src/main.js, import src/index.css. import { createApp } from 'vue' import App from './App.vue' import './index.css' createApp(App).mount('#app') . WindiCSS (optional) . Possible replacement for TailwindCSS. See installation here. ",
    "url": "/docs/vue/init.html#install-tailwind-css",
    "relUrl": "/docs/vue/init.html#install-tailwind-css"
  },"519": {
    "doc": "Vue Project Setup",
    "title": "Add path alias",
    "content": "Unlike webpack, Vite does not automatically provide the @ path alias to src. To enable this alias go to vite.config.ts and import path from 'path'. If import path from 'path' shows a type warning: yarn add @types/node --dev. Then add the following to defineConfig in vite.config.ts: . // vite.config.ts import path from 'path' export default defineConfig({ ..., resolve:{ alias: [ { find: '@', replacement: path.resolve(__dirname, './src') } ] } }) . ",
    "url": "/docs/vue/init.html#add-path-alias",
    "relUrl": "/docs/vue/init.html#add-path-alias"
  },"520": {
    "doc": "Vue Project Setup",
    "title": "Desktop App with Electron (Optional)",
    "content": "Really nice detail in this blog. Don’t forget to yarn add concurrently cross-end wait-on electron-buider --dev. They are needed to run the package.json scripts. ",
    "url": "/docs/vue/init.html#desktop-app-with-electron-optional",
    "relUrl": "/docs/vue/init.html#desktop-app-with-electron-optional"
  },"521": {
    "doc": "Vue Project Setup",
    "title": "Install ESLint and Prettier (Optional)",
    "content": "yarn add eslint prettier eslint-plugin-vue eslint-config-prettier --dev . Then create two files .eslintrc.js and .prettierrc.js in the project root directory, . // .eslintrc.js module.exports = { extends: [ 'plugin:vue/vue3-essential', 'prettier', ], rules: { // override/add rules settings here, such as: 'vue/no-unused-vars': 'error', }, } . // .prettierrc.js module.exports = { semi: false, tabWidth: 2, useTabs: false, printWidth: 80, endOfLine: 'auto', singleQuote: true, trailingComma: 'es5', bracketSpacing: true, arrowParens: 'always', } . ",
    "url": "/docs/vue/init.html#install-eslint-and-prettier-optional",
    "relUrl": "/docs/vue/init.html#install-eslint-and-prettier-optional"
  },"522": {
    "doc": "Basic Linux Setup",
    "title": "Basic Linux Setup",
    "content": "A note for myself. | Update packages | Install packages | Change to zsh | Color schemes | Little bit of customization . | GNOME Shell Integration | Dash to Panel | . | . ",
    "url": "/docs/linux/init.html",
    "relUrl": "/docs/linux/init.html"
  },"523": {
    "doc": "Basic Linux Setup",
    "title": "Update packages",
    "content": "sudo apt update &amp;&amp; sudo apt upgrade . ",
    "url": "/docs/linux/init.html#update-packages",
    "relUrl": "/docs/linux/init.html#update-packages"
  },"524": {
    "doc": "Basic Linux Setup",
    "title": "Install packages",
    "content": "List of packages I frequently use. To be added . sudo apt install &lt;package-name&gt; . | vim | net-tools | git | gnome-tweaks | htop | neofetch | nautilus | curl | tree | tmux | fasd | fzf | bat | fd | exa | . ",
    "url": "/docs/linux/init.html#install-packages",
    "relUrl": "/docs/linux/init.html#install-packages"
  },"525": {
    "doc": "Basic Linux Setup",
    "title": "Change to zsh",
    "content": "See here. ",
    "url": "/docs/linux/init.html#change-to-zsh",
    "relUrl": "/docs/linux/init.html#change-to-zsh"
  },"526": {
    "doc": "Basic Linux Setup",
    "title": "Color schemes",
    "content": "See here for details. First install the following, . sudo apt-get install dconf-cli uuid-runtime . Then, . bash -c \"$(curl -sLo- https://git.io/vQgMr)\" . Recommended color schemes are: . | Snazzy (174) | Tomorrow Night (204) | . ",
    "url": "/docs/linux/init.html#color-schemes",
    "relUrl": "/docs/linux/init.html#color-schemes"
  },"527": {
    "doc": "Basic Linux Setup",
    "title": "Little bit of customization",
    "content": "GNOME Shell Integration . Download the following extension from chrome. https://chrome.google.com/webstore/detail/gnome-shell-integration/gphhapmejobijbbhgpjhcjognlahblep . Dash to Panel . Download the following extension form chrome. https://extensions.gnome.org/extension/1160/dash-to-panel/ . ",
    "url": "/docs/linux/init.html#little-bit-of-customization",
    "relUrl": "/docs/linux/init.html#little-bit-of-customization"
  },"528": {
    "doc": "What is Time Series Analysis?",
    "title": "What is Time Series Analysis?",
    "content": ". | Time Series Analysis | Early Time Series Analysis | . ",
    "url": "/docs/data-science/time-series/intro-to-time-series.html",
    "relUrl": "/docs/data-science/time-series/intro-to-time-series.html"
  },"529": {
    "doc": "What is Time Series Analysis?",
    "title": "Time Series Analysis",
    "content": "Time series analysis can be defined as the following: . Extracting meaningful summary and statistic from points arranged in chronological order . Analysis goes both ways . | Diagnose past events | Predict future events | . Analysis of time series rose from various disciplines and applications: . | Medicine | Weather | Economics | Astronomy | . ECG and EEG are most common time series data in medicine. In the past, these fields faced challenges due to the scarcity of recordings and bias of data towards those with symptoms. However, with the advancement of wearable technology, it is now possible to collect routine measurements. Time series analysis have developed tremendously with the advancement of computing power. ",
    "url": "/docs/data-science/time-series/intro-to-time-series.html#time-series-analysis",
    "relUrl": "/docs/data-science/time-series/intro-to-time-series.html#time-series-analysis"
  },"530": {
    "doc": "What is Time Series Analysis?",
    "title": "Early Time Series Analysis",
    "content": "Traditional analysis models often presupposed its own outcome. For example, a cyclical model presupposes cyclical data. Which results in a lot of shortcomings. With the emergence of autoregressive models, time series analysis became more flexible. Unlike some other fields that are driven by theory, time series analysis was driven by practical applications. | Businesses led the field because they had more data than academics | Practical results were more important than underlying theory | . ",
    "url": "/docs/data-science/time-series/intro-to-time-series.html#early-time-series-analysis",
    "relUrl": "/docs/data-science/time-series/intro-to-time-series.html#early-time-series-analysis"
  },"531": {
    "doc": "Introduction to Stats",
    "title": "Introduction to Statistics for Data Analysis",
    "content": ". | Goals of Data Analysis . | Summarize the Data | Describe the Data . | Causality | Correlation | . | Predict the Data | . | Purpose of Statistics in Data Analysis | Types of Statistics . | Descriptive Statistics | Inferential Statistics . | Statistical Inference | Statistical Testing | . | . | . ",
    "url": "/docs/statistics/basics/intro.html#introduction-to-statistics-for-data-analysis",
    "relUrl": "/docs/statistics/basics/intro.html#introduction-to-statistics-for-data-analysis"
  },"532": {
    "doc": "Introduction to Stats",
    "title": "Goals of Data Analysis",
    "content": "There are three main goals of data analysis: . | Summarize the data | Describe the data | Predict the data | . ",
    "url": "/docs/statistics/basics/intro.html#goals-of-data-analysis",
    "relUrl": "/docs/statistics/basics/intro.html#goals-of-data-analysis"
  },"533": {
    "doc": "Introduction to Stats",
    "title": "Summarize the Data",
    "content": "An example of summarizing the data is to calculate the mean of a variable. ",
    "url": "/docs/statistics/basics/intro.html#summarize-the-data",
    "relUrl": "/docs/statistics/basics/intro.html#summarize-the-data"
  },"534": {
    "doc": "Introduction to Stats",
    "title": "Describe the Data",
    "content": "This is more like understanding the characteristics of the data and the relationships between variables. Questions like “Was this effective?” or “Is there a relationship between ~” fit here. In data analysis, there are two main types of relationships: causality and correlation. Causality . | Relationship of cause and effect: if-then | If you know the cause, you can not only predict the effect, but also control the outcome by intervening the cause. | e.g. If you take this treatment, you will get better. | . | . Correlation . | Relationship of tendency: when one is of a certain state, then the other tends to be of a certain state. | Different from causality in that changing one variable does not necessarily change the other. | e.g. People with higher income tend to be happier, but giving people more money does not necessarily make them happier since other factors come into play. | . | . ",
    "url": "/docs/statistics/basics/intro.html#describe-the-data",
    "relUrl": "/docs/statistics/basics/intro.html#describe-the-data"
  },"535": {
    "doc": "Introduction to Stats",
    "title": "Predict the Data",
    "content": "Based on data already collected, you predict the next data. ",
    "url": "/docs/statistics/basics/intro.html#predict-the-data",
    "relUrl": "/docs/statistics/basics/intro.html#predict-the-data"
  },"536": {
    "doc": "Introduction to Stats",
    "title": "Purpose of Statistics in Data Analysis",
    "content": "Dispersion is the degree of spread of the data. Higher the dispersion, the more difficult it is to really understand the data, because it bring uncertainty to the relationship between variables. Observations that abide the laws of Physics are relatively deterministic, hence have low dispersion. This uncertainty is the reason we need statistics. Probability is the language of uncertainty. ",
    "url": "/docs/statistics/basics/intro.html#purpose-of-statistics-in-data-analysis",
    "relUrl": "/docs/statistics/basics/intro.html#purpose-of-statistics-in-data-analysis"
  },"537": {
    "doc": "Introduction to Stats",
    "title": "Types of Statistics",
    "content": " ",
    "url": "/docs/statistics/basics/intro.html#types-of-statistics",
    "relUrl": "/docs/statistics/basics/intro.html#types-of-statistics"
  },"538": {
    "doc": "Introduction to Stats",
    "title": "Descriptive Statistics",
    "content": "Descriptive statistics is the type of statistics that summarizes and describes the data. The main focus is on understanding and explaining the data itself. ",
    "url": "/docs/statistics/basics/intro.html#descriptive-statistics",
    "relUrl": "/docs/statistics/basics/intro.html#descriptive-statistics"
  },"539": {
    "doc": "Introduction to Stats",
    "title": "Inferential Statistics",
    "content": "Inferential statistics aims to infer the characteristics of the source from which the data was collected. To predict the data, we need to know understand the source. One way to do this is to come up with a probability model of the source. There are two main types of inferential statistics: . | Statistical inference | Statistical testing | . Statistical Inference . e.g. Throw a coin 100 times and count the number of heads. Then we can infer the probability of getting heads. Statistical Testing . e.g. Hypothesize that the coin is fair. Then we can test the hypothesis by throwing the coin 100 times and see if the number of heads is close to 50. ",
    "url": "/docs/statistics/basics/intro.html#inferential-statistics",
    "relUrl": "/docs/statistics/basics/intro.html#inferential-statistics"
  },"540": {
    "doc": "Introduction to Stats",
    "title": "Introduction to Stats",
    "content": " ",
    "url": "/docs/statistics/basics/intro.html",
    "relUrl": "/docs/statistics/basics/intro.html"
  },"541": {
    "doc": "Invertible matrix",
    "title": "Invertible matrix",
    "content": ". | Inverse matrix . | Matrix of a system of linear equations . | Homogeneous system of linear equations | . | Singular matrix is not invertible | Properties of inverse matrix | . | How to find the inverse matrix . | Using determinant and adjugate matrix | Using elementary row operations | Using LU decomposition | . | . ",
    "url": "/docs/linalg/notes/invertible.html",
    "relUrl": "/docs/linalg/notes/invertible.html"
  },"542": {
    "doc": "Invertible matrix",
    "title": "Inverse matrix",
    "content": "Inverse matrix of a $n \\times n$ square matrix $A$ is denoted by $A^{-1}$ and defined as the matrix that satisfies the following equation: . $$ AA^{-1} = A^{-1}A = I_n $$ . If a matrix has an inverse matrix, it is called invertible. ",
    "url": "/docs/linalg/notes/invertible.html#inverse-matrix",
    "relUrl": "/docs/linalg/notes/invertible.html#inverse-matrix"
  },"543": {
    "doc": "Invertible matrix",
    "title": "Matrix of a system of linear equations",
    "content": "If a matrix $A$ is invertible, then the system of linear equations $Ax = b$ has a unique solution $x = A^{-1}b$. If a matrix is singular, then the system of linear equations $Ax = b$ has either no solution or infinitely many solutions. Homogeneous system of linear equations . For a homogeneous system of linear equations $Ax = 0$, if $A$ is invertible, then the only solution is $x = 0$. $x = 0$ in a homogeneous system of linear equations is called the trivial solution. ",
    "url": "/docs/linalg/notes/invertible.html#matrix-of-a-system-of-linear-equations",
    "relUrl": "/docs/linalg/notes/invertible.html#matrix-of-a-system-of-linear-equations"
  },"544": {
    "doc": "Invertible matrix",
    "title": "Singular matrix is not invertible",
    "content": "A matrix is singular if and only if its determinant is zero. If the determinant is zero, then the matrix is not invertible. See down below. You’ll see that the inverse matrix is undefined when the determinant is zero. By the same reasoning, a non-singular square matrix is invertible. ",
    "url": "/docs/linalg/notes/invertible.html#singular-matrix-is-not-invertible",
    "relUrl": "/docs/linalg/notes/invertible.html#singular-matrix-is-not-invertible"
  },"545": {
    "doc": "Invertible matrix",
    "title": "Properties of inverse matrix",
    "content": "For any invertible matrix $A$ and $B$ of the same size and scalar $c \\neq 0$, . | $(A^{-1})^{-1} = A$ | $AB$ is invertible and $(AB)^{-1} = B^{-1}A^{-1}$ | $(cA)^{-1} = \\frac{1}{c}A^{-1}$ | $A^k$ is invertible and $(A^k)^{-1} = (A^{-1})^k$ for $k &gt; 0 \\wedge k \\in \\mathbb{Z}$ | . Invertible Matrix Theorem For a square matrix $A \\in \\mathbb{R}^{n \\times n}$, the following statements are all equivalent: . | $A$ is invertible. | $A$ is non-singular. | $A$ is row equivalent to $I_n$. | $A$ in a row echelon form has $n$ pivots. | The system of linear equations $Ax = b$ has a unique solution for every $b \\in \\mathbb{R}^n$. | The homogeneous system of linear equations $Ax = 0$ has only the trivial solution. | The columns of $A$ are linearly independent. | The columns of $A$ span $\\mathbb{R}^n$. | $A^T$ is invertible. | . ",
    "url": "/docs/linalg/notes/invertible.html#properties-of-inverse-matrix",
    "relUrl": "/docs/linalg/notes/invertible.html#properties-of-inverse-matrix"
  },"546": {
    "doc": "Invertible matrix",
    "title": "How to find the inverse matrix",
    "content": " ",
    "url": "/docs/linalg/notes/invertible.html#how-to-find-the-inverse-matrix",
    "relUrl": "/docs/linalg/notes/invertible.html#how-to-find-the-inverse-matrix"
  },"547": {
    "doc": "Invertible matrix",
    "title": "Using determinant and adjugate matrix",
    "content": "If a square matrix $A$ is invertible, then the inverse matrix $A^{-1}$ can be found by . $$ A^{-1} = \\frac{1}{\\det(A)} \\operatorname{adj}(A) $$ . where $\\det(A)$ is the determinant of $A$ and $\\operatorname{adj}(A)$ is the adjugate matrix of $A$. ",
    "url": "/docs/linalg/notes/invertible.html#using-determinant-and-adjugate-matrix",
    "relUrl": "/docs/linalg/notes/invertible.html#using-determinant-and-adjugate-matrix"
  },"548": {
    "doc": "Invertible matrix",
    "title": "Using elementary row operations",
    "content": "If we can transform a matrix $A$ into an identity matrix $I_n$ using a sequence of elementary row operations, then the matrix multiplication of the corresponding elementary matrices is the inverse matrix $A^{-1}$: . $$ E_k \\cdots E_2 E_1 A = I_n \\Rightarrow A^{-1} = E_k \\cdots E_2 E_1 $$ . ",
    "url": "/docs/linalg/notes/invertible.html#using-elementary-row-operations",
    "relUrl": "/docs/linalg/notes/invertible.html#using-elementary-row-operations"
  },"549": {
    "doc": "Invertible matrix",
    "title": "Using LU decomposition",
    "content": "To be added . ",
    "url": "/docs/linalg/notes/invertible.html#using-lu-decomposition",
    "relUrl": "/docs/linalg/notes/invertible.html#using-lu-decomposition"
  },"550": {
    "doc": "jenv",
    "title": "jenv",
    "content": ". | Installation | Typical usage | Basic commands . | Add JDK | List all added JDKs | Set global JDK | Set JDK for current working directory | . | . ",
    "url": "/docs/java/jenv.html",
    "relUrl": "/docs/java/jenv.html"
  },"551": {
    "doc": "jenv",
    "title": "Installation",
    "content": "brew install jenv echo 'export PATH=\"$HOME/.jenv/bin:$PATH\"' &gt;&gt; ~/.zshrc echo 'eval \"$(jenv init -)\"' &gt;&gt; ~/.zshrc exec $SHELL -l jenv enable-plugin export exec $SHELL -l . ",
    "url": "/docs/java/jenv.html#installation",
    "relUrl": "/docs/java/jenv.html#installation"
  },"552": {
    "doc": "jenv",
    "title": "Typical usage",
    "content": "First install the JDK you want to use, . brew install --cask corretto11 . Then add it to jenv, . jenv rehash jenv add \"$(/usr/libexec/java_home)\" # Or if the following does not work jenv add /Library/Java/JavaVirtualMachines/amazon-corretto-11.jdk/Contents/Home . Check that JDK has been added by, . $ jenv versions * system 11.0 11.0.16.1 corretto64-11.0.16.1 . To set a JDK for current working directory, . jenv local 11.0 # OR 11.0.16.1 OR corretto64-11.0.16.1 . Confirm that JAVA_HOME has been set, . echo ${JAVA_HOME} . ",
    "url": "/docs/java/jenv.html#typical-usage",
    "relUrl": "/docs/java/jenv.html#typical-usage"
  },"553": {
    "doc": "jenv",
    "title": "Basic commands",
    "content": "jenv -h or jenv help &lt;command&gt; for more details. Add JDK . jenv add ${PATH_TO_JVM_HOME} . List all added JDKs . jenv versions . Set global JDK . jenv global ${version_name_from_jenv_versions} . Set JDK for current working directory . jenv local ${version_name_from_jenv_versions} . ",
    "url": "/docs/java/jenv.html#basic-commands",
    "relUrl": "/docs/java/jenv.html#basic-commands"
  },"554": {
    "doc": "Java Memory",
    "title": "Java Memory",
    "content": ". | Runtime Data Area | jmap . | Example usage | Errors | . | . ",
    "url": "/docs/java/jmap.html",
    "relUrl": "/docs/java/jmap.html"
  },"555": {
    "doc": "Java Memory",
    "title": "Runtime Data Area",
    "content": "To be added . ",
    "url": "/docs/java/jmap.html#runtime-data-area",
    "relUrl": "/docs/java/jmap.html#runtime-data-area"
  },"556": {
    "doc": "Java Memory",
    "title": "jmap",
    "content": "jmap is a CLI tool used to get memory-related information of a JVM. Example usage . jmap -heap $PID . Errors . If you get the following error, . Can't attach to the process: ptrace(PTRACE_ATTACH, ..) failed for $PID: Operation not permitted . Try the following, . echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope . If you get the following error, . cannot open binary file . Make sure you are running the command as the same user that started the JVM. ",
    "url": "/docs/java/jmap.html#jmap",
    "relUrl": "/docs/java/jmap.html#jmap"
  },"557": {
    "doc": "JS/TS Cheatsheet",
    "title": "Javascript/Typescript Cheatsheet",
    "content": ". | Installation | Typescript compiler | Basic typing . | Primitive types | Array and tuple types | Union and enum types | Object types and type alias | Function types | Literal types | . | Type assertions | Interface . | Object interface | Function interface | . | Undefined values . | Non-null assertions (!) | . | Index signature | Generics | Classes | Readonly arrays and tuples | Symbol type | Computed property names | Template strings | . ",
    "url": "/docs/vue/jsts.html#javascripttypescript-cheatsheet",
    "relUrl": "/docs/vue/jsts.html#javascripttypescript-cheatsheet"
  },"558": {
    "doc": "JS/TS Cheatsheet",
    "title": "Installation",
    "content": "I personally prefer to install typescript compiler per project directory. npm i typescript --save-dev # or -D # OR yarn add typescript --dev . ",
    "url": "/docs/vue/jsts.html#installation",
    "relUrl": "/docs/vue/jsts.html#installation"
  },"559": {
    "doc": "JS/TS Cheatsheet",
    "title": "Typescript compiler",
    "content": "Basic usage: . tsc &lt;filename&gt; # One-time compile tsc --watch &lt;filename&gt; # Livereloading . When specifiying the filename for tsc you may include or leave out the .ts extension. tsc --init . init creates the tsconfig.json file. With a tsconfig.json file, you can leave out the filename when runnig tsc. ",
    "url": "/docs/vue/jsts.html#typescript-compiler",
    "relUrl": "/docs/vue/jsts.html#typescript-compiler"
  },"560": {
    "doc": "JS/TS Cheatsheet",
    "title": "Basic typing",
    "content": "Primitive types . Primitives are all lowercase. let a: number = 1; let b: string = \"a\"; let c: boolean = true; // Lowercase let d: any = { x: 0 }; . Unless declared without initialization, these are usually inferred. Array and tuple types . Simply add brackets: . let a: number[] = [1, 2, 3, 4]; // You can also use Array&lt;number&gt; let b: [number, string] = [1, \"a\"]; let c: [number, string][] = [[1, \"a\"], [2, \"b\"]]; . Union and enum types . Union: . let id: string | number; . Enum: . enum MyEnum { Up = 1, // Default is 0 Down, Left, Right } . The first constant in an enum always has the value of 0. If you set it to 1, the rest will have an ascending value of 2, 3, and 4. You can also give string values to enums. Object types and type alias . Object typing without the type alias can be messy: . const obj: { a: number, b?: string, readonly c: boolean, } = { a: 1, c: true } . Using type: . type MyObj = { a: number, b?: string, readonly c: boolean }; const obj: MyObj = { a: 1, c: true } . The ? means it is an optional property or an optional parameter if used in functions. Function types . function f(x: number, y: string): void { ... } . Literal types . You can use this like a constant or quicky enum: . function f(x: number, y: \"a\" | \"b\"): -1 | 0 | 1 { ... } . To change an object to a literal type use as const: . const x = { a: \"hello\", b: \"world\" } as const . ",
    "url": "/docs/vue/jsts.html#basic-typing",
    "relUrl": "/docs/vue/jsts.html#basic-typing"
  },"561": {
    "doc": "JS/TS Cheatsheet",
    "title": "Type assertions",
    "content": "To give any variables explicit types: . let a: any = 1; let b = a as number // OR let c = &lt;number&gt;a . ",
    "url": "/docs/vue/jsts.html#type-assertions",
    "relUrl": "/docs/vue/jsts.html#type-assertions"
  },"562": {
    "doc": "JS/TS Cheatsheet",
    "title": "Interface",
    "content": "Object interface . interface MyInterface { a: number, b?: string } . Function interface . interface FuncInterface { (x: number, y: string): void } . While it is similar to type aliasing, there are some differences: . | You cannot use union types with an interface. | . type MyType = string | number; // OK // interface MyType2 = string | number; // NO . | You can add new fields to existing interfaces but not in type aliasing. | . interface MyInterface{ a: number } interface MyInterface{ b: string } . ",
    "url": "/docs/vue/jsts.html#interface",
    "relUrl": "/docs/vue/jsts.html#interface"
  },"563": {
    "doc": "JS/TS Cheatsheet",
    "title": "Undefined values",
    "content": "Use null or undefined. function f(x: number | null): void{ ... } . Non-null assertions (!) . someObj!.runFunction(); . ",
    "url": "/docs/vue/jsts.html#undefined-values",
    "relUrl": "/docs/vue/jsts.html#undefined-values"
  },"564": {
    "doc": "JS/TS Cheatsheet",
    "title": "Index signature",
    "content": "To be added . ",
    "url": "/docs/vue/jsts.html#index-signature",
    "relUrl": "/docs/vue/jsts.html#index-signature"
  },"565": {
    "doc": "JS/TS Cheatsheet",
    "title": "Generics",
    "content": "To be added . ",
    "url": "/docs/vue/jsts.html#generics",
    "relUrl": "/docs/vue/jsts.html#generics"
  },"566": {
    "doc": "JS/TS Cheatsheet",
    "title": "Classes",
    "content": "To be added . ",
    "url": "/docs/vue/jsts.html#classes",
    "relUrl": "/docs/vue/jsts.html#classes"
  },"567": {
    "doc": "JS/TS Cheatsheet",
    "title": "Readonly arrays and tuples",
    "content": "To be added . ",
    "url": "/docs/vue/jsts.html#readonly-arrays-and-tuples",
    "relUrl": "/docs/vue/jsts.html#readonly-arrays-and-tuples"
  },"568": {
    "doc": "JS/TS Cheatsheet",
    "title": "Symbol type",
    "content": "To be added . ",
    "url": "/docs/vue/jsts.html#symbol-type",
    "relUrl": "/docs/vue/jsts.html#symbol-type"
  },"569": {
    "doc": "JS/TS Cheatsheet",
    "title": "Computed property names",
    "content": "To be added . ",
    "url": "/docs/vue/jsts.html#computed-property-names",
    "relUrl": "/docs/vue/jsts.html#computed-property-names"
  },"570": {
    "doc": "JS/TS Cheatsheet",
    "title": "Template strings",
    "content": "let a = `Put ${variableName} here.` . ",
    "url": "/docs/vue/jsts.html#template-strings",
    "relUrl": "/docs/vue/jsts.html#template-strings"
  },"571": {
    "doc": "JS/TS Cheatsheet",
    "title": "JS/TS Cheatsheet",
    "content": " ",
    "url": "/docs/vue/jsts.html",
    "relUrl": "/docs/vue/jsts.html"
  },"572": {
    "doc": "Lagrange Multipliers",
    "title": "Method of Lagrange Multipliers",
    "content": "Popular method to solve constrained optimization problems. | Lagrange Multipliers . | Multiple constraints | . | Lagrangian | Lagrangian dual function | . ",
    "url": "/docs/statistics/notes/lagrangian.html#method-of-lagrange-multipliers",
    "relUrl": "/docs/statistics/notes/lagrangian.html#method-of-lagrange-multipliers"
  },"573": {
    "doc": "Lagrange Multipliers",
    "title": "Lagrange Multipliers",
    "content": "Optimization problem with constraints is often defined as follows: . | Objective function to be maximized or minimized | Constraint(s) that must be satisfied | . In the case of the figure above, the objective function is $f(x,y)$ and the constraint is $g(x,y) = c$. Very rough idea is as follows: . Out of the many level curves $f(x,y) = d$ can take, we want to pick the one with maximum or minimum $d$. Since we have a constraint, we can only expand or shrink the curve until it touches the constraint $g(x,y) = c$. There should be a point where both curves become tangent to each other. At this tangent, $f(x,y)$ is at a local extreme while satisfying the constraint. The gradient vector $\\nabla f(x,y)$ is orthogal to its tangent vector, and so is the gradient vector $\\nabla g(x,y)$ to its tangent vector. Although they may be in opposite directions and/or different magnitudes, both gradient vectors are aligned. In the figure above, they are the blue dotted line vector and the red line vector at the tangent point facing opposite directions. If we scale the gradient vector $\\nabla g(x,y)$ by a constant $\\lambda$, we can make it equal to the gradient vector $\\nabla f(x,y)$. $$ \\nabla f(x,y) = \\lambda \\nabla g(x,y) \\quad \\Rightarrow\\quad \\nabla f(x,y) - \\lambda \\nabla g(x,y) = 0 $$ . This constant $\\lambda$ is called the Lagrange multiplier, and $(x,y)$ that satisfies the above equation is the optimal solution. ",
    "url": "/docs/statistics/notes/lagrangian.html",
    "relUrl": "/docs/statistics/notes/lagrangian.html"
  },"574": {
    "doc": "Lagrange Multipliers",
    "title": "Multiple constraints",
    "content": "When there are multiple constraints, i.e. $g_i(x,y) = c_i$, we can extend the above equation as follows: . $$ \\nabla f(x,y) = \\sum_{i=1}^{m} \\lambda_i \\nabla g_i(x,y) \\quad \\Rightarrow\\quad \\nabla f(x,y) - \\sum_{i=1}^{m} \\lambda_i \\nabla g_i(x,y) = 0 $$ . where $m$ is the number of constraints. $\\lambda_i$ are the set of Lagrange multipliers. ",
    "url": "/docs/statistics/notes/lagrangian.html#multiple-constraints",
    "relUrl": "/docs/statistics/notes/lagrangian.html#multiple-constraints"
  },"575": {
    "doc": "Lagrange Multipliers",
    "title": "Lagrangian",
    "content": "You may be wondering where the actual constraint constant of each $g_i(x,y) = c_i$ went. Because that information is not captured in the equation above. In fact, the above system of equations does not provide enough information to solve for the optimal solution, because now we have added $m$ more unknowns $\\lambda_i$, while the gradient vectors $\\nabla f(x,y)$ and $\\nabla g_i(x,y)$ only provide enough for $x$ and $y$. We want to package the information of the constraints into the system of equations. The Lagrangian is the function that accomplishes this. $$ \\mathcal{L}(x,y,\\lambda) = f(x,y) - \\sum_{i=1}^{m} \\lambda_i (g_i(x,y) - c_i) $$ . To solve for the optimal solution, now you just need to solve: . $$ \\nabla \\mathcal{L}(x,y,\\lambda) = 0 $$ . \\[\\nabla \\mathcal{L}(x,y,\\lambda) = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial x} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial y} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda_1} \\\\ \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda_m} \\end{bmatrix} = \\vec{0}\\] . ",
    "url": "/docs/statistics/notes/lagrangian.html#lagrangian",
    "relUrl": "/docs/statistics/notes/lagrangian.html#lagrangian"
  },"576": {
    "doc": "Lagrange Multipliers",
    "title": "Lagrangian dual function",
    "content": "To be added . ",
    "url": "/docs/statistics/notes/lagrangian.html#lagrangian-dual-function",
    "relUrl": "/docs/statistics/notes/lagrangian.html#lagrangian-dual-function"
  },"577": {
    "doc": "Lecture 1 - Abstraction, Hello Python, Memory models",
    "title": "Lecture 1 - Abstraction, Hello Python, Memory models",
    "content": ". | Abstraction | Basic Python . | Primitive expressions | Types | Call expressions | Variables | . | Memory model | . ",
    "url": "/docs/bootcamp/computing/lecture1.html",
    "relUrl": "/docs/bootcamp/computing/lecture1.html"
  },"578": {
    "doc": "Lecture 1 - Abstraction, Hello Python, Memory models",
    "title": "Abstraction",
    "content": ". | Separation of interface and implementation | Keeping information relevant to context | . For example, in Python coding there are 3 levels of abstraction: Hardware &lt;- OS &lt;- Interpreter &lt;- Code. ",
    "url": "/docs/bootcamp/computing/lecture1.html#abstraction",
    "relUrl": "/docs/bootcamp/computing/lecture1.html#abstraction"
  },"579": {
    "doc": "Lecture 1 - Abstraction, Hello Python, Memory models",
    "title": "Basic Python",
    "content": " ",
    "url": "/docs/bootcamp/computing/lecture1.html#basic-python",
    "relUrl": "/docs/bootcamp/computing/lecture1.html#basic-python"
  },"580": {
    "doc": "Lecture 1 - Abstraction, Hello Python, Memory models",
    "title": "Primitive expressions",
    "content": "Primitive expression consists of: . | Operators: +, -, *, /, **, //, % | Operands . A single operand is also an expression. | . ",
    "url": "/docs/bootcamp/computing/lecture1.html#primitive-expressions",
    "relUrl": "/docs/bootcamp/computing/lecture1.html#primitive-expressions"
  },"581": {
    "doc": "Lecture 1 - Abstraction, Hello Python, Memory models",
    "title": "Types",
    "content": "A type in Python consists of: . | A set of values | A set of operators applicable to those values | . ",
    "url": "/docs/bootcamp/computing/lecture1.html#types",
    "relUrl": "/docs/bootcamp/computing/lecture1.html#types"
  },"582": {
    "doc": "Lecture 1 - Abstraction, Hello Python, Memory models",
    "title": "Call expressions",
    "content": ". | Functions are operators | A fuction call is a call expression | . ",
    "url": "/docs/bootcamp/computing/lecture1.html#call-expressions",
    "relUrl": "/docs/bootcamp/computing/lecture1.html#call-expressions"
  },"583": {
    "doc": "Lecture 1 - Abstraction, Hello Python, Memory models",
    "title": "Variables",
    "content": ". | Acceptable variable names . | Letters, digits, underscores | Cannot start with a digit | Case sensitive | No empty spaces | No keywords (reserved word) | . | Variable is created via assignment statement | . ",
    "url": "/docs/bootcamp/computing/lecture1.html#variables",
    "relUrl": "/docs/bootcamp/computing/lecture1.html#variables"
  },"584": {
    "doc": "Lecture 1 - Abstraction, Hello Python, Memory models",
    "title": "Memory model",
    "content": "A memory object consists of: . | Address | Value | . A variable is a pointer to a memory object. ",
    "url": "/docs/bootcamp/computing/lecture1.html#memory-model",
    "relUrl": "/docs/bootcamp/computing/lecture1.html#memory-model"
  },"585": {
    "doc": "Lecture 3 - Modules and Classes",
    "title": "Lecture 3 - Modules and Classes",
    "content": ". | Module . | Dot operator | Specific import caveats | . | Class . | Methods | . | . ",
    "url": "/docs/bootcamp/computing/lecture3.html",
    "relUrl": "/docs/bootcamp/computing/lecture3.html"
  },"586": {
    "doc": "Lecture 3 - Modules and Classes",
    "title": "Module",
    "content": "A package of reusable variables and functions grouped together. import math import math as my_math from math import pi . Importing a single function does not save memory compared to importing the whole module. The entire module is still loaded into memory anyways. Only difference is namespace mapping. ",
    "url": "/docs/bootcamp/computing/lecture3.html#module",
    "relUrl": "/docs/bootcamp/computing/lecture3.html#module"
  },"587": {
    "doc": "Lecture 3 - Modules and Classes",
    "title": "Dot operator",
    "content": "Access variables and functions in a module with the dot operator .. math.pi . ",
    "url": "/docs/bootcamp/computing/lecture3.html#dot-operator",
    "relUrl": "/docs/bootcamp/computing/lecture3.html#dot-operator"
  },"588": {
    "doc": "Lecture 3 - Modules and Classes",
    "title": "Specific import caveats",
    "content": "from english import hello from french import hello . hello is now mapped to the last french import because the name is overwritten. So do not use from math import * because it pollutes the namespace. ",
    "url": "/docs/bootcamp/computing/lecture3.html#specific-import-caveats",
    "relUrl": "/docs/bootcamp/computing/lecture3.html#specific-import-caveats"
  },"589": {
    "doc": "Lecture 3 - Modules and Classes",
    "title": "Class",
    "content": "A class is an object that contains variables and functions. | Types are classes | help(int) shows documentation for class int | Is an example of abstraction (in the sense that underlying implementation can be unknown to the user) | OOP because class has state (variables) and behavior (functions) | . ",
    "url": "/docs/bootcamp/computing/lecture3.html#class",
    "relUrl": "/docs/bootcamp/computing/lecture3.html#class"
  },"590": {
    "doc": "Lecture 3 - Modules and Classes",
    "title": "Methods",
    "content": "Functions inside a class are called methods. A method takes the class instance (class object) self as the first argument: . class MyClass: def my_method(self): print(\"Hello world!\") . ",
    "url": "/docs/bootcamp/computing/lecture3.html#methods",
    "relUrl": "/docs/bootcamp/computing/lecture3.html#methods"
  },"591": {
    "doc": "Lecture 4 - Lists and Loops",
    "title": "Lecture 4 - Lists and Loops",
    "content": ". | List . | Type hinting | Basic operations | Basic methods | Slicing | Copy and Alias . | Copy | Alias | . | . | Loop . | for loop | while loop | . | . ",
    "url": "/docs/bootcamp/computing/lecture4.html",
    "relUrl": "/docs/bootcamp/computing/lecture4.html"
  },"592": {
    "doc": "Lecture 4 - Lists and Loops",
    "title": "List",
    "content": ". | Ordered | Mutable container with mutable elements | Heterogeneous elements allowed | . However, it is error-prone to have heterogeneous elements in a list. Not recommended. a = [1, 2, 3] a[0] # 1 a[0] = 4 a # [4, 2, 3] . ",
    "url": "/docs/bootcamp/computing/lecture4.html#list",
    "relUrl": "/docs/bootcamp/computing/lecture4.html#list"
  },"593": {
    "doc": "Lecture 4 - Lists and Loops",
    "title": "Type hinting",
    "content": "from typing import List def f(a: List[int]) -&gt; int: ... ",
    "url": "/docs/bootcamp/computing/lecture4.html#type-hinting",
    "relUrl": "/docs/bootcamp/computing/lecture4.html#type-hinting"
  },"594": {
    "doc": "Lecture 4 - Lists and Loops",
    "title": "Basic operations",
    "content": ". | len(a) | max(a) | min(a) | sum(a) | sorted(a) | reversed(a) | a1 + a2 | a * 3 | del a[0] | val in a | . ",
    "url": "/docs/bootcamp/computing/lecture4.html#basic-operations",
    "relUrl": "/docs/bootcamp/computing/lecture4.html#basic-operations"
  },"595": {
    "doc": "Lecture 4 - Lists and Loops",
    "title": "Basic methods",
    "content": ". | a.append(val) | a.insert(idx, val) | a.remove(val) | a.clear() | a.pop() | a.pop(idx) | a.index(val) | a.count(val) | a.sort() | a.reverse() | . ",
    "url": "/docs/bootcamp/computing/lecture4.html#basic-methods",
    "relUrl": "/docs/bootcamp/computing/lecture4.html#basic-methods"
  },"596": {
    "doc": "Lecture 4 - Lists and Loops",
    "title": "Slicing",
    "content": "a[start:stop:step] where start is inclusive and stop is exclusive. ",
    "url": "/docs/bootcamp/computing/lecture4.html#slicing",
    "relUrl": "/docs/bootcamp/computing/lecture4.html#slicing"
  },"597": {
    "doc": "Lecture 4 - Lists and Loops",
    "title": "Copy and Alias",
    "content": "Copy . c = a.copy() or c = a[:] . As long as the elements are immutable, modifying c will not modify a. Alias . c = a . Modifying c will also modify a. List passed as argument to a function becomes an alias. Modifying the parameter list will also modify the original list. ",
    "url": "/docs/bootcamp/computing/lecture4.html#copy-and-alias",
    "relUrl": "/docs/bootcamp/computing/lecture4.html#copy-and-alias"
  },"598": {
    "doc": "Lecture 4 - Lists and Loops",
    "title": "Loop",
    "content": " ",
    "url": "/docs/bootcamp/computing/lecture4.html#loop",
    "relUrl": "/docs/bootcamp/computing/lecture4.html#loop"
  },"599": {
    "doc": "Lecture 4 - Lists and Loops",
    "title": "for loop",
    "content": "for x in range(1, 10, 2): ... a can be a list, string, tuple, dictionary, or any iterable object. For dictionary, x will be the key. If you want to use both key and value, use for k, v in d.items(). ",
    "url": "/docs/bootcamp/computing/lecture4.html#for-loop",
    "relUrl": "/docs/bootcamp/computing/lecture4.html#for-loop"
  },"600": {
    "doc": "Lecture 4 - Lists and Loops",
    "title": "while loop",
    "content": "while True: ... | Watch out for infinite loop | Condition should be updated inside the loop | Condition should not be too specific | break and continue can be used (useful but can make code harder to read) | . ",
    "url": "/docs/bootcamp/computing/lecture4.html#while-loop",
    "relUrl": "/docs/bootcamp/computing/lecture4.html#while-loop"
  },"601": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "content": ". | Set . | Basic set methods | . | Tuple | Dictionary . | Looping over dictionary | Basic methods | . | Mutability . | Mutable container | Mutable element | . | Quick summary on order and mutability | . ",
    "url": "/docs/bootcamp/computing/lecture5.html",
    "relUrl": "/docs/bootcamp/computing/lecture5.html"
  },"602": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Set",
    "content": ". | Unordered | Unique elements | Mutable container with immutable elements | Hetereogeneous elements allowed | . Set itself is mutable, but the elements in the set must be immutable because membership check is done by hashing. s = {1, 2, 3, 4, 5} s = set(\"hello\") # {'e', 'h', 'l', 'o'} s = set() # empty set (not {}, which is empty dictionary) . ",
    "url": "/docs/bootcamp/computing/lecture5.html#set",
    "relUrl": "/docs/bootcamp/computing/lecture5.html#set"
  },"603": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Basic set methods",
    "content": ". | s.add(x) | s.remove(x) | s.clear() | s.issubset(t) | s.issuperset(t) | s.union(t) | s.intersection(t) | s.difference(t) | s.symmetric_difference(t) | . ",
    "url": "/docs/bootcamp/computing/lecture5.html#basic-set-methods",
    "relUrl": "/docs/bootcamp/computing/lecture5.html#basic-set-methods"
  },"604": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Tuple",
    "content": ". | Ordered | Immutable container with mutable elements | . x = () x = (3,) # singleton tuple: , is required to differentiate from arithmetic (3) x[0] # 3 . ",
    "url": "/docs/bootcamp/computing/lecture5.html#tuple",
    "relUrl": "/docs/bootcamp/computing/lecture5.html#tuple"
  },"605": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Dictionary",
    "content": ". | Ordered (Python 3.7+) by insertion order | Mutable container with mutable elements | Key-value pair mapping | . Key must be immutable because membership check is done by hashing. d = {'a': 1, 'b': 2, 'c': 3} d = dict(a=1, b=2, c=3) d = dict([('a', 1), ('b', 2), ('c', 3)]) d = dict(zip(['a', 'b', 'c'], [1, 2, 3])) d = dict.fromkeys(['a', 'b', 'c'], 0) # {'a': 0, 'b': 0, 'c': 0} d = {} # empty dictionary . ",
    "url": "/docs/bootcamp/computing/lecture5.html#dictionary",
    "relUrl": "/docs/bootcamp/computing/lecture5.html#dictionary"
  },"606": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Looping over dictionary",
    "content": "Simple iteration over dictionary will iterate over keys. To iterate over values, use d.values(). To iterate over key-value pairs, use d.items(). ",
    "url": "/docs/bootcamp/computing/lecture5.html#looping-over-dictionary",
    "relUrl": "/docs/bootcamp/computing/lecture5.html#looping-over-dictionary"
  },"607": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Basic methods",
    "content": ". | d[k] = v | del d[k] | k in d | d.clear() | d.keys() | d.values() | d.items() | d.get(k) | d.get(k, default): if k is not in d, return default | d.pop(k) | d.pop(k, default): if k is not in d, return default | d.setdefault(k): if k is not in d, set d[k] = None | d.setdefault(k, default): if k is not in d, set d[k] = default | d.update(t): update d with key-value pairs from t | . ",
    "url": "/docs/bootcamp/computing/lecture5.html#basic-methods",
    "relUrl": "/docs/bootcamp/computing/lecture5.html#basic-methods"
  },"608": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Mutability",
    "content": " ",
    "url": "/docs/bootcamp/computing/lecture5.html#mutability",
    "relUrl": "/docs/bootcamp/computing/lecture5.html#mutability"
  },"609": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Mutable container",
    "content": "Can add, remove, or change the address that each element points to. ",
    "url": "/docs/bootcamp/computing/lecture5.html#mutable-container",
    "relUrl": "/docs/bootcamp/computing/lecture5.html#mutable-container"
  },"610": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Mutable element",
    "content": "Can add, remove, or change the value of the element. ",
    "url": "/docs/bootcamp/computing/lecture5.html#mutable-element",
    "relUrl": "/docs/bootcamp/computing/lecture5.html#mutable-element"
  },"611": {
    "doc": "Lecture 5 - Sets, Tuples, Dictionaries and Mutability",
    "title": "Quick summary on order and mutability",
    "content": "| Type | Ordered | Container | Element | . | List | Ordered | Mutable | Mutable | . | Set | Unordered | Mutable | Immutable | . | Tuple | Ordered | Immutable | Mutable | . | Dictionary | (Python 3.7+) Ordered | Mutable | (K) Immutable, (V) Mutable | . ",
    "url": "/docs/bootcamp/computing/lecture5.html#quick-summary-on-order-and-mutability",
    "relUrl": "/docs/bootcamp/computing/lecture5.html#quick-summary-on-order-and-mutability"
  },"612": {
    "doc": "Lecture 6 - File I/O",
    "title": "Lecture 6 - File I/O",
    "content": ". | Basic file opening . | with context manager | . | File path | Basic file reading . | Downloading a file from the Internet | . | Basic file writing . | Reading and writing together | . | Reading techniques | . ",
    "url": "/docs/bootcamp/computing/lecture6.html",
    "relUrl": "/docs/bootcamp/computing/lecture6.html"
  },"613": {
    "doc": "Lecture 6 - File I/O",
    "title": "Basic file opening",
    "content": "Most basic form: . f = open(\"filename.txt\", \"r\") # `r` for read, `w` for write, `a` for append content = f.read() f.close() . ",
    "url": "/docs/bootcamp/computing/lecture6.html#basic-file-opening",
    "relUrl": "/docs/bootcamp/computing/lecture6.html#basic-file-opening"
  },"614": {
    "doc": "Lecture 6 - File I/O",
    "title": "with context manager",
    "content": "with open(\"filename.txt\", \"r\") as f: content = f.read() . f object can be type hinted with from typing import TextIO. To make file processing functions more general, we typically open the file (create reader/writer objects) outside the function, and pass it as an argument to the function. ",
    "url": "/docs/bootcamp/computing/lecture6.html#with-context-manager",
    "relUrl": "/docs/bootcamp/computing/lecture6.html#with-context-manager"
  },"615": {
    "doc": "Lecture 6 - File I/O",
    "title": "File path",
    "content": ". | Absolute path: /Users/username/Documents/filename.txt | Relative path: filename.txt, ../filename.txt | . Useful modules: . | import os | from pathlib import Path | . ",
    "url": "/docs/bootcamp/computing/lecture6.html#file-path",
    "relUrl": "/docs/bootcamp/computing/lecture6.html#file-path"
  },"616": {
    "doc": "Lecture 6 - File I/O",
    "title": "Basic file reading",
    "content": "The cursor moves forward as you read the file. If you want to read the file again, you need to close and reopen the file. | f.read(): read the entire file as a string | f.read(n): read the next n characters as a string | f.readline(): read the next line as a string | f.readlines(): read the entire file as a list of lines . | Whitespaces like \\n are preserved | Use line.strip() to remove whitespaces | . | . ",
    "url": "/docs/bootcamp/computing/lecture6.html#basic-file-reading",
    "relUrl": "/docs/bootcamp/computing/lecture6.html#basic-file-reading"
  },"617": {
    "doc": "Lecture 6 - File I/O",
    "title": "Downloading a file from the Internet",
    "content": "import urllib.request url = \"https://...\" with urllib.request.urlopen(url) as f: for line in file: line = line.strip().decode(\"utf-8\") print(line) . Unknown file types are read as bytes. ",
    "url": "/docs/bootcamp/computing/lecture6.html#downloading-a-file-from-the-internet",
    "relUrl": "/docs/bootcamp/computing/lecture6.html#downloading-a-file-from-the-internet"
  },"618": {
    "doc": "Lecture 6 - File I/O",
    "title": "Basic file writing",
    "content": ". | w: open as write mode, overwrite the file | a: open as append mode, append to the file | . with open(\"filename.txt\", \"w\") as f: f.write(\"Hello world!\") . For newline, you need to manually add \\n. ",
    "url": "/docs/bootcamp/computing/lecture6.html#basic-file-writing",
    "relUrl": "/docs/bootcamp/computing/lecture6.html#basic-file-writing"
  },"619": {
    "doc": "Lecture 6 - File I/O",
    "title": "Reading and writing together",
    "content": "with open(\"input.txt\", \"r\") as input_file, open(\"output.txt\", \"w\") as output_file: for line in input_file: output_file.write(line) . Notice that input_file and output_file are independent. ",
    "url": "/docs/bootcamp/computing/lecture6.html#reading-and-writing-together",
    "relUrl": "/docs/bootcamp/computing/lecture6.html#reading-and-writing-together"
  },"620": {
    "doc": "Lecture 6 - File I/O",
    "title": "Reading techniques",
    "content": ". | Know the delimeters | Know the structure of the file | Know the encoding of the file | Skip the header and footer | Handle missing values and typos | . ",
    "url": "/docs/bootcamp/computing/lecture6.html#reading-techniques",
    "relUrl": "/docs/bootcamp/computing/lecture6.html#reading-techniques"
  },"621": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "Lecture 7 - Object-Oriented Programming",
    "content": ". | Procedural vs. Object-Oriented Programming . | Spaghetti Code | . | 4 Principles of OOP . | Encapsulation | Abstraction | Inheritance | Polymorphism | . | OOP with Python Classes . | Basic class usage | type of a subclass | isinstance | issubclass | . | . ",
    "url": "/docs/bootcamp/computing/lecture7.html",
    "relUrl": "/docs/bootcamp/computing/lecture7.html"
  },"622": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "Procedural vs. Object-Oriented Programming",
    "content": "In procedural programming, variables, data structures, and functions are the main components. On the other hand, in object-oriented programming, classes and objects are the main components. OOP is all about encapsulating data and functions into objects. ",
    "url": "/docs/bootcamp/computing/lecture7.html#procedural-vs-object-oriented-programming",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#procedural-vs-object-oriented-programming"
  },"623": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "Spaghetti Code",
    "content": "OOP can help to avoid spaghetti code, which is a program that is difficult to read and maintain because the codes are way too interdependent. OOP aims to make the code more modular and reusable. If something goes wrong, you just need to find the problem in the object, instead of searching through the entire code. ",
    "url": "/docs/bootcamp/computing/lecture7.html#spaghetti-code",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#spaghetti-code"
  },"624": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "4 Principles of OOP",
    "content": " ",
    "url": "/docs/bootcamp/computing/lecture7.html#4-principles-of-oop",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#4-principles-of-oop"
  },"625": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "Encapsulation",
    "content": "Relate information and behavior into a single entity. Makes code cleaner and easier to maintain. ",
    "url": "/docs/bootcamp/computing/lecture7.html#encapsulation",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#encapsulation"
  },"626": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "Abstraction",
    "content": "Hiding implementation details from the user by providing a simple interface. Abstraction makes changes easier because it isolates impact of changes from other code. ",
    "url": "/docs/bootcamp/computing/lecture7.html#abstraction",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#abstraction"
  },"627": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "Inheritance",
    "content": "Inheritance allows a class to inherit properties and methods from its superclass. It makes code reusable and eliminates redundant code. ",
    "url": "/docs/bootcamp/computing/lecture7.html#inheritance",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#inheritance"
  },"628": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "Polymorphism",
    "content": "Allow generic functions that generalize a certain behavior and allow subclasses to override that behavior with more specific behavior. Essentially: same function name, different behavior. It can also get rid of excessive conditional statements. ",
    "url": "/docs/bootcamp/computing/lecture7.html#polymorphism",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#polymorphism"
  },"629": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "OOP with Python Classes",
    "content": " ",
    "url": "/docs/bootcamp/computing/lecture7.html#oop-with-python-classes",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#oop-with-python-classes"
  },"630": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "Basic class usage",
    "content": "class Member: def __init__(self, name: str, email: str, dob: str, affiliation: str) -&gt; None: self.name = name self.email = email self.dob = dob self.affiliation = affiliation def print_info(self) -&gt; None: print(f\"Name: {self.name}\") print(f\"Email: {self.email}\") print(f\"Date of birth: {self.dob}\") print(f\"Affiliation: {self.affiliation}\") class Student(Member): def __init__( self, name: str, email: str, dob: str, affiliation: str, student_id: str ) -&gt; None: super().__init__(name, email, dob, affiliation) self.student_id = student_id def print_info(self) -&gt; None: super().print_info() print(f\"Student ID: {self.student_id}\") . ",
    "url": "/docs/bootcamp/computing/lecture7.html#basic-class-usage",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#basic-class-usage"
  },"631": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "type of a subclass",
    "content": "student = Student(\"John Doe\", ...) type(student) # __main__.Student . ",
    "url": "/docs/bootcamp/computing/lecture7.html#type-of-a-subclass",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#type-of-a-subclass"
  },"632": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "isinstance",
    "content": "isinstance(student, Student) # True . ",
    "url": "/docs/bootcamp/computing/lecture7.html#isinstance",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#isinstance"
  },"633": {
    "doc": "Lecture 7 - Object-Oriented Programming",
    "title": "issubclass",
    "content": "issubclass(Student, Member) # True . ",
    "url": "/docs/bootcamp/computing/lecture7.html#issubclass",
    "relUrl": "/docs/bootcamp/computing/lecture7.html#issubclass"
  },"634": {
    "doc": "Lecture 8 - Search, Sort, and Big-O",
    "title": "Lecture 8 - Search, Sort, and Big-O",
    "content": ". | Search . | Linear search . | while loop | while loop with sentinel | for loop | . | Binary search | . | Sorting . | Selection sort | Insertion sort | . | Measuring time performanace with code | Big-O . | Time complexity analysis | . | . ",
    "url": "/docs/bootcamp/computing/lecture8.html",
    "relUrl": "/docs/bootcamp/computing/lecture8.html"
  },"635": {
    "doc": "Lecture 8 - Search, Sort, and Big-O",
    "title": "Search",
    "content": ". | If the target exists, return the index of the target. | If the target does not exist, return -1. | . ",
    "url": "/docs/bootcamp/computing/lecture8.html#search",
    "relUrl": "/docs/bootcamp/computing/lecture8.html#search"
  },"636": {
    "doc": "Lecture 8 - Search, Sort, and Big-O",
    "title": "Linear search",
    "content": ". | Most basic sequential search algorithm: $O(n)$. | Finds the first occurrence of the target. | . There can be many ways to implement (program) a single algorithm: . while loop . This version iterates through the entire array even if the target is found early. Code def lin_search1(L: List, v: int) -&gt; int: i, n = 0, len(L) while i &lt; n and L[i] != v: i += 1 if i == n: return -1 else: return i . while loop with sentinel . You could add target as a sentinel at the end of the array to avoid the additional if statement in the while loop that checks if the index is out of range. However, this would require modifying the original input array which is an error-prone practice. Keeping it as a reference, but I don’t really see the beauty of this approach. Code def lin_search_sentinel(L: List, v: int) -&gt; int: i, n = 0, len(L) L.append(v) while L[i] != v: i += 1 L.pop() if i == n: return -1 else: return i . for loop . Stops iterating when the target is found. Code def lin_search_for(L: List, v: int) -&gt; int: for i in range(len(L)): if L[i] == v: return i return -1 . In terms of linear search in Python, the built-in list.index(x) method is the fastest $O(n)$ search because it is implemented in C. ",
    "url": "/docs/bootcamp/computing/lecture8.html#linear-search",
    "relUrl": "/docs/bootcamp/computing/lecture8.html#linear-search"
  },"637": {
    "doc": "Lecture 8 - Search, Sort, and Big-O",
    "title": "Binary search",
    "content": ". | Requires a sorted array: $O(\\log n)$. | mid = (start + end) // 2 | Compare arr[mid] with target and update start = mid + 1 and end = mid - 1 accordingly. | . The real issue is the sorting complexity. Code def binary_search(L: List, v: int) -&gt; int: start, end = 0, len(L) - 1 while start &lt;= end: mid = (start + end) // 2 cur = L[mid] if cur == v: return mid elif cur &lt; v: start = mid + 1 elif cur &gt; v: end = mid - 1 return -1 . ",
    "url": "/docs/bootcamp/computing/lecture8.html#binary-search",
    "relUrl": "/docs/bootcamp/computing/lecture8.html#binary-search"
  },"638": {
    "doc": "Lecture 8 - Search, Sort, and Big-O",
    "title": "Sorting",
    "content": " ",
    "url": "/docs/bootcamp/computing/lecture8.html#sorting",
    "relUrl": "/docs/bootcamp/computing/lecture8.html#sorting"
  },"639": {
    "doc": "Lecture 8 - Search, Sort, and Big-O",
    "title": "Selection sort",
    "content": ". | Minimum selection sort on unordered list: $O(n^2)$. | Smallest element is selected and swapped with the first element. | Selection takes the most effort | . The following code is an non-destructive implementation of selection sort. Code def selection_sorted(L: List) -&gt; List: arr = L.copy() for i in range(len(arr)): min_idx = i for j in range(i + 1, len(arr)): if arr[j] &lt; arr[min_idx]: min_idx = j arr[i], arr[min_idx] = arr[min_idx], arr[i] return arr . Why $O(n^2)$? . | At $n$ iterations of the outer loop. | $n - 1$ iterations of the inner loop. | . | . Also Triangular numbers: $\\sum_{i=1}^n i = \\frac{n(n+1)}{2}$. ",
    "url": "/docs/bootcamp/computing/lecture8.html#selection-sort",
    "relUrl": "/docs/bootcamp/computing/lecture8.html#selection-sort"
  },"640": {
    "doc": "Lecture 8 - Search, Sort, and Big-O",
    "title": "Insertion sort",
    "content": ". | Insertion sort on unordered list: $O(n^2)$. | Take the leftmost (unsorted) element and figure out where it belongs in the sorted list. | Insertion takes the most effort. | . The basic idea is: . | The first element is considered already sorted. | Then the original list is theoretically split into two parts: . | Sorted part on the left. | Unsorted part on the right. | . | The first element of the unsorted part is inserted into the sorted part . | Gradually compare and swap this first unsorted element with the last element of the sorted part, and move towards the left until the correct position is found. | . | . The following code is an non-destructive implementation of insertion sort. Code def insertion_sorted(L: List) -&gt; List: arr = L.copy() for i in range(1, len(arr)): for j in range(i, 0, -1): if arr[j] &lt; arr[j - 1]: arr[j], arr[j - 1] = arr[j - 1], arr[j] else: break return arr . Although both selection sort and insertion sort are $O(n^2)$, selection sort has higher lookup/comparison cost (you have to look at all the elements in the unsorted part) and lower swap cost (only once per iteration), while insertion sort has lower expected lookup/comparison cost (if you’re lucky, you only have to look at some of the elements in the sorted part) and higher swap cost (gotta swap with everything on the left). In the real world, the swap cost is usually higher than the lookup cost, so insertion sort can be a bit slower than selection sort. Insertion sort shines when the input is almost sorted. Because a lot of the swaps can be skipped, it reduces to $O(n)$. That is, there is a high probability that I won’t have to swap until the left end. ",
    "url": "/docs/bootcamp/computing/lecture8.html#insertion-sort",
    "relUrl": "/docs/bootcamp/computing/lecture8.html#insertion-sort"
  },"641": {
    "doc": "Lecture 8 - Search, Sort, and Big-O",
    "title": "Measuring time performanace with code",
    "content": "import time start = time.perf_counter() # float value of time in seconds # Code to be timed end = time.perf_counter() print(f\"Time elapsed: {end - start:0.4f} seconds\") . ",
    "url": "/docs/bootcamp/computing/lecture8.html#measuring-time-performanace-with-code",
    "relUrl": "/docs/bootcamp/computing/lecture8.html#measuring-time-performanace-with-code"
  },"642": {
    "doc": "Lecture 8 - Search, Sort, and Big-O",
    "title": "Big-O",
    "content": "Types of programming cost: . | Execution cost (focus of algorithm complexity analysis) . | Time complexity | Space complexity | . | Programming (development) cost (in the real world) . | Readability | Maintainability | Extensibility | Reusability | … | . | . ",
    "url": "/docs/bootcamp/computing/lecture8.html#big-o",
    "relUrl": "/docs/bootcamp/computing/lecture8.html#big-o"
  },"643": {
    "doc": "Lecture 8 - Search, Sort, and Big-O",
    "title": "Time complexity analysis",
    "content": ". | Measuring via code execution time is not a good idea. | It depends on the hardware, implementation, and input. | It could require a lot of time to run. | . | . So analyze in terms of worst-case number of operations on large input size $n$. Complexity . ",
    "url": "/docs/bootcamp/computing/lecture8.html#time-complexity-analysis",
    "relUrl": "/docs/bootcamp/computing/lecture8.html#time-complexity-analysis"
  },"644": {
    "doc": "Linear Regression",
    "title": "Linear Regression",
    "content": ". | What is linear regression? | Simple linear regression | Multiple linear regression . | Downside of having too many features | Feature selection methods . | Backward elimination | Forward selection | Bidirectional elimination | . | . | Polynomial regression | When is it adequate to use linear regression? . | Linearity | No multicollinearity | Homoscedasticity of the residuals | No autocorrelation of the residuals | Normality of the residuals | No outliers | . | . ",
    "url": "/docs/statistics/notes/linear-regression.html",
    "relUrl": "/docs/statistics/notes/linear-regression.html"
  },"645": {
    "doc": "Linear Regression",
    "title": "What is linear regression?",
    "content": "In statistical estimation, linear regression models the relationship between a dependent variable $y$ and one or more independent variables $x_i$ as a linear transformation of the independent variables plus some error/noise $\\varepsilon$. In general it models the relationship with the following equation: . | $y \\in \\mathbb{R}^{n \\times 1}$ is the dependent variable | $X \\in \\mathbb{R}^{n \\times p}$ is the matrix of independent variables consisting of $n$ observations and $p$ features | $\\beta \\in \\mathbb{R}^{p \\times 1}$ is the vector of coefficients | $\\varepsilon \\in \\mathbb{R}^{n \\times 1}$ is the error term | . To be more precise... During actual estimation: . | $X$ is a matrix of dimension $(n, p+1)$ with the first column consisting of 1s (for the intercept term) | . \\[X = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{bmatrix}\\] . | $\\beta$ is a vector of dimension $(p+1, 1)$ with the first element being the intercept term $\\beta_0$ | . \\[\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\\] Just so that the matrix multiplication $X \\beta$ produces: . $$ y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i $$ . $$ y = X \\beta + \\varepsilon $$ . For a linear regression model, $\\beta$ is the model parameter that we try to learn. Therefore, the prediction of the model is: . $$ \\hat{y} = X \\beta $$ . The model fitting process consists of estimating the parameter matrix $\\beta$ such that the error term $\\varepsilon$ is minimized. The most common method used for $\\varepsilon$ minimization is the ordinary least squares (OLS) method. ",
    "url": "/docs/statistics/notes/linear-regression.html#what-is-linear-regression",
    "relUrl": "/docs/statistics/notes/linear-regression.html#what-is-linear-regression"
  },"646": {
    "doc": "Linear Regression",
    "title": "Simple linear regression",
    "content": "Simple linear regression is a linear regression model with a single independent variable. In other words, the number of features $p$ is equal to 1, so the feature matrix $X$ is a just a column vector. $$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i $$ . Entire process is equivalent to finding a best-fit line in a 2D graph. ",
    "url": "/docs/statistics/notes/linear-regression.html#simple-linear-regression",
    "relUrl": "/docs/statistics/notes/linear-regression.html#simple-linear-regression"
  },"647": {
    "doc": "Linear Regression",
    "title": "Multiple linear regression",
    "content": "Multiple linear regression is a linear regression model with multiple independent variables. $$ y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i $$ . Do not confuse this with multivariate linear regression, which is a linear regression model with multiple dependent variables. ",
    "url": "/docs/statistics/notes/linear-regression.html#multiple-linear-regression",
    "relUrl": "/docs/statistics/notes/linear-regression.html#multiple-linear-regression"
  },"648": {
    "doc": "Linear Regression",
    "title": "Downside of having too many features",
    "content": ". | Unlike simple linear regression, it is obvious that it is not easy or possible to visualize the resulting model due to the high dimensionality of the feature space. But to be fair, this is unavoidable. | Not only that, but it is also difficult to explain/grasp the relationship between the dependent variable and the independent variables as more features are added to the model. | And sometimes, not all features are relevant to the dependent variable and including them in the model can only bring in noise and overfitting. Which is the most concerning point. | . So, it would be better to select only the relevant features for the model. Modern ML libraries with regression models usually automatically perform feature selection for you. ",
    "url": "/docs/statistics/notes/linear-regression.html#downside-of-having-too-many-features",
    "relUrl": "/docs/statistics/notes/linear-regression.html#downside-of-having-too-many-features"
  },"649": {
    "doc": "Linear Regression",
    "title": "Feature selection methods",
    "content": "First, the obvious ones: . | Just use all the features . | Might be reasonable if you already have some prior domain knowledge about the problem (e.g. via laws of physics) and you’re sure that all the features are relevant and beneficial to the model. | . | Brute-force all the possible combinations of features . | Decide on a metric to evaluate the model. Choose the combination of features that maximizes good fit. | This is not feasible in most cases ($2^p - 1$ combinations for $p$ features). | But if you have a small number of features, this might be okay. | . | . Then, the more sophisticated stepwise regression methods that selects features step-by-step based on the results of hypothesis testing: . Backward elimination . Start with all the features and remove the least significant feature . | Pick a significance level $\\alpha$ (e.g. 0.05). | Fit the model with all the features. | Find the feature with the highest $p$-value. | If $p &gt; \\alpha$, remove that feature. Otherwise, stop. | Fit the model again with the remaining features and repeat until stop. | . Forward selection . Start with no feature and cumulatively add the most significant feature . | Pick a significance level $\\alpha$ (e.g. 0.05). | For each feature $X_i$, fit a simple regression model and select $X_i$ with the lowest $p$-value. | If the lowest $p &lt; \\alpha$, add that feature to the model. Otherwise, stop. | For each feature $X_j$ that is previously not considered, fit a regression model together with all the previously selected features. Then select $X_j$ with the lowest $p$-value. | Repeat until stop. | . Bidirectional elimination . Combination of backward elimination and forward selection . | Pick two significance levels $\\alpha_{enter}$ and $\\alpha_{stay}$. | Perform one step of forward selection (selecting if $p &lt; \\alpha_{enter}$). | Perform the entire backward elimination (removing in each step if $p &gt; \\alpha_{stay}$) In other words, features \"stay\" if $p &lt; \\alpha_{stay}$. | If no feature was added or removed in the previous two steps, stop. Otherwise, repeat. | . ",
    "url": "/docs/statistics/notes/linear-regression.html#feature-selection-methods",
    "relUrl": "/docs/statistics/notes/linear-regression.html#feature-selection-methods"
  },"650": {
    "doc": "Linear Regression",
    "title": "Polynomial regression",
    "content": "Polynomial regression is actually just a special case of multiple linear regression. It’s just that we transform the feature matrix to include polynomial terms. What matters is the coefficients of the model, because they are the actual unknowns that we try to estimate while the features are collections of known values. Although the features are in polynomial forms, the model itself is still a linear combination of the coefficients. For a single feature $x$ and a polynomial degree of $d$, . $$ y_i = \\beta_0 + \\beta_1 x_{i} + \\beta_2 x_{i}^2 + \\cdots + \\beta_d x_{i}^d + \\varepsilon_i $$ . Which is still just our regular . \\[y = X \\beta + \\varepsilon\\] when we define the feature matrix $X$ as: . \\[X = \\begin{bmatrix} 1 &amp; x_{1} &amp; x_{1}^2 &amp; \\cdots &amp; x_{1}^d \\\\ 1 &amp; x_{2} &amp; x_{2}^2 &amp; \\cdots &amp; x_{2}^d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n} &amp; x_{n}^2 &amp; \\cdots &amp; x_{n}^d \\end{bmatrix}\\] This matrix is also called the Vandermonde matrix. For two features $x_1$ and $x_2$ and a polynomial degree of $2$, . $$ y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i1}^2 + \\beta_4 x_{i2}^2 + \\beta_5 x_{i1} x_{i2} + \\varepsilon_i $$ . so the feature matrix $X$ is: . \\[X = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{11}^2 &amp; x_{12}^2 &amp; x_{11} x_{12} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{21}^2 &amp; x_{22}^2 &amp; x_{21} x_{22} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; x_{n1}^2 &amp; x_{n2}^2 &amp; x_{n1} x_{n2} \\end{bmatrix}\\] Only difference is that now joint polynomial terms are now included to model the interaction between the two features. When we have $p$ features and a polynomial degree of $d$, the number of features becomes a combination of $p+1$ variables (including the intercept) selected $d$ at a time with repetition allowed. If you increase the complexity of the model too much, estimation becomes computationally expensive. Also complex polynomial models are prone to overfitting. ",
    "url": "/docs/statistics/notes/linear-regression.html#polynomial-regression",
    "relUrl": "/docs/statistics/notes/linear-regression.html#polynomial-regression"
  },"651": {
    "doc": "Linear Regression",
    "title": "When is it adequate to use linear regression?",
    "content": "For the linear model to be useful, typically the following checks should hold: . While some of the following conditions are intuitive, some of the not-so-obvious reasons come from the fact that they are required for parameter estimation to have a closed form solution, making estimation feasible or efficient (e.g. inverse of a matrix exists). ",
    "url": "/docs/statistics/notes/linear-regression.html#when-is-it-adequate-to-use-linear-regression",
    "relUrl": "/docs/statistics/notes/linear-regression.html#when-is-it-adequate-to-use-linear-regression"
  },"652": {
    "doc": "Linear Regression",
    "title": "Linearity",
    "content": "The relationship between the dependent variable and the independent variables should be linear. Quite obvious: you don’t want to fit an arch to a line: . ",
    "url": "/docs/statistics/notes/linear-regression.html#linearity",
    "relUrl": "/docs/statistics/notes/linear-regression.html#linearity"
  },"653": {
    "doc": "Linear Regression",
    "title": "No multicollinearity",
    "content": "The independent variables should not be correlated with each other. If the value of one predictor is predetermined by the values of the other predictors, you have multicollinearity. Multicollinearity can cause the following problems: . | Computationally expensive . | Increases unnecessary coefficients to be estimated | The feature matrix becomes singular . | In other words, the inverse matrix does not exist, which is required for some parameter estimation methods to have a closed form solution | . | . | Introduces confusion . | Harder to explain the effects of each predictor because they affect one another | . | . ",
    "url": "/docs/statistics/notes/linear-regression.html#no-multicollinearity",
    "relUrl": "/docs/statistics/notes/linear-regression.html#no-multicollinearity"
  },"654": {
    "doc": "Linear Regression",
    "title": "Homoscedasticity of the residuals",
    "content": "The variance of the residuals should be constant. If you think about it, you want the data points to be within a certain boundary around the regression line for the model to be useful. ",
    "url": "/docs/statistics/notes/linear-regression.html#homoscedasticity-of-the-residuals",
    "relUrl": "/docs/statistics/notes/linear-regression.html#homoscedasticity-of-the-residuals"
  },"655": {
    "doc": "Linear Regression",
    "title": "No autocorrelation of the residuals",
    "content": "Autocorrelation is the correlation with the lagged version of itself. For linear regression to be suitable, there should be no autocorrelation. In the figure below, there is a clear correlation between certain lagged periods: . ",
    "url": "/docs/statistics/notes/linear-regression.html#no-autocorrelation-of-the-residuals",
    "relUrl": "/docs/statistics/notes/linear-regression.html#no-autocorrelation-of-the-residuals"
  },"656": {
    "doc": "Linear Regression",
    "title": "Normality of the residuals",
    "content": "If the linear regression model captures the relationship well enough, the residuals should be normally distributed. For example, the following figure suggests maybe it wasn’t the best idea to use a single line to explain the samples: . Also having this condition brings additional benefits especially when using the OLS method for parameter estimation. And most of the time, the OLS method is used for parameter estimation. ",
    "url": "/docs/statistics/notes/linear-regression.html#normality-of-the-residuals",
    "relUrl": "/docs/statistics/notes/linear-regression.html#normality-of-the-residuals"
  },"657": {
    "doc": "Linear Regression",
    "title": "No outliers",
    "content": "Outliers are data points that are significantly different from the rest of the data. If you know that these odd data points are truly outliers (in other words, they are not representative of the population, but only noise), then you can just remove them. However, if these points are, although rare, representative of the population maybe due to some special features, then you should consider using a different model, because a linear regression model is not robust to outliers. Modern ML libraries with linear regression models usually provide robust estimation methods that deweight outliers. ",
    "url": "/docs/statistics/notes/linear-regression.html#no-outliers",
    "relUrl": "/docs/statistics/notes/linear-regression.html#no-outliers"
  },"658": {
    "doc": "5 - Longest Palindromic Substring - Medium",
    "title": "Longest Palindromic Substring",
    "content": ". | Problem | Explanation | Solution | . ",
    "url": "/docs/compsci/leetcode/longest-palindromic-substring.html#longest-palindromic-substring",
    "relUrl": "/docs/compsci/leetcode/longest-palindromic-substring.html#longest-palindromic-substring"
  },"659": {
    "doc": "5 - Longest Palindromic Substring - Medium",
    "title": "Problem",
    "content": "Given a string s, return the longest palindromic substring in s. ",
    "url": "/docs/compsci/leetcode/longest-palindromic-substring.html#problem",
    "relUrl": "/docs/compsci/leetcode/longest-palindromic-substring.html#problem"
  },"660": {
    "doc": "5 - Longest Palindromic Substring - Medium",
    "title": "Explanation",
    "content": "The solution uses dynamic programming, with the memoization matrix $M[i,j]$ defined as follows: . \\[M[i,j] = \\begin{cases} \\text{true} &amp; \\text{if } s[i, j] \\text{ is a palindrome} \\\\[0.5em] \\text{false} &amp; \\text{otherwise} \\end{cases}\\] where $s[i, j]$ is the substring of $s$ from index $i$ to index $j$ (inclusive). One thing to note is that a substring of length $1$ is always a palindrome. This will be our base case: . \\[M[i,i] = \\text{true} \\quad \\forall i \\in [0, n)\\] If we want to know if substring $s[i, j]$ is a palindrome, . | For substrings of length $2$, it suffices to check $s[i] = s[j]$. | For substrings of length $l \\geq 3$, in addition to checking $s[i] = s[j]$, we also need to check if $s[i+1, j-1]$ is a palindrome ($M[i+1, j-1] = \\text{true}$) . Checking $s[i+1, j-1]$ is a palindrome is equivalent to checking the value of the cell at the one left-bottom diagonal of $M[i,j]$ (the orange arrow). | . To ensure that $M[i+1, j-1]$ is already computed before computing $M[i,j]$, we need to make sure that we are computing the cells in the matrix in the right order. The computation order must be in a diagonal fashion, starting from $M[0,1]$ and going to the right bottom. We can do this by fixing the length of the substring $l$, and then iterate through the string $s$ from left to right. ",
    "url": "/docs/compsci/leetcode/longest-palindromic-substring.html#explanation",
    "relUrl": "/docs/compsci/leetcode/longest-palindromic-substring.html#explanation"
  },"661": {
    "doc": "5 - Longest Palindromic Substring - Medium",
    "title": "Solution",
    "content": "| 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 . | public String longestPalindrome(String s) { int n = s.length(); boolean[][] M = new boolean[n][n]; for (int i = 0; i &lt; n; i++) { M[i][i] = true; // Set base case } int max = 1; int start = 0; for (int l = 2; l &lt;= n; l++) { for (int i = 0; i &lt; n - l + 1; i++) { int j = i + l - 1; boolean isPalindrome = s.charAt(i) == s.charAt(j); isPalindrome = isPalindrome &amp;&amp; (l == 2 || M[i + 1][j - 1]); if (isPalindrome) { max = l; start = i; } M[i][j] = isPalindrome; } } return s.substring(start, start + max); } . | . The complexity of the solution is $O(n^2)$. ",
    "url": "/docs/compsci/leetcode/longest-palindromic-substring.html#solution",
    "relUrl": "/docs/compsci/leetcode/longest-palindromic-substring.html#solution"
  },"662": {
    "doc": "5 - Longest Palindromic Substring - Medium",
    "title": "5 - Longest Palindromic Substring - Medium",
    "content": " ",
    "url": "/docs/compsci/leetcode/longest-palindromic-substring.html",
    "relUrl": "/docs/compsci/leetcode/longest-palindromic-substring.html"
  },"663": {
    "doc": "3 - Longest Substring Without Repeating Characters - Medium",
    "title": "Longest Substring Without Repeating Characters",
    "content": ". | Problem | Solution | . ",
    "url": "/docs/compsci/leetcode/longest-substring-no-repeat.html#longest-substring-without-repeating-characters",
    "relUrl": "/docs/compsci/leetcode/longest-substring-no-repeat.html#longest-substring-without-repeating-characters"
  },"664": {
    "doc": "3 - Longest Substring Without Repeating Characters - Medium",
    "title": "Problem",
    "content": "Given a string s, find the length of the longest substring without repeating characters. Input: s = \"abcabcbb\" Output: 3 Explanation: The answer is \"abc\", with the length of 3. ",
    "url": "/docs/compsci/leetcode/longest-substring-no-repeat.html#problem",
    "relUrl": "/docs/compsci/leetcode/longest-substring-no-repeat.html#problem"
  },"665": {
    "doc": "3 - Longest Substring Without Repeating Characters - Medium",
    "title": "Solution",
    "content": "The main idea is to use an imaginary sliding window. Until we see a repeated character, we just can keep expanding the window. However, once we see a repeated character, the starting point of the window needs to be moved to the right until the last occurrence of the repeated character is no longer in the window. \\[\\fbox{abcde} \\mbox{fd} \\implies \\fbox{abcdef} \\mbox{d} \\implies \\mbox{abcd} \\fbox{efd}\\] So, we should to keep track of the last occurrence of each character. I will be using a HashMap&lt;Character, Integer&gt; to do this, where each character is mapped to its index of last occurrence. HashMap&lt;Character, Integer&gt; seen = new HashMap&lt;&gt;(); int maxLen = 0; // Max found so far int curLen = 0; // Size of current window for (int j = 0; j &lt; s.length(); j++) { Character c = s.charAt(j); if (seen.containsKey(c)) { // Found a repeated character int i = seen.get(c); // Retrieve index of last occurrence if (j - curLen &gt; i) { // If last occurrence is not within window curLen++; } else { curLen = j - (i + 1) + 1; // Calculate new window size } seen.replace(c, j); // Update last occurrence } else { curLen++; // Expand window seen.put(c, j); // Record last occurrence } maxLen = Math.max(maxLen, curLen); } return maxLen; . Everything is straightforward except for: . if (j - curLen &gt; i) { // If last occurrence is not within window curLen++; } else { curLen = j - (i + 1) + 1; // Calculate new window size } . What is the if statement doing? . Take the example of abbcda. When we see the last a at index 5, the window looks like . \\[\\mbox{ab} \\fbox{bcd} \\mbox{a}\\] The window should now ignore everything before the second b, but the HashMap still has a -&gt; 0 in it. Calculating j - (i + 1) + 1 will give us the wrong window of . \\[\\fbox{abbcda}\\] so we have to make the window ignore everything outside of current window. ",
    "url": "/docs/compsci/leetcode/longest-substring-no-repeat.html#solution",
    "relUrl": "/docs/compsci/leetcode/longest-substring-no-repeat.html#solution"
  },"666": {
    "doc": "3 - Longest Substring Without Repeating Characters - Medium",
    "title": "3 - Longest Substring Without Repeating Characters - Medium",
    "content": " ",
    "url": "/docs/compsci/leetcode/longest-substring-no-repeat.html",
    "relUrl": "/docs/compsci/leetcode/longest-substring-no-repeat.html"
  },"667": {
    "doc": "M1 TensorFlow",
    "title": "M1 Tensorflow",
    "content": "GPU accelerated TensorFlow on M1 . See here for the official documentation. | Install Miniforge | Install TensorFlow dependencies | Install base TensorFlow and tensorflow-metal | To upgrade to a new TensorFlow version | . ",
    "url": "/docs/data-science/m1-tf.html#m1-tensorflow",
    "relUrl": "/docs/data-science/m1-tf.html#m1-tensorflow"
  },"668": {
    "doc": "M1 TensorFlow",
    "title": "Install Miniforge",
    "content": "Download and install Miniforge3 conda: . curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh chmod +x Miniforge3-MacOSX-arm64.sh sh Miniforge3-MacOSX-arm64.sh # Optional conda config --set auto_activate_base false . For information on conda see here. Installing Miniconda3 macOS Apple M1 64-bit bash (Py38 conda 4.10.1 2021-11-08) did not work. Miniforge’s arm64 was the only thing that worked. ",
    "url": "/docs/data-science/m1-tf.html#install-miniforge",
    "relUrl": "/docs/data-science/m1-tf.html#install-miniforge"
  },"669": {
    "doc": "M1 TensorFlow",
    "title": "Install TensorFlow dependencies",
    "content": "Create a new environment: . conda create -n tf-m1 python=3.9 . You can use either Python 3.8 or 3.9. Install tensorflow-deps: . conda install -c apple tensorflow-deps . ",
    "url": "/docs/data-science/m1-tf.html#install-tensorflow-dependencies",
    "relUrl": "/docs/data-science/m1-tf.html#install-tensorflow-dependencies"
  },"670": {
    "doc": "M1 TensorFlow",
    "title": "Install base TensorFlow and tensorflow-metal",
    "content": "Making sure the environment is activated: . Check environment is indeed activated by which pip. Especially if you’re doing it in a VSCode terminal. pip install tensorflow-macos . pip install tensorflow-metal . ",
    "url": "/docs/data-science/m1-tf.html#install-base-tensorflow-and-tensorflow-metal",
    "relUrl": "/docs/data-science/m1-tf.html#install-base-tensorflow-and-tensorflow-metal"
  },"671": {
    "doc": "M1 TensorFlow",
    "title": "To upgrade to a new TensorFlow version",
    "content": "From the official doc, the recommended way is: . # uninstall existing tensorflow-macos and tensorflow-metal python -m pip uninstall tensorflow-macos python -m pip uninstall tensorflow-metal # Upgrade tensorflow-deps conda install -c apple tensorflow-deps --force-reinstall # or point to specific conda environment conda install -c apple tensorflow-deps --force-reinstall -n my_env . ",
    "url": "/docs/data-science/m1-tf.html#to-upgrade-to-a-new-tensorflow-version",
    "relUrl": "/docs/data-science/m1-tf.html#to-upgrade-to-a-new-tensorflow-version"
  },"672": {
    "doc": "M1 TensorFlow",
    "title": "M1 TensorFlow",
    "content": " ",
    "url": "/docs/data-science/m1-tf.html",
    "relUrl": "/docs/data-science/m1-tf.html"
  },"673": {
    "doc": "Mapping",
    "title": "Mapping",
    "content": "Quick recap of definitions. | Basic definitions . | Domain | Codomain | Image | Range | . | Surjection | Injection | Bijection | Inverse mapping | . ",
    "url": "/docs/linalg/notes/mapping.html",
    "relUrl": "/docs/linalg/notes/mapping.html"
  },"674": {
    "doc": "Mapping",
    "title": "Basic definitions",
    "content": "Let $f$ be a function or mapping from a set $A$ to a set $B$: . $$ f: X \\to Y $$ . ",
    "url": "/docs/linalg/notes/mapping.html#basic-definitions",
    "relUrl": "/docs/linalg/notes/mapping.html#basic-definitions"
  },"675": {
    "doc": "Mapping",
    "title": "Domain",
    "content": "The set $X$ is called the domain of $f$. ",
    "url": "/docs/linalg/notes/mapping.html#domain",
    "relUrl": "/docs/linalg/notes/mapping.html#domain"
  },"676": {
    "doc": "Mapping",
    "title": "Codomain",
    "content": "The set $Y$ is called the codomain of $f$. ",
    "url": "/docs/linalg/notes/mapping.html#codomain",
    "relUrl": "/docs/linalg/notes/mapping.html#codomain"
  },"677": {
    "doc": "Mapping",
    "title": "Image",
    "content": "When $f(x_i) = y_i$, we call $x_i$ the image of $y_i$ under $f$. ",
    "url": "/docs/linalg/notes/mapping.html#image",
    "relUrl": "/docs/linalg/notes/mapping.html#image"
  },"678": {
    "doc": "Mapping",
    "title": "Range",
    "content": "The set of all images of $f$ is called the range of $f$: . \\[\\{f(x_i) \\mid x_i \\in X\\}\\] Sometimes image and range are used interchangeably. ",
    "url": "/docs/linalg/notes/mapping.html#range",
    "relUrl": "/docs/linalg/notes/mapping.html#range"
  },"679": {
    "doc": "Mapping",
    "title": "Surjection",
    "content": "Let $f: X \\to Y$, $f$ is surjective if . $$ \\forall y \\in Y,\\; \\exists x \\in X \\mathbin{s.t.} f(x) = y $$ . In other words, if the range of $f$ is equal to its codomain. Also called onto mapping. ",
    "url": "/docs/linalg/notes/mapping.html#surjection",
    "relUrl": "/docs/linalg/notes/mapping.html#surjection"
  },"680": {
    "doc": "Mapping",
    "title": "Injection",
    "content": "Let $f: X \\to Y$, $f$ is injective if . $$ \\forall x_1, x_2 \\in X,\\; f(x_1) = f(x_2) \\Rightarrow x_1 = x_2 $$ . Also called one-to-one mapping. ",
    "url": "/docs/linalg/notes/mapping.html#injection",
    "relUrl": "/docs/linalg/notes/mapping.html#injection"
  },"681": {
    "doc": "Mapping",
    "title": "Bijection",
    "content": "Let $f: X \\to Y$, $f$ is bijective if it is both surjective and injective. Also called one-to-one correspondence. ",
    "url": "/docs/linalg/notes/mapping.html#bijection",
    "relUrl": "/docs/linalg/notes/mapping.html#bijection"
  },"682": {
    "doc": "Mapping",
    "title": "Inverse mapping",
    "content": "Let $f: X \\to Y$ be a bijection, then there exists a inverse mapping $f^{-1}: Y \\to X$ such that . $$ \\forall x \\in X,\\; f^{-1}(f(x)) = x $$ . ",
    "url": "/docs/linalg/notes/mapping.html#inverse-mapping",
    "relUrl": "/docs/linalg/notes/mapping.html#inverse-mapping"
  },"683": {
    "doc": "Matrix Decomposition",
    "title": "Matrix Decomposition",
    "content": ". | LU decomposition . | Condition | How to find $L$ and $U$ . | Using elementary row operations | Doolittle algorithm | . | Using LU to solve linear equations | . | . ",
    "url": "/docs/linalg/notes/matrix-decomposition.html",
    "relUrl": "/docs/linalg/notes/matrix-decomposition.html"
  },"684": {
    "doc": "Matrix Decomposition",
    "title": "LU decomposition",
    "content": "The LU decomposition of any matrix $A$ is a factorization of $A$ into a product of a lower triangular matrix $L$ and an upper triangular matrix $U$. $$ A = LU $$ . $A$ does not have to be a square matrix. It is conventional to have the principal diagonal of $L$ to be all $1$. ",
    "url": "/docs/linalg/notes/matrix-decomposition.html#lu-decomposition",
    "relUrl": "/docs/linalg/notes/matrix-decomposition.html#lu-decomposition"
  },"685": {
    "doc": "Matrix Decomposition",
    "title": "Condition",
    "content": "For a matrix $A$ to have an LU decomposition, it must be possible to rewrite $A$ as an upper triangular matrix with only non-zero row scalings and row additions. So basically the elementary row operations except row exchange. If row exchange is needed, then $A$ has a PLU decomposition, where $P$ is a permutation matrix. ",
    "url": "/docs/linalg/notes/matrix-decomposition.html#condition",
    "relUrl": "/docs/linalg/notes/matrix-decomposition.html#condition"
  },"686": {
    "doc": "Matrix Decomposition",
    "title": "How to find $L$ and $U$",
    "content": "Using elementary row operations . As stated above, matrix $A$ must be able to be rewritten as an upper triangular matrix $U$ for it to have an LU decomposition. The elementary matrix multiplication $E_1 \\dots E_k$ that are used to transform $A$ has an inverse of $E_1^{-1} \\dots E_k^{-1}$ by its property. \\[E_k \\dots E_1 A = U \\Rightarrow A = E_1^{-1} \\dots E_k^{-1} U\\] therefore: . $$ L = E_1^{-1} \\dots E_k^{-1} $$ . Doolittle algorithm . The Doolittle algorithm is a method to find the LU decomposition of a matrix $A$. Assuming the principal diagonal of $L$ is all $1$. Starting from the top left corner of $A$, where $l_{1k} = [1, 0, \\dots, 0]$ is the first row vector of $L$, and $u_{k1} = [u_{11}, 0, \\dots, 0]^T$ is the first column vector of $U$: . \\[a_{11} = l_{1k} \\cdot u_{k1} = u_{11}\\] We iteratively find the following: . $$ \\begin{gather*} \\forall i &gt; j,\\; l_{ij} = \\frac{a_{ij} - \\sum_{k=1}^{j-1} l_{ik} u_{kj}}{u_{jj}} \\\\ \\forall i \\geq j,\\; u_{ij} = a_{ij} - \\sum_{k=1}^{j-1} l_{ik} u_{kj} \\end{gather*} $$ . ",
    "url": "/docs/linalg/notes/matrix-decomposition.html#how-to-find-l-and-u",
    "relUrl": "/docs/linalg/notes/matrix-decomposition.html#how-to-find-l-and-u"
  },"687": {
    "doc": "Matrix Decomposition",
    "title": "Using LU to solve linear equations",
    "content": "Let $A$ be a matrix that has an LU decomposition $LU$, . Then the linear equation $Ax = b$ can be solved by: . \\[Ax = b \\Rightarrow LUx = b \\Rightarrow Ly = b \\land Ux = y\\] where $y$ is a vector that can be found by solving $Ly = b$. We do this because with triangular matrices in augmented form, Gauss-Jordan elimination becomes much easier. ",
    "url": "/docs/linalg/notes/matrix-decomposition.html#using-lu-to-solve-linear-equations",
    "relUrl": "/docs/linalg/notes/matrix-decomposition.html#using-lu-to-solve-linear-equations"
  },"688": {
    "doc": "Basic Matrix Definitions",
    "title": "Basic Matrix Definitions",
    "content": "Basic definitions. | Matrix . | Elementary row operations . | Row equivalence | . | Basic matrix math operations | Matrix multiplication | . | Square matrix . | Identity matrix | Elementary matrix | Power of a matrix | . | Transpose of a matrix . | Symmetric matrix | Skew-symmetric matrix | Properties of transpose | . | . ",
    "url": "/docs/linalg/notes/matrix.html",
    "relUrl": "/docs/linalg/notes/matrix.html"
  },"689": {
    "doc": "Basic Matrix Definitions",
    "title": "Matrix",
    "content": "Following is a $\\mathbf{m \\times n}$ matrix $A$, with $m$ rows and $n$ columns: . $$ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{bmatrix} $$ . | $a_ij$ is the $(i, j)$-th entry or element of $A$. | Often denoted in a short-hand notation: . \\[A = [a_{ij}]_{m \\times n}\\] | . ",
    "url": "/docs/linalg/notes/matrix.html#matrix",
    "relUrl": "/docs/linalg/notes/matrix.html#matrix"
  },"690": {
    "doc": "Basic Matrix Definitions",
    "title": "Elementary row operations",
    "content": ". | Row exchange: swap two rows of a matrix. | Row scaling: multiply a row by a nonzero scalar. | Row addition: add a scalar multiple of one row to another row. | . Row equivalence . Matrices $A$ and $B$ are row equivalent if there is a sequence of elementary row operations that transforms $A$ into $B$. We denote row equivalence by: . $$ A \\sim B $$ . ",
    "url": "/docs/linalg/notes/matrix.html#elementary-row-operations",
    "relUrl": "/docs/linalg/notes/matrix.html#elementary-row-operations"
  },"691": {
    "doc": "Basic Matrix Definitions",
    "title": "Basic matrix math operations",
    "content": ". | Addition and subtraction of matrices are defined only for matrices of the same size. | Scalar multiplication is defined for any matrix and any scalar. | . Basic matrix calculations generally satisfy all the usual rules of algebra: commutative, associative, distributive, etc. ",
    "url": "/docs/linalg/notes/matrix.html#basic-matrix-math-operations",
    "relUrl": "/docs/linalg/notes/matrix.html#basic-matrix-math-operations"
  },"692": {
    "doc": "Basic Matrix Definitions",
    "title": "Matrix multiplication",
    "content": "Matrix multiplication $A \\times B$ is defined only if the number of columns of $A$ is equal to the number of rows of $B$: $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$. If the resulting matrix $C = A \\times B$ is defined, then $C \\in \\mathbb{R}^{m \\times p}$. The $(i, j)$-th element of $C$ is the dot product of the $i$-th row of $A$ and the $j$-th column of $B$: . $$ c_{ij} = \\sum_{k=1}^n a_{ik} b_{kj} $$ . | Associative: $(A B) C = A (B C)$ | Distributive: $A (B + C) = A B + A C$ and $(A + B) C = A C + B C$ | Not commutative: $A B \\neq B A$ | Not cancellative: $A B = A C$ does not imply $B = C$ | $A B = 0$ does not imply $A = 0$ or $B = 0$ | . Expand $(A+B)^2$, etc. to check your understanding Given that $A$ and $B$ are square matrices of the same size, confirm that . \\[\\begin{gather*} (A+B)^2 = A^2 + AB + BA + B^2\\\\ (A-B)^2 = A^2 - AB - BA + B^2\\\\ (A+B)(A-B) = A^2 - AB + BA - B^2 \\end{gather*}\\] . ",
    "url": "/docs/linalg/notes/matrix.html#matrix-multiplication",
    "relUrl": "/docs/linalg/notes/matrix.html#matrix-multiplication"
  },"693": {
    "doc": "Basic Matrix Definitions",
    "title": "Square matrix",
    "content": "A square matrix is a matrix with the same number of rows and columns. A $\\mathbf{n \\times n}$ matrix is called an $n$-dimensional square matrix. ",
    "url": "/docs/linalg/notes/matrix.html#square-matrix",
    "relUrl": "/docs/linalg/notes/matrix.html#square-matrix"
  },"694": {
    "doc": "Basic Matrix Definitions",
    "title": "Identity matrix",
    "content": "An identity matrix (unit matrix) is a diagonal matrix whose principal diagonal elements are all one. The identity matrix is often denoted by $I$ or $I_n$. \\[I = \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{bmatrix}\\] ",
    "url": "/docs/linalg/notes/matrix.html#identity-matrix",
    "relUrl": "/docs/linalg/notes/matrix.html#identity-matrix"
  },"695": {
    "doc": "Basic Matrix Definitions",
    "title": "Elementary matrix",
    "content": "An elementary matrix is a square matrix that can be obtained from the identity matrix by a single elementary row operation (row exchange, linear combination of rows). From $I_3$, we could obtain the following few examples of elementary matrices: . \\[I = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\quad E_1 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\quad E_2 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -3 &amp; 0 &amp; 1 \\end{bmatrix} \\quad E_3 = \\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] Multiplying a matrix by an elementary matrix is equivalent to performing the corresponding elementary row operation. Elementary matrices are invertible. Which makes sense, because elementary row operations are invertible. The inverse of an elementary matrix is also an elementary matrix which undoes the elementary row operation. ",
    "url": "/docs/linalg/notes/matrix.html#elementary-matrix",
    "relUrl": "/docs/linalg/notes/matrix.html#elementary-matrix"
  },"696": {
    "doc": "Basic Matrix Definitions",
    "title": "Power of a matrix",
    "content": "For a square matrix $A$, $A^k$ is defined as the power of matrix $A$. It has the following properties: . | $A^0 = I$ | $(A^k)^l = A^{kl}$ | $A^k A^l = A^{k+l}$ | . ",
    "url": "/docs/linalg/notes/matrix.html#power-of-a-matrix",
    "relUrl": "/docs/linalg/notes/matrix.html#power-of-a-matrix"
  },"697": {
    "doc": "Basic Matrix Definitions",
    "title": "Transpose of a matrix",
    "content": "The transpose of a matrix $A$ is denoted by $A^T$: . \\[A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{bmatrix} \\quad A^T = \\begin{bmatrix} a_{11} &amp; a_{21} &amp; \\cdots &amp; a_{m1} \\\\ a_{12} &amp; a_{22} &amp; \\cdots &amp; a_{m2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{1n} &amp; a_{2n} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}\\] ",
    "url": "/docs/linalg/notes/matrix.html#transpose-of-a-matrix",
    "relUrl": "/docs/linalg/notes/matrix.html#transpose-of-a-matrix"
  },"698": {
    "doc": "Basic Matrix Definitions",
    "title": "Symmetric matrix",
    "content": "A matrix $A$ is symmetric if $A = A^T$. ",
    "url": "/docs/linalg/notes/matrix.html#symmetric-matrix",
    "relUrl": "/docs/linalg/notes/matrix.html#symmetric-matrix"
  },"699": {
    "doc": "Basic Matrix Definitions",
    "title": "Skew-symmetric matrix",
    "content": "A matrix $A$ is skew-symmetric if $A^T = -A$. ",
    "url": "/docs/linalg/notes/matrix.html#skew-symmetric-matrix",
    "relUrl": "/docs/linalg/notes/matrix.html#skew-symmetric-matrix"
  },"700": {
    "doc": "Basic Matrix Definitions",
    "title": "Properties of transpose",
    "content": "For any matrices $A$ and $B$ and any scalar $c$: . | $(A^T)^T = A$ | $(A + B)^T = A^T + B^T$ | $(A B)^T = B^T A^T$ | $(c A)^T = c A^T$ | If $A$ is invertible, then $(A^{-1})^T = (A^T)^{-1}$ | . ",
    "url": "/docs/linalg/notes/matrix.html#properties-of-transpose",
    "relUrl": "/docs/linalg/notes/matrix.html#properties-of-transpose"
  },"701": {
    "doc": "4 - Median of Two Sorted Arrays - Hard",
    "title": "Median of Two Sorted Arrays",
    "content": ". | Problem | Explanation | Solution | . ",
    "url": "/docs/compsci/leetcode/median-of-two-sorted-arrays.html#median-of-two-sorted-arrays",
    "relUrl": "/docs/compsci/leetcode/median-of-two-sorted-arrays.html#median-of-two-sorted-arrays"
  },"702": {
    "doc": "4 - Median of Two Sorted Arrays - Hard",
    "title": "Problem",
    "content": "Given two sorted arrays nums1 and nums2 of size m and n respectively, return the median of the two sorted arrays. Input: nums1 = [1,3], nums2 = [2] Output: 2.00000 Explanation: merged array = [1,2,3] and median is 2. Input: nums1 = [1,2], nums2 = [3,4] Output: 2.50000 Explanation: merged array = [1,2,3,4] and median is (2 + 3) / 2 = 2.5. ",
    "url": "/docs/compsci/leetcode/median-of-two-sorted-arrays.html#problem",
    "relUrl": "/docs/compsci/leetcode/median-of-two-sorted-arrays.html#problem"
  },"703": {
    "doc": "4 - Median of Two Sorted Arrays - Hard",
    "title": "Explanation",
    "content": "Let $A$ and $B$ be the two sorted arrays of size $m$ and $n$ respectively. Let’s assume that $m \\leq n$. Let $x$ be the median of the two sorted arrays. This $x$ will divide each array into two parts: . \\[\\begin{gather*} A_{left} \\mid A_{right} \\\\ B_{left} \\mid B_{right} \\end{gather*}\\] where $A_{left}$ and $B_{left}$ consist of respective elements $\\le x$, and $A_{right}$ and $B_{right}$ consist of respective elements $\\gt x$. For $x$ to be the median of the two sorted arrays, the following conditions must be satisfied: . | $\\max(A_{left}) \\le \\min(B_{right})$ and $\\max(B_{left}) \\le \\min(A_{right})$ | If $m + n$ is odd, then $|A_{left}| + |B_{left}| = |A_{right}| + |B_{right}| + 1$. If $m + n$ is even, then $|A_{left}| + |B_{left}| = |A_{right}| + |B_{right}|$ . | . Let $i$ be the number of elements in $A_{left}$, and $j$ be the number of elements in $B_{left}$. The first condition can be rewritten as: . \\[a_{i - 1} \\le b_j \\wedge b_{j - 1} \\le a_i\\] The notation follows the coding convention of 0-based indexing. The second condition can be rewritten as: . \\[\\begin{gather*} |A_{left}| + |B_{left}| = |A_{right}| + |B_{right}| + 1 \\\\[1em] i + j = (m - i) + (n - j) + 1 \\\\[1em] 2j = m + n + 1 - 2i \\\\[1em] j = \\frac{m + n + 1}{2} - i \\end{gather*}\\] Later in code, the integer division truncating (m + n + 1) / 2 takes care of both odd and even cases. We see that $j$ is predetermined by $i$, so our problem boils down to finding the right $i$ that satisfies the first condition. Once we find the right $i$ and $j$, we can compute the median $x$ as follows: . \\[x = \\begin{cases} \\max(a_{i-1}, b_{j-1}) &amp; \\text{if } m + n \\text{ is odd} \\\\[1em] \\operatorname{avg}(\\max(a_{i-1}, b_{j-1}), \\min(a_{i}, b_{j})) &amp; \\text{if } m + n \\text{ is even} \\end{cases}\\] . | Why $\\max(a_{i-1}, b_{j-1})$ for odd $m + n$? . Because we know the left side has one more element which is the median. | Why $\\operatorname{avg}(\\max(a_{i-1}, b_{j-1}), \\min(a_{i}, b_{j}))$ for even $m + n$? . Because we want the average of the two middle elements. | . Now we do a binary search on $i$ in the range $[0, m]$, to find the right $i$ that satisfies the conditions. ",
    "url": "/docs/compsci/leetcode/median-of-two-sorted-arrays.html#explanation",
    "relUrl": "/docs/compsci/leetcode/median-of-two-sorted-arrays.html#explanation"
  },"704": {
    "doc": "4 - Median of Two Sorted Arrays - Hard",
    "title": "Solution",
    "content": "| 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 . | public double findMedianSortedArrays(int[] A, int[] B) { int m = A.length; int n = B.length; // Because we want to do a binary search on the smaller array if (m &gt; n) return findMedianSortedArrays(B, A); // Setting candidate range for i int iMin = 0; int iMax = m; int lenCondition = (m + n + 1) / 2; while (iMin &lt;= iMax) { int i = (iMin + iMax) / 2; int j = lenCondition - i; int maxALeft = (i &lt; 1) ? Integer.MIN_VALUE : A[i - 1]; int maxBLeft = (j &lt; 1) ? Integer.MIN_VALUE : B[j - 1]; int minARight = (i &gt;= m) ? Integer.MAX_VALUE : A[i]; int minBRight = (j &gt;= n) ? Integer.MAX_VALUE : B[j]; if (maxALeft &lt;= minBRight &amp;&amp; maxBLeft &lt;= minARight) { if ((m + n) % 2 &gt; 0) { // When m + n is odd, return Math.max(maxALeft, maxBLeft); // Median is at the left } else { // When m + n is even, // Median is the average of the two middle elements return (Math.max(maxALeft, maxBLeft) + Math.min(minARight, minBRight)) / 2.0; } } else if (maxALeft &gt; minBRight) { // We overshot our partition for A iMax = i - 1; } else { // We undershot our partition for A iMin = i + 1; } } return 0.0; } . | . Everything is explained in the Explanation section, except for the lines 17-20. | maxALeft: $a_{i-1}$ | maxBLeft: $b_{j-1}$ | minARight: $a_i$ | minBRight: $b_j$ | . We must note that a partition of $A$ and $B$ by $x$ may be empty. Take $A = [1, 2]$ and $B = [3, 4]$, where the median $2.5$ is not in the middle of the two arrays. \\[\\begin{gather*} A_{left} = [1, 2] \\mid A_{right} = [] \\\\[1em] B_{left} = [] \\mid B_{right} = [3, 4] \\end{gather*}\\] We want $a_{i-1} \\le b_j$ and $b_{j-1} \\le a_i$, but we can’t compare $b_{j-1}$ and $a_i$ because $B_{left}$ and $A_{right}$ are empty. That is why we need to turn them into . \\[\\begin{gather*} A_{left} = [1, 2] \\mid A_{right} = [\\infty] \\\\[1em] B_{left} = [-\\infty] \\mid B_{right} = [3, 4] \\end{gather*}\\] By setting $a_{i-1}, b_{j-1} = -\\infty$ and $a_i, b_j = \\infty$, when indices are out of bound. The complexity is $O(\\log(\\min(m, n)))$. ",
    "url": "/docs/compsci/leetcode/median-of-two-sorted-arrays.html#solution",
    "relUrl": "/docs/compsci/leetcode/median-of-two-sorted-arrays.html#solution"
  },"705": {
    "doc": "4 - Median of Two Sorted Arrays - Hard",
    "title": "4 - Median of Two Sorted Arrays - Hard",
    "content": " ",
    "url": "/docs/compsci/leetcode/median-of-two-sorted-arrays.html",
    "relUrl": "/docs/compsci/leetcode/median-of-two-sorted-arrays.html"
  },"706": {
    "doc": "Generate n-Length K-ary String",
    "title": "Generate n-Length K-ary String",
    "content": ". | Problem | Backtracking | . ",
    "url": "/docs/compsci/algo/n-bit-binary-string.html",
    "relUrl": "/docs/compsci/algo/n-bit-binary-string.html"
  },"707": {
    "doc": "Generate n-Length K-ary String",
    "title": "Problem",
    "content": "Generate all possible n-length k-ary strings. K-ary string is a string that only contains characters from the set $\\{0, 1, \\ldots, k-1\\}$. ",
    "url": "/docs/compsci/algo/n-bit-binary-string.html#problem",
    "relUrl": "/docs/compsci/algo/n-bit-binary-string.html#problem"
  },"708": {
    "doc": "Generate n-Length K-ary String",
    "title": "Backtracking",
    "content": ". | Create an array arr of length n, initialized with all zeros. | Have a helper function that wil print the array arr as a single string. | In the recursion: | . void generate(int[] arr, int n, int k) { if (n &lt; 1) { printArray(arr); } else { for (int i = 0; i &lt; k; i++) { arr[n - 1] = i; generate(arr, n - 1, k); } } } . The idea is to fix a single character at a time, and recursively generate all possible strings with the fixed character. Once all possibilities with the fixed character are exhausted, we move on to experiment with another character. The recurrence relation is . \\[T(n) = k T(n - 1) + O(n)\\] which is $O(k^n) \\cdot O(n)$. The $O(n)$ comes from naively printing the array as a string. ",
    "url": "/docs/compsci/algo/n-bit-binary-string.html#backtracking",
    "relUrl": "/docs/compsci/algo/n-bit-binary-string.html#backtracking"
  },"709": {
    "doc": "Docker Networks",
    "title": "Docker Networks",
    "content": ". | Some useful commands | How do I talk to the container? | How can containers talk to each other? . | Default bridge network | User-defined bridge network | . | . ",
    "url": "/docs/docker/networks.html",
    "relUrl": "/docs/docker/networks.html"
  },"710": {
    "doc": "Docker Networks",
    "title": "Some useful commands",
    "content": "# List all networks docker network ls . docker network inspect network-name . # Disconnect any containers using this network docker network disconnect network-name my-container docker network rm network-name . # Remove unused networks docker network prune . Link to documentation. ",
    "url": "/docs/docker/networks.html#some-useful-commands",
    "relUrl": "/docs/docker/networks.html#some-useful-commands"
  },"711": {
    "doc": "Docker Networks",
    "title": "How do I talk to the container?",
    "content": "When a container is created, none of the ports inside the container are exposed. In order for the Docker host (your computer) or other containers to talk to it, it must first publish a port. The following maps a port 1234 inside a container to 4321 on Docker host. docker create -p 1234:4321 . Now you can communicate with the container via http://localhost:4321. ",
    "url": "/docs/docker/networks.html#how-do-i-talk-to-the-container",
    "relUrl": "/docs/docker/networks.html#how-do-i-talk-to-the-container"
  },"712": {
    "doc": "Docker Networks",
    "title": "How can containers talk to each other?",
    "content": "If the containers are running on the same Docker daemon host (ie. all running on your computer), then the easiest way is to put them on the same bridge network. Default bridge network . Check the existing docker networks with . docker network ls . You will see a network with the name bridge. That is the default bridge network. Every started container is automatically added to the default bridge network if you didn’t specify anything else. With the default bridge you talk to other containers by using their IP Address. docker inspect my-container | grep IPAddress . Downsides to using the default bridge network: . | Using an IP address sucks: it is not immediate which container I’m referring to. | Every container can talk to every other container, which may cause security issues. | . User-defined bridge network . You can instead add a user-defined bridge network. It still uses the same bridge driver, but unlike the default bridge not everyone is invited to it. docker network create my-bridge # You can add after container creation docker network connect my-bridge my-container # Or when you create it docker create --network my-bridge . In user-defined bridge network, containers can talk to each other by using the container names as hostnames. So if my container was named my-db with port published at 1234, then the API would be: . http://my-db:1234 . References: . | Docker Networking | . ",
    "url": "/docs/docker/networks.html#how-can-containers-talk-to-each-other",
    "relUrl": "/docs/docker/networks.html#how-can-containers-talk-to-each-other"
  },"713": {
    "doc": "Comparing with Nonparametric Tests",
    "title": "Comparing with Nonparametric Tests",
    "content": "To be added . When it is hard to assume normality, we generally use nonparametric tests. In such cases, instead of comparing the means of the data, we use other representative statistics (such as the median, rank, etc.) that describes the position of the data. | Wilcoxon Rank Sum Test | . ",
    "url": "/docs/statistics/basics/nonparametric-test.html",
    "relUrl": "/docs/statistics/basics/nonparametric-test.html"
  },"714": {
    "doc": "Comparing with Nonparametric Tests",
    "title": "Wilcoxon Rank Sum Test",
    "content": "Instead of the mean like in parametric tests, Wilcoxon rank sum test is based on the rank of the data. ",
    "url": "/docs/statistics/basics/nonparametric-test.html#wilcoxon-rank-sum-test",
    "relUrl": "/docs/statistics/basics/nonparametric-test.html#wilcoxon-rank-sum-test"
  },"715": {
    "doc": "Python Package",
    "title": "Python Package",
    "content": "To be added . | Basic structure of a Python package | Install package . | Editable (development) mode | . | . ",
    "url": "/docs/python/package.html",
    "relUrl": "/docs/python/package.html"
  },"716": {
    "doc": "Python Package",
    "title": "Basic structure of a Python package",
    "content": "To be added . ",
    "url": "/docs/python/package.html#basic-structure-of-a-python-package",
    "relUrl": "/docs/python/package.html#basic-structure-of-a-python-package"
  },"717": {
    "doc": "Python Package",
    "title": "Install package",
    "content": "Editable (development) mode . To install a package in editable mode, you must have defined the package in a setup.py file. Navigate to the package directory and run: . pip3 install --editable . # OR pip3 install -e . References: . | setuptools | . ",
    "url": "/docs/python/package.html#install-package",
    "relUrl": "/docs/python/package.html#install-package"
  },"718": {
    "doc": "9 - Palindrome Number - Easy",
    "title": "Palindrome Number",
    "content": ". | Problem | Explanation | Solution | . ",
    "url": "/docs/compsci/leetcode/palindrome-number.html#palindrome-number",
    "relUrl": "/docs/compsci/leetcode/palindrome-number.html#palindrome-number"
  },"719": {
    "doc": "9 - Palindrome Number - Easy",
    "title": "Problem",
    "content": "Given an integer x, return true if x is a palindrome, and false otherwise. Input: x = 121 Output: true Explanation: 121 reads as 121 from left to right and from right to left. Input: x = -121 Output: false Explanation: From left to right, it reads -121. From right to left, it becomes 121-. Therefore it is not a palindrome. Input: x = 10 Output: false Explanation: Reads 01 from right to left. Therefore it is not a palindrome. ",
    "url": "/docs/compsci/leetcode/palindrome-number.html#problem",
    "relUrl": "/docs/compsci/leetcode/palindrome-number.html#problem"
  },"720": {
    "doc": "9 - Palindrome Number - Easy",
    "title": "Explanation",
    "content": "Things to note: . | Negative numbers are not palindromes. | First half of the digits must be the same as the reversed second half of the digits. | . Let $n$ be the number of digits in $x$. The first half of the digits can be obtained by dividing $x$ by $10^{n/2}$. When $n$ is odd, however, the middle digit does not matter, so we should divide $x$ by $10^{(n+1)/2}$ instead. \\[\\texttt{121 / 10^(4/2) = 1}\\] Then reverse sum the second half and compare it with the first half. ",
    "url": "/docs/compsci/leetcode/palindrome-number.html#explanation",
    "relUrl": "/docs/compsci/leetcode/palindrome-number.html#explanation"
  },"721": {
    "doc": "9 - Palindrome Number - Easy",
    "title": "Solution",
    "content": "| 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 . | public static boolean isPalindrome(int x) { if (x &lt; 0) return false; // Calculate the number of digits in x int n = 0; int xx = x; while (xx &gt; 0) { n++; xx /= 10; } int halfN = n / 2; int front = x / (int) Math.pow(10, (n % 2) == 0 ? halfN : halfN + 1); int backSum = 0; xx = x; for (int i = 0; i &lt; halfN; i++) { backSum = backSum * 10 + xx % 10; xx /= 10; } return front == backSum; } . | . ",
    "url": "/docs/compsci/leetcode/palindrome-number.html#solution",
    "relUrl": "/docs/compsci/leetcode/palindrome-number.html#solution"
  },"722": {
    "doc": "9 - Palindrome Number - Easy",
    "title": "9 - Palindrome Number - Easy",
    "content": " ",
    "url": "/docs/compsci/leetcode/palindrome-number.html",
    "relUrl": "/docs/compsci/leetcode/palindrome-number.html"
  },"723": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "Comparing Means in Parametric Tests",
    "content": "With parametric tests, we often compare the representative value mean against a hypothesis. This page describes different methods of comparing means in parametric tests. You don’t have to know all the equations by heart because most of the time you will be using a software to do the calculations for you. However, it is important to know the differences between the methods and when to use which method. | One-Sample t-Test | Paired t-Test | Two-Sample t-Test . | Pooled Standard Deviation | Homogeneity of Variance | t-Statistic for Two Independent Samples | Degrees of Freedom | . | Paired t-Test vs. Unpaired t-Test | . ",
    "url": "/docs/statistics/basics/parametric-test-mean.html",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html"
  },"724": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "One-Sample t-Test",
    "content": "One-sample t-test is a hypothesis test that determines an unknown mean of a population which the sample is drawn from. Because we use a t-distribution, we need to assume the normality of the population. For some value $m$, our null hypothesis would be: . $$ H_0: \\mu = m $$ . And our alternative hypothesis: . $$ H_A: \\mu \\ne m $$ . If $p &lt; \\alpha$, we would conclude that the population mean is unlikely to be $m$. It is important to note that the value $m$ must be selected so that it serves as a meaningful value to the researcher (there should be a reason why we chose $m$). For example, if we were testing for the population’s average height, there’s no point in just choosing $m = 170cm$, because that’s an arbitrary value. In this case, we are actually better off just calculating a confidence interval for the population mean. However, if there was a better reasoning behind $m = 170cm$ (such as, average height in a certain country was 170cm and we want to test if it is the same in this country), then our hypothesis test would be more meaningful. ",
    "url": "/docs/statistics/basics/parametric-test-mean.html#one-sample-t-test",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html#one-sample-t-test"
  },"725": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "Calculation",
    "content": ". | We assume $H_0$ is true and thus $\\mu = m$ | Calculate the sample mean $\\bar{x}$ and subtract $m$ to get $\\bar{x} - \\mu$. | Calculate the sample standard deviation $s$ | Calculate the standard error of the mean $\\text{SEM} = s/\\sqrt{n}$ | Calculate the t-statistic $t = (\\bar{x} - m) / \\text{SEM}$ | Obtain the t-score $t_{\\alpha/2, \\nu}$ (where $\\nu = n - 1$ is the degrees of freedom) | If our $t$ does not fall within the range $-t_{\\alpha/2, \\nu} \\le t \\le t_{\\alpha/2, \\nu}$, we know that $p &lt; \\alpha$ and thus we reject $H_0$. | . ",
    "url": "/docs/statistics/basics/parametric-test-mean.html#calculation",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html#calculation"
  },"726": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "Paired t-Test",
    "content": "Paired t-test is a hypothesis test that compares the means of two populations that are related in some way. Most common case is comparing the data of identical subjects before and after a treatment. This might seem like a new testing method, but it is actually just a special case of one-sample t-test. Instead of setting up a null hypothesis on the population mean, we set up a null hypothesis on the difference between the means before/after. Consider the following table: . | Subject | Before | After | Difference | . | A | 10 | 12 | 2 | . | B | 10 | 14 | 4 | . | C | 12 | 14 | 2 | . | D | 11 | 15 | 4 | . Let $X_0$ be the population of the data before the treatment, and $X_1$ be the population of the data after the treatment. Then our null hypothesis would be on the $\\Delta X = X_1 - X_0$: . $$ H_0: \\mu_{\\Delta X} = 0 $$ . I won’t go into the details of the calculation, but following the same steps as one-sample t-test, you will find that our difference in the table above is significantly different from 0 (with $\\alpha = 0.05$). You may wonder if we can just use a two-sample t-test (described below) to compare the means before/after. However, the problem is that the samples are not independent of each other, and this makes the two-sample t-test more prone to errors. When suitable, paired t-test has more power than two-sample t-test. ",
    "url": "/docs/statistics/basics/parametric-test-mean.html#paired-t-test",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html#paired-t-test"
  },"727": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "Two-Sample t-Test",
    "content": "Two-sample t-test is a hypothesis test that compares the means of two populations. The populations must have normality. Two-sample t-test is also known as unpaired t-test or independent t-test, where the samples are independent of each other. Our null hypothesis would be: . $$ H_0: \\mu_A = \\mu_B $$ . And our alternative hypothesis: . $$ H_A: \\mu_A \\ne \\mu_B $$ . ",
    "url": "/docs/statistics/basics/parametric-test-mean.html#two-sample-t-test",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html#two-sample-t-test"
  },"728": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "Pooled Standard Deviation",
    "content": "Pooled standard deviation is the weighted average of standard deviations of two or more sample groups. Larger samples are given more weight. We typically denote the pooled standard deviation as: . $$ s_p $$ . The unbiased pooled standard deviation for $k$ samples is: . $$ s_p = \\sqrt{\\frac{\\sum_{i=1}^k (n_i - 1) s_i^2}{\\sum_{i=1}^k n_i}} $$ . For equal sample sizes, the pooled standard deviation is simplified to: . $$ s_p = \\sqrt{\\frac{\\sum_{i=1}^k s_i^2}{k}} $$ . If sample size and sample standard deviation are the same for all samples, then the pooled standard deviation is the same as the sample standard deviation. Confirm this by plugging in $s_i = s$ for all $i$. ",
    "url": "/docs/statistics/basics/parametric-test-mean.html#pooled-standard-deviation",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html#pooled-standard-deviation"
  },"729": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "Homogeneity of Variance",
    "content": "When we perform a two-sample t-test, we generally assume that the two populations have the same variance. However, if this assumption cannot be met, we use a modified version of the t-test called Welch’s t-test. Normality is still assumed. ",
    "url": "/docs/statistics/basics/parametric-test-mean.html#homogeneity-of-variance",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html#homogeneity-of-variance"
  },"730": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "t-Statistic for Two Independent Samples",
    "content": "For two-sample t-test where we assume equal population variance, we use the following formula for the t-statistic: . $$ \\begin{equation} \\label{eq:t-two-sample-homovar} t = \\frac{(\\bar{x}_A - \\bar{x}_B) - (\\mu_A - \\mu_B)} {s_p \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}} \\end{equation} $$ . And if we have a similar sample size $n_A = n_B = n$, the formula can be further simplified to: . $$ \\begin{equation} \\label{eq:t-two-sample-simple} t = \\frac{(\\bar{x}_A - \\bar{x}_B) - (\\mu_A - \\mu_B)}{s_p \\sqrt{2/n}} \\end{equation} $$ . ",
    "url": "/docs/statistics/basics/parametric-test-mean.html#t-statistic-for-two-independent-samples",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html#t-statistic-for-two-independent-samples"
  },"731": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "Degrees of Freedom",
    "content": "The degrees of freedom $\\nu$ for two-sample t-test is: . $$ \\nu = n_A + n_B - 2 $$ . Two freedom are lost because we use our sample standard deviations to estimate the population standard deviations (pooled standard deviation). ",
    "url": "/docs/statistics/basics/parametric-test-mean.html#degrees-of-freedom",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html#degrees-of-freedom"
  },"732": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "Calculation",
    "content": ". | We assume $H_0$ is true and thus $\\mu_A - \\mu_B = 0$ | Calculate the sample standard deviation $s_A$ and $s_B$ | Calculate the pooled standard deviation $s_p$ where $k = 2$ | Calculate the t-statistic $t$ using Equation \\eqref{eq:t-two-sample-homovar} with $\\mu_A - \\mu_B = 0$ substituted in | Obtain the t-score $t_{\\alpha/2, \\nu}$ (where $\\nu = n_A + n_B - 2$ is the degrees of freedom) | If our $t$ does not fall within the range $-t_{\\alpha/2, \\nu} \\le t \\le t_{\\alpha/2, \\nu}$, we know that $p &lt; \\alpha$ and thus we reject $H_0$. | . ",
    "url": "/docs/statistics/basics/parametric-test-mean.html#calculation-1",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html#calculation-1"
  },"733": {
    "doc": "Comparing Means in Parametric Tests",
    "title": "Paired t-Test vs. Unpaired t-Test",
    "content": "For a recap, paired t-test is used when we have two samples that are dependent on each other, while unpaired t-test is used when we have two samples that are independent. The following figure illustrates in which situations either test should be used: . When applicable, it is always better to use paired t-test because it has more power. Just because there are two samples that need to be compared, it does not mean two-sample t-test is most appropriate. We should always consider the nature and relationship of the samples before deciding a testing method. ",
    "url": "/docs/statistics/basics/parametric-test-mean.html#paired-t-test-vs-unpaired-t-test",
    "relUrl": "/docs/statistics/basics/parametric-test-mean.html#paired-t-test-vs-unpaired-t-test"
  },"734": {
    "doc": "Parity of a Permutation",
    "title": "Parity of a Permutation",
    "content": ". | Permutation . | Identity permutation | . | Parity | Even and odd permutations | . ",
    "url": "/docs/compsci/math/parity-of-permutation.html",
    "relUrl": "/docs/compsci/math/parity-of-permutation.html"
  },"735": {
    "doc": "Parity of a Permutation",
    "title": "Permutation",
    "content": "Let $X$ be a finite set of $n &gt; 1$ elements. A permutation $\\sigma$ is a bijection from $X$ to $X$. Given a set $\\{a, b, c\\}$, the permutation $\\{b, c, a\\}$ is: . \\[\\begin{gather*} \\sigma(a) = b \\\\ \\sigma(b) = c \\\\ \\sigma(c) = a \\end{gather*}\\] It is often denoted in the following form: . \\[\\sigma = \\begin{pmatrix} a &amp; b &amp; c \\\\ b &amp; c &amp; a \\end{pmatrix}\\] ",
    "url": "/docs/compsci/math/parity-of-permutation.html#permutation",
    "relUrl": "/docs/compsci/math/parity-of-permutation.html#permutation"
  },"736": {
    "doc": "Parity of a Permutation",
    "title": "Identity permutation",
    "content": "The identity permutation is the permutation that maps every element to itself. \\[\\begin{pmatrix} a &amp; b &amp; c \\\\ a &amp; b &amp; c \\end{pmatrix}\\] ",
    "url": "/docs/compsci/math/parity-of-permutation.html#identity-permutation",
    "relUrl": "/docs/compsci/math/parity-of-permutation.html#identity-permutation"
  },"737": {
    "doc": "Parity of a Permutation",
    "title": "Parity",
    "content": "The parity of a permutation $\\sigma$ is the parity of the number of paired inversions in $\\sigma$ required to transform it into the identity permutation. The parity or the sign of a permutation $\\sigma$ is denoted: . $$ \\operatorname{sgn}(\\sigma) = (-1)^{\\text{number of inversions}} $$ . Continuing with the example above, the number of inversions need for . \\[\\begin{pmatrix} a &amp; b &amp; c \\\\ b &amp; c &amp; a \\end{pmatrix}\\] would be 2: . | Invert $a$ and $c$ | Invert $a$ and $b$ | . Therefore, the parity of the permutation is $(-1)^2 = 1$. ",
    "url": "/docs/compsci/math/parity-of-permutation.html#parity",
    "relUrl": "/docs/compsci/math/parity-of-permutation.html#parity"
  },"738": {
    "doc": "Parity of a Permutation",
    "title": "Even and odd permutations",
    "content": "The set of permutations of $X$ can be partitioned into two subsets: . | Even permutations: $\\operatorname{sgn}(\\sigma) = 1$ | Odd permutations: $\\operatorname{sgn}(\\sigma) = -1$ | . ",
    "url": "/docs/compsci/math/parity-of-permutation.html#even-and-odd-permutations",
    "relUrl": "/docs/compsci/math/parity-of-permutation.html#even-and-odd-permutations"
  },"739": {
    "doc": "Pipenv",
    "title": "Pipenv",
    "content": ". | Advantage to venv | Basic usage . | Install | Create environment and install | Activate and deactivate environment . | Activate | Deactivate | . | Create Pipfile.lock | Delete environment | . | Use pipenv with a specific Python version | . ",
    "url": "/docs/python/envs/pipenv.html",
    "relUrl": "/docs/python/envs/pipenv.html"
  },"740": {
    "doc": "Pipenv",
    "title": "Advantage to venv",
    "content": "Pipenv’s Pipfile serves as an upgrade to requirements.txt. It allows you to separately mark production and development dependencies. ",
    "url": "/docs/python/envs/pipenv.html#advantage-to-venv",
    "relUrl": "/docs/python/envs/pipenv.html#advantage-to-venv"
  },"741": {
    "doc": "Pipenv",
    "title": "Basic usage",
    "content": "Install . brew install pipenv . Create environment and install . Go to the desired project folder. To create and use a virtual environment for this root: . pipenv pipenv install &lt;package-name&gt; # Install specific package . If there’s already a Pipfile, create env and install listed requirements: . pipenv install # With Pipfile . All the pipenv environments are in ~/.local/share/virtualenvs/root-dir-name-hash/ by default. Activate and deactivate environment . Activate . pipenv shell . Deactivate . exit . Do not use deactivate. Create Pipfile.lock . pipenv lock . Delete environment . To delete the environment for current directory: . pipenv --rm . ",
    "url": "/docs/python/envs/pipenv.html#basic-usage",
    "relUrl": "/docs/python/envs/pipenv.html#basic-usage"
  },"742": {
    "doc": "Pipenv",
    "title": "Use pipenv with a specific Python version",
    "content": "You can set a specific version of Python when creating a pipenv virtual environment. pipenv --python 3.x . However, it requires that Python 3.x is already installed on your local machine unlike conda create -n myenv python=3.x. You can either, . | (Not recommended) brew install the desired Python 3.x | (Recommended) Use pyenv | (Meh..) Create a conda environment with the desired version and only use the binary | . Then navigate to the root of the project and make Pipenv use the active Python: . pipenv --python=$(which python) --site-packages # Creates an env in cwd pipenv run which python # It will point to a binary in `~/.local/share/virtualenvs/some-root-dir-hash/bin/python` pipenv run python -V # Check that it is indeed 3.x . ",
    "url": "/docs/python/envs/pipenv.html#use-pipenv-with-a-specific-python-version",
    "relUrl": "/docs/python/envs/pipenv.html#use-pipenv-with-a-specific-python-version"
  },"743": {
    "doc": "Poetry",
    "title": "Poetry",
    "content": "Yet another Python virtual environment &amp; package manager! . | Installation | Enable tab completion | Using Poetry with a specific Python version . | Set Python binary | . | Managing environments . | See all virtual envs associated with this directory/project | Delete environments | . | Basic usage . | Activate environment | Deactivate environment | Add dependencies | Install dependencies | Remove dependencies | . | . ",
    "url": "/docs/python/envs/poetry.html",
    "relUrl": "/docs/python/envs/poetry.html"
  },"744": {
    "doc": "Poetry",
    "title": "Installation",
    "content": "Use Homebrew: . brew install poetry . Check installation via . poetry --version . ",
    "url": "/docs/python/envs/poetry.html#installation",
    "relUrl": "/docs/python/envs/poetry.html#installation"
  },"745": {
    "doc": "Poetry",
    "title": "Enable tab completion",
    "content": "You can enable poetry tab completion for various shells. Check poetry help completions for other shells. For Oh-My-Zsh: . mkdir $ZSH_CUSTOM/plugins/poetry poetry completions zsh &gt; $ZSH_CUSTOM/plugins/poetry/_poetry . Then go to ~/.zshrc and add the following plugin: . # ~/.zshrc plugins( ... poetry ) . ",
    "url": "/docs/python/envs/poetry.html#enable-tab-completion",
    "relUrl": "/docs/python/envs/poetry.html#enable-tab-completion"
  },"746": {
    "doc": "Poetry",
    "title": "Using Poetry with a specific Python version",
    "content": "Have the Python version you want ready. Always check that you do indeed have the version you want by which python . You can set the version you want . | With pyenv (Recommended) | With conda | . Now init Poetry in project to create a pyproject.toml file. cd &lt;project-dir&gt; poetry init . By default, Poetry uses the currently active Python. Poetry virtual environments are created in ~/Library/Caches/pypoetry/virtualenvs. Set Python binary . If you’d like to change a version after init, activate a new Python binary and do: . poetry env use `pyenv which python3` . Check that it is using the right binary by: . poetry env info . ",
    "url": "/docs/python/envs/poetry.html#using-poetry-with-a-specific-python-version",
    "relUrl": "/docs/python/envs/poetry.html#using-poetry-with-a-specific-python-version"
  },"747": {
    "doc": "Poetry",
    "title": "Managing environments",
    "content": "See all virtual envs associated with this directory/project . poetry env list . Delete environments . poetry env remove &lt;proj-hash--py3.x&gt; ## Check exact name with poetry env list . ",
    "url": "/docs/python/envs/poetry.html#managing-environments",
    "relUrl": "/docs/python/envs/poetry.html#managing-environments"
  },"748": {
    "doc": "Poetry",
    "title": "Basic usage",
    "content": "Activate environment . poetry shell # Creates a new child shell # OR source `poetry env info --path`/bin/activate # Does not open a child shell . Deactivate environment . exit # If in child shell # OR deactivate # If activated with source &lt;path&gt;/bin/activate . Add dependencies . poetry add &lt;package&gt; # OR poetry add &lt;package&gt; --dev . Install dependencies . poetry install # OR poetry install --no-dev . Remove dependencies . poetry remove &lt;package&gt; # OR poetry remove &lt;package&gt; --dev . References: . | Poetry | Poetry Commands | . ",
    "url": "/docs/python/envs/poetry.html#basic-usage",
    "relUrl": "/docs/python/envs/poetry.html#basic-usage"
  },"749": {
    "doc": "Population and Sample",
    "title": "Population and Sample",
    "content": ". | Selecting the goal and target of analysis | Population . | Finite population | Infinite population | . | Finding the characteristics of the population . | Complete enumeration | Sample survey . | Sample size | . | . | . ",
    "url": "/docs/statistics/basics/population.html",
    "relUrl": "/docs/statistics/basics/population.html"
  },"750": {
    "doc": "Population and Sample",
    "title": "Selecting the goal and target of analysis",
    "content": "Before you start analyzing any data, it is important to clearly define the goal and target of analysis. As described in the introduction, the goal may be to understand the data or to predict the data. Once the goal is set, you need to define the target of analysis. If the goal is to confirm the effect of a certain treatment, then the target of analysis would be the entire population of people who have the related disease. ",
    "url": "/docs/statistics/basics/population.html#selecting-the-goal-and-target-of-analysis",
    "relUrl": "/docs/statistics/basics/population.html#selecting-the-goal-and-target-of-analysis"
  },"751": {
    "doc": "Population and Sample",
    "title": "Population",
    "content": "The target of the analysis is called the population. The number of elements in the population is called the population size. There are two types of population depending on the size: . ",
    "url": "/docs/statistics/basics/population.html#population",
    "relUrl": "/docs/statistics/basics/population.html#population"
  },"752": {
    "doc": "Population and Sample",
    "title": "Finite population",
    "content": "No matter how large the population is, if the cardinality of the population can be fully enumerated, then it is a finite population. ",
    "url": "/docs/statistics/basics/population.html#finite-population",
    "relUrl": "/docs/statistics/basics/population.html#finite-population"
  },"753": {
    "doc": "Population and Sample",
    "title": "Infinite population",
    "content": "If the cardinality of the population is infinite, then it is an infinite population. Population that changes over time is also considered an infinite population. ",
    "url": "/docs/statistics/basics/population.html#infinite-population",
    "relUrl": "/docs/statistics/basics/population.html#infinite-population"
  },"754": {
    "doc": "Population and Sample",
    "title": "Finding the characteristics of the population",
    "content": "If we know the characteristics of the population, then it becomes easier for us to predict the data. Then how would we know the characteristics of the population? . ",
    "url": "/docs/statistics/basics/population.html#finding-the-characteristics-of-the-population",
    "relUrl": "/docs/statistics/basics/population.html#finding-the-characteristics-of-the-population"
  },"755": {
    "doc": "Population and Sample",
    "title": "Complete enumeration",
    "content": "This is a survey you can perform on a finite population. If we can enumerate the entire population, then we can know the characteristics of the population. Because the number data you analyze equals the population size, performing descriptive statistics is enough to understand the data. However, in most cases, complete enumeration is not realistic due to the cost and time required. Also, if the population is infinite, then complete enumeration is obviously impossible. ",
    "url": "/docs/statistics/basics/population.html#complete-enumeration",
    "relUrl": "/docs/statistics/basics/population.html#complete-enumeration"
  },"756": {
    "doc": "Population and Sample",
    "title": "Sample survey",
    "content": "If complete enumeration is not possible, then we need to sample the population. Sampling is the process of selecting a subset of the population. Inferential statistics will be used to infer the characteristics of the population from the sample. Sample size . The number of elements in the sample is called the sample size. The sample size is usually denoted by: . $$ n $$ . It is important to differentiate number of samples from sample size. Number of samples is the number of times you perform the sampling process, while sample size is the number of elements in the sample. ",
    "url": "/docs/statistics/basics/population.html#sample-survey",
    "relUrl": "/docs/statistics/basics/population.html#sample-survey"
  },"757": {
    "doc": "Post-Hoc Test",
    "title": "Post-Hoc Test",
    "content": "As the name suggests, post-hoc test is performed after a test to correct error rates and assess the significance of the results. | Error Rates in Multiple Comparison . | Family-Wise Error Rate | False Discovery Rate | Positive False Discovery Rate | . | Bonferroni Correction | Holm’s Method | q-Value | Benjamini-Hochberg Procedure . | Comparison to controlling FWER | . | Tukey’s HSD Test . | Studentized Range Distribution | HSD | . | Dunnett’s Test | Williams Test | . ",
    "url": "/docs/statistics/basics/post-hoc-test.html",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html"
  },"758": {
    "doc": "Post-Hoc Test",
    "title": "Error Rates in Multiple Comparison",
    "content": "Suppose we are testing a family of $m$ hypotheses. Let’s denote: . |   | $H_0$ is true | $H_A$ is true |   | . | Test declared insignificant | $U$ | $T$ | $m - R$ | . | Test declared significant | $V$ | $S$ | $R$ | . |   | $m_0$ | $m - m_0$ | $m$ | . where each cell represents the number of hypotheses: . | $m_0$: the number of true null hypotheses | $R$: the number of rejected null hypotheses | $U$: the number of true negatives | $T$: the number of false negatives | $V$: the number of false positives | $S$: the number of true positives | . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#error-rates-in-multiple-comparison",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#error-rates-in-multiple-comparison"
  },"759": {
    "doc": "Post-Hoc Test",
    "title": "Family-Wise Error Rate",
    "content": "Family-wise error rate (FWER) is the probability of making at least one Type I error in a family of tests. $$ \\text{FWER} = P(\\text{at least one Type I error}) = P(V \\geq 1) $$ . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#family-wise-error-rate",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#family-wise-error-rate"
  },"760": {
    "doc": "Post-Hoc Test",
    "title": "False Discovery Rate",
    "content": "False discovery rate (FDR) is the expected proportion of false positives among all rejected hypotheses. Simply: . $$ \\text{FDR} = E\\left[\\frac{V}{R}\\right] $$ . What if R = 0? Formally, . $$ \\text{FDR} = E\\left[\\frac{V}{R} \\mid R &gt; 0\\right] \\cdot P(R &gt; 0) $$ . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#false-discovery-rate",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#false-discovery-rate"
  },"761": {
    "doc": "Post-Hoc Test",
    "title": "Positive False Discovery Rate",
    "content": "To be added . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#positive-false-discovery-rate",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#positive-false-discovery-rate"
  },"762": {
    "doc": "Post-Hoc Test",
    "title": "Bonferroni Correction",
    "content": "As we’ve seen in the multiple comparisons problem, repeating the a pairwise test multiple times increases the FWER. The Bonferroni correction is a method to correct the FWER to $\\alpha$ by rejecting the null hypothesis less frequently in each test. The idea is simple, which is to correct each significance level to: . $$ \\alpha' = \\frac{\\alpha}{m} $$ . where $m$ is the number of pairwise tests. Bonferroni correction has pros: . | It is simple | It can be used in any test that produces a p-value | . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#bonferroni-correction",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#bonferroni-correction"
  },"763": {
    "doc": "Post-Hoc Test",
    "title": "Issues with Bonferroni Correction",
    "content": "The problem with the Bonferroni correction is that it is too conservative. Although it corrects the FWER to approximately $\\alpha$, the power of each test is reduced significantly as the number of tests increases. Suppose we have $\\alpha = 0.05$ and we perform 10 pairwise tests. Then our Bonferroni-corrected $\\alpha’$ for each test is 0.005, and this gets worse as the number of tests increases. It gets harder and harder to call any result significant. ",
    "url": "/docs/statistics/basics/post-hoc-test.html#issues-with-bonferroni-correction",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#issues-with-bonferroni-correction"
  },"764": {
    "doc": "Post-Hoc Test",
    "title": "Holm’s Method",
    "content": "Just like Bonferroni, Holm’s method tries to correct the Type I error rate by controlling the FWER. ",
    "url": "/docs/statistics/basics/post-hoc-test.html#holms-method",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#holms-method"
  },"765": {
    "doc": "Post-Hoc Test",
    "title": "Procedure",
    "content": "Let’s say we have $m$ hypotheses to test. Notice the difference in notation between $p_i$ and $p_{(j)}$. $p_i$ is the p-value of $H_i$, while $p_{(j)}$ is the $j$-th smallest p-value among $p_i$. | Calculate the p-value $p_i$ for each test ($1 \\leq i \\leq m$) | Sort the p-values in ascending order: $p_{(1)} \\leq p_{(2)} \\leq \\dots \\leq p_{(m)}$ | Calculate lowerbound rank $L$: $$ L = \\min\\left\\{j \\in \\{1, \\dots, m\\} \\mid p_{(j)} &gt; \\frac{\\alpha}{m + 1 - j}\\right\\} $$ . | Reject all null hypotheses with $p_i &lt; p_{(L)}$ | . Alternatively . | Calculate the p-value $p_i$ for each test ($1 \\leq i \\leq m$) | Sort the p-values in ascending order: $p_{(1)} \\leq p_{(2)} \\leq \\dots \\leq p_{(m)}$ | Reject all $p_{(j)}$ such that $$ p_{(j)} &lt; \\frac{\\alpha}{m + 1 - j} $$ . | . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#procedure",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#procedure"
  },"766": {
    "doc": "Post-Hoc Test",
    "title": "Comparison to Bonferroni",
    "content": "Both Bonferroni and Holm’s method control the FWER to $\\alpha$. However Holm’s method is at least as powerful as Bonferroni correction, and thus less conservative. Anything Bonferroni rejects, Holm’s method will also reject. Proof Suppose Bonferroni rejects $H_{0i}$ for some $i \\in [1, m]$. Then $p_i &lt; \\frac{\\alpha}{m}$. Because . \\[\\forall j \\in [1, m],\\; \\frac{\\alpha}{m} \\leq \\frac{\\alpha}{m + 1 - j}\\] We have $p_i &lt; \\frac{\\alpha}{m + 1 - L} &lt; p_{(L)}$ by definition. So Holm’s method will also reject $H_{0i}$. ",
    "url": "/docs/statistics/basics/post-hoc-test.html#comparison-to-bonferroni",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#comparison-to-bonferroni"
  },"767": {
    "doc": "Post-Hoc Test",
    "title": "q-Value",
    "content": "Remember that running multiple tests suffers increased overall Type I error. So we use correction methods on the p-value to stabilize the FWER. However, controlling the FWER to correct type I error often becomes too conservative. So we look for other methods to correct Type I errors. Instead of trying to control the FWER, we can try to control the false discovery rate (FDR), and use a q-value which is the FDR analogue of the p-value. For a recap, the FDR is the expected proportion of false positives among all rejected null hypotheses. So compared to FWER which measures the chance of making any false positives, FDR is more lenient. The q-value of a test is the minimum FDR at which the test may be called significant. ",
    "url": "/docs/statistics/basics/post-hoc-test.html#q-value",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#q-value"
  },"768": {
    "doc": "Post-Hoc Test",
    "title": "Benjamini-Hochberg Procedure",
    "content": "Benjamini-Hochberg procedure is a method to control the FDR. If we set a threshold $q$ and follow the procedure, $\\text{FDR} \\leq q$. ",
    "url": "/docs/statistics/basics/post-hoc-test.html#benjamini-hochberg-procedure",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#benjamini-hochberg-procedure"
  },"769": {
    "doc": "Post-Hoc Test",
    "title": "Procedure",
    "content": ". | Calculate the p-value $p_i$ for each test ($1 \\leq i \\leq m$) | Sort the p-values in ascending order: $p_{(1)} \\leq p_{(2)} \\leq \\dots \\leq p_{(m)}$ | Calculate the rank $L$ $$ \\begin{equation*} \\label{eq:bh_rank} \\tag{Maximum Rank} L = \\max\\left\\{j \\in \\{1, \\dots, m\\} \\mid p_{(j)} &lt; \\frac{q}{m}\\cdot j\\right\\} \\end{equation*} $$ . | Reject all null hypotheses with $p_i &lt; p_{(L)}$. | . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#procedure-1",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#procedure-1"
  },"770": {
    "doc": "Post-Hoc Test",
    "title": "Comparison to controlling FWER",
    "content": "The slope of Benjamini-Hochberge threshold is defined by $q/m$ (see \\ref{eq:bh_rank}). The p-values below each line are the rejected ones (discoveries). From the graph, we see that Benjamini-Hochberg procedure makes more discoveries than well-known methods that control FWER. ",
    "url": "/docs/statistics/basics/post-hoc-test.html#comparison-to-controlling-fwer",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#comparison-to-controlling-fwer"
  },"771": {
    "doc": "Post-Hoc Test",
    "title": "Tukey’s HSD Test",
    "content": "HSD stands for honestly significant difference, which is the minimum difference between two means that is considered (actually/honestly) statistically significant. So Tukey’s HSD test is a method that calculates the HSD, and checks which pair has a significant difference (i.e. greater than HSD). It works better than the Bonferroni correction in that it maintains the FWER at $\\alpha$ but without losing as much power. Typically, Tukey’s HSD test is used after ANOVA. However, it can be used standalone as well. The null hypothesis of Tukey’s HSD test is: . $$ H_0: \\forall i, j \\in [1, k],\\; \\mu_i = \\mu_j $$ . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#tukeys-hsd-test",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#tukeys-hsd-test"
  },"772": {
    "doc": "Post-Hoc Test",
    "title": "Assumptions",
    "content": "Tukey’s test assumes the following: . | The observations are independent (within and among) | Each group follows a normal distribution | Homogeneity of variance in each group | The sample sizes are equal ($n_k = n$) If the sample sizes are unequal, Tukey's test becomes more conservative. | . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#assumptions",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#assumptions"
  },"773": {
    "doc": "Post-Hoc Test",
    "title": "Studentized Range Distribution",
    "content": "Suppose we have $k$ populations with normal distribution, and we each take a sample of size $n$. Let $\\bar{x}_{min}$ and $\\bar{x}_{max}$ be the smallest and largest sample means respectively, and $s_p$ the pooled standard deviation (with equal sample sizes). The following random variable has a Studentized range distribution: . $$ Q = \\frac{\\bar{x}_{max} - \\bar{x}_{min}}{s_p / \\sqrt{n}} $$ . Largest difference between two sample means measured by the standard error. ",
    "url": "/docs/statistics/basics/post-hoc-test.html#studentized-range-distribution",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#studentized-range-distribution"
  },"774": {
    "doc": "Post-Hoc Test",
    "title": "HSD",
    "content": "Test Statistic . We use the following statistic to measure the difference in means: . $$ \\frac{\\mid\\bar{x}_i - \\bar{x}_j\\mid}{\\sqrt{MSE / n}} $$ . where $\\bar{x}_i$ and $\\bar{x}_j$ are the sample means of group $i$ and $j$ respectively. When null hypothesis is true, the statistic follows the studentized range distribution. Critical Value of Studentized Range Distribution . We denote the critical value as . $$ q_{\\alpha, \\nu, k} $$ . where: . | $\\alpha$ is the significance level | $\\nu$ is the degrees of freedom of the distribution | $k$ is the number of groups | . We then compare our statistic to a critical value of the distribution. If the statistic is greater than the critical value, we reject the null hypothesis. The Studentized range distribution is measured on the largest difference, so it makes sense that any difference measure greater than the critical value should be considered significant. Just like z-scores and t-scores, the values can be obtained from a table. Defining HSD . For our differences to be significant, we need the following to be true: . \\[\\frac{\\mid\\bar{x}_i - \\bar{x}_j\\mid}{\\sqrt{MSE / n}} \\geq q_{\\alpha, \\nu, k}\\] We can rearrange the inequality to get the following: . \\[\\mid\\bar{x}_i - \\bar{x}_j\\mid \\geq q_{\\alpha, \\nu, k} \\cdot \\sqrt{\\frac{MSE}{n}}\\] The right-hand side of the inequality is then defined HSD: . $$ HSD = q_{\\alpha, \\nu, k} \\cdot \\sqrt{\\frac{MSE}{n}} $$ . where: . | $q_{\\alpha, \\nu, N}$ is the critical value of the studentized range distribution with $\\alpha$ significance level, $\\nu$ degrees of freedom, and $k$ groups . | Standard error of the group mean: . | $MSE$ is the mean square errror (or within), which you can get from the ANOVA summary table | $n$ is the sample size of each group (which is assumed to be all equal) | . | . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#hsd",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#hsd"
  },"775": {
    "doc": "Post-Hoc Test",
    "title": "Dunnett’s Test",
    "content": "To be added . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#dunnetts-test",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#dunnetts-test"
  },"776": {
    "doc": "Post-Hoc Test",
    "title": "Williams Test",
    "content": "To be added . ",
    "url": "/docs/statistics/basics/post-hoc-test.html#williams-test",
    "relUrl": "/docs/statistics/basics/post-hoc-test.html#williams-test"
  },"777": {
    "doc": "pyenv",
    "title": "pyenv",
    "content": "Python version manager . GitHub . | What is pyenv | Installation | Typical usage | Basic commands . | Check activated Python version | List installed Python versions | List all available Python for install | Install a Python version | Uninstall a Python version | Show installed directory | Set Python version | Show Python binary | . | Uninstall pyenv . | Remove all shell startup configuration | Remove all Python versions | Remove pyenv | . | . ",
    "url": "/docs/python/envs/pyenv.html",
    "relUrl": "/docs/python/envs/pyenv.html"
  },"778": {
    "doc": "pyenv",
    "title": "What is pyenv",
    "content": "pyenv is a version manager for Python. As it started off as a fork of rbenv, the syntax and usage are very similar. It is a Python version manager not a virtual environment manager. To manage a virtual environment for Python libraries, use in junction with venv, poetry, pipenv, etc. ",
    "url": "/docs/python/envs/pyenv.html#what-is-pyenv",
    "relUrl": "/docs/python/envs/pyenv.html#what-is-pyenv"
  },"779": {
    "doc": "pyenv",
    "title": "Installation",
    "content": "Easiest way is to use Homebrew: . brew install pyenv . Then, . echo 'eval \"$(pyenv init --path)\"' &gt;&gt; ~/.zprofile echo 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.zshrc . or . echo 'eval \"$(pyenv init --path)\"' &gt;&gt; ~/.zshrc . If using .zprofile, Terminal app should open shell as login shell. Restart shell and install Python build dependencies: . brew install openssl readline sqlite3 xz zlib . ",
    "url": "/docs/python/envs/pyenv.html#installation",
    "relUrl": "/docs/python/envs/pyenv.html#installation"
  },"780": {
    "doc": "pyenv",
    "title": "Typical usage",
    "content": "To install a specific Python version for a project, navigate to your project root and do: . pyenv intall -l # Decide a version number pyenv install -s 3.x.x # -s means skip installation if it already exists pyenv rehash # Makes all Python binaries available to system pyenv local 3.x.x # Make sure you're in project root . ",
    "url": "/docs/python/envs/pyenv.html#typical-usage",
    "relUrl": "/docs/python/envs/pyenv.html#typical-usage"
  },"781": {
    "doc": "pyenv",
    "title": "Basic commands",
    "content": "Full commands are listed here . Check activated Python version . pyenv version . Do not confuse with below. Notice the plural. List installed Python versions . pyenv versions . List all available Python for install . pyenv install -l . Install a Python version . pyenv install 3.x.x pyenv rehash . Uninstall a Python version . pyenv uninstall 3.x.x . Show installed directory . pyenv prefix 3.x.x . Set Python version . pyenv global 3.x.x pyenv local 3.x.x . Show Python binary . pyenv which python3 . ",
    "url": "/docs/python/envs/pyenv.html#basic-commands",
    "relUrl": "/docs/python/envs/pyenv.html#basic-commands"
  },"782": {
    "doc": "pyenv",
    "title": "Uninstall pyenv",
    "content": "Remove all shell startup configuration . Remove the following from .zprofile and .zshrc: . echo 'eval \"$(pyenv init --path)\"' echo 'eval \"$(pyenv init -)\"' . Remove all Python versions . rm -rf $(pyenv root) . Remove pyenv . brew uninstall pyenv . ",
    "url": "/docs/python/envs/pyenv.html#uninstall-pyenv",
    "relUrl": "/docs/python/envs/pyenv.html#uninstall-pyenv"
  },"783": {
    "doc": "Vue Quick Notes",
    "title": "Vue Quick Notes",
    "content": "Quick notes for dummies. Using &lt;script setup lang=\"ts\"&gt;. | Props . | To use props in child component . | Without default values | With default values | . | . | Ref / Reactive | Computed / Watch | Event / Emits . | To use emits | . | v-model . | Parent-Child usage example | . | Provide / Inject | Etc . | $keyword equivalent in the script tag | . | . ",
    "url": "/docs/vue/quick-notes.html",
    "relUrl": "/docs/vue/quick-notes.html"
  },"784": {
    "doc": "Vue Quick Notes",
    "title": "Props",
    "content": ". | Props are reactive by default | You don’t explicitly import defineProps; it is a compiler macro for &lt;script setup&gt; | If you don’t pass optional props, they have a value of undefined | In the template, you can access the props without having to do props.someVarName, just use someVarName. | Even if you toRef a prop, they will not become a copy. If you modify the toRef-ed prop, it will affect the original. | . To use props in child component . Without default values . const props = defineProps&lt;{ a: string b: number }&gt;() . With default values . interface MyProps { a: strings b?: number } const props = withDefaults(defineProps&lt;MyProps&gt;(), { b: 0 }) . ",
    "url": "/docs/vue/quick-notes.html#props",
    "relUrl": "/docs/vue/quick-notes.html#props"
  },"785": {
    "doc": "Vue Quick Notes",
    "title": "Ref / Reactive",
    "content": ". | You can use ref with primitive types like string and number but not with reactive | You access refs by refObj.value and reactives by reactiveObj | Everything that belonged in the data part before the Composition API should be wrapped with ref or reactive. (Unless they’re nonchanging in value.) | . ",
    "url": "/docs/vue/quick-notes.html#ref--reactive",
    "relUrl": "/docs/vue/quick-notes.html#ref--reactive"
  },"786": {
    "doc": "Vue Quick Notes",
    "title": "Computed / Watch",
    "content": "See details here. | watch watches for a specific set of changes, and runs a function. | With watch you can also get the prev and new value. | You can watch ref like a normal variable, but you gotta use () =&gt; reactiveObj instead for reactive objects. | watchEffect watches for all change in every variable used in its function. | watchEffect is kinda more like computed except it’s purpose is not to to get or set a variable. | computed with a single arrow function creates a getter (so immutable). If you give it instead an object with get and set, it will be writable. | If you store watch and watchEffect in a variable named myVar for example, you can stop its watch behavior by calling myVar(). | There exists, onTrack and onTrigger for debugging. | . ",
    "url": "/docs/vue/quick-notes.html#computed--watch",
    "relUrl": "/docs/vue/quick-notes.html#computed--watch"
  },"787": {
    "doc": "Vue Quick Notes",
    "title": "Event / Emits",
    "content": "To use emits . const emits = defineEmits&lt;{ (e: 'myEvent', valueImGivingBack: string): void }&gt;( // Then later function onEvent(e: Event) { const newVal = (e.target as HTMLTextAreaElement).value emits('myEvent', newVal) } . ",
    "url": "/docs/vue/quick-notes.html#event--emits",
    "relUrl": "/docs/vue/quick-notes.html#event--emits"
  },"788": {
    "doc": "Vue Quick Notes",
    "title": "v-model",
    "content": ". | Syntax changed since Vue2, see here for details. | v-model is a syntactic sugar: you can always do the same thing with regular propping and emitting. | Basically, if you use the vanilla v-model as is, the name of the prop and the event will have to be modelValue and update:modelValue. | If you give it a name like v-model:childProp, it will be childProp and update:childProp. | You can use multiple v-models with a child component; just give it different names. | Remember, v-model needs to be used with ref or reactive variable | . Parent-Child usage example . // Parent.vue &lt;template&gt; &lt;Child v-model:childProp=\"parentVar\"&gt; &lt;/template&gt; &lt;script setup lang=\"ts\"&gt; const parentVar = ref('hey child') &lt;/script&gt; . // Child.vue &lt;template&gt; &lt;input @keyup.enter=\"onEnterPressed\"&gt; &lt;/template&gt; &lt;script setup lang=\"ts\"&gt; const defineProps&lt;{ childProp: string }&gt;() const emits = defineEmits&lt;{ (e: 'update:childProp', childProp: string): void }&gt;() function onEnterPressed(e: Event) { const someVal = 'sup' emits('update:childProp', someVal) } &lt;/script&gt; . ",
    "url": "/docs/vue/quick-notes.html#v-model",
    "relUrl": "/docs/vue/quick-notes.html#v-model"
  },"789": {
    "doc": "Vue Quick Notes",
    "title": "Provide / Inject",
    "content": ". | To make typing work, you gotta use the InjectionKey&lt;T&gt;. See here for details. | To update provided reactive props, make the parent component provide mutation functions as well. Always recommended to have the root (providing) component to be in charge of mutations. | . ",
    "url": "/docs/vue/quick-notes.html#provide--inject",
    "relUrl": "/docs/vue/quick-notes.html#provide--inject"
  },"790": {
    "doc": "Vue Quick Notes",
    "title": "Etc",
    "content": ". | You can use $event, $router, $route, $slots, $attrs, $emit in the template tag, but not in the script tag. | ref, reactive, toRef, toRefs, computed, watch, watchEffect are all in vue | attrs are basically all the stuff passed down to a child naturally from being an HTML element, but not actually a Vue prop. Ex) class | You can use the normal &lt;script&gt; tag along with the &lt;script setup&gt;. Two things you’ll have to do within the normal &lt;script&gt; tag is setting name and inheritAttrs. | . $keyword equivalent in the script tag . See here for details. But basically: . import { useSlots, useAttrs } from 'vue import { useRouter, useRoute } from 'vue-router' const slots = useSlots() const attrs = useAttrs() const router = useRouter() const route = useRoute() . ",
    "url": "/docs/vue/quick-notes.html#etc",
    "relUrl": "/docs/vue/quick-notes.html#etc"
  },"791": {
    "doc": "R-Squared",
    "title": "R-Squared",
    "content": ". | Coefficient of Determination $R^2$ . | Explained variation | Interpretation | . | Adjusted $R^2$ . | The issue with regular $R^2$ | Adjustment | . | . ",
    "url": "/docs/statistics/notes/r-squared.html",
    "relUrl": "/docs/statistics/notes/r-squared.html"
  },"792": {
    "doc": "R-Squared",
    "title": "Coefficient of Determination $R^2$",
    "content": "Coefficient of determination ($R^2$) is a statistical measure of how well the model captures the variation in the dependent variable. So essentially, goodness of fit. To understand $R^2$, we need to refer back to sum of squares. ",
    "url": "/docs/statistics/notes/r-squared.html#coefficient-of-determination-r2",
    "relUrl": "/docs/statistics/notes/r-squared.html#coefficient-of-determination-r2"
  },"793": {
    "doc": "R-Squared",
    "title": "Explained variation",
    "content": "In cases like linear regression with OLS, we have seen that the following decomposition is true: . \\[SS_{tot} = SS_{exp} + SS_{res}\\] Since we want to know how much of the variation is explained by the model, we solve for the proportion of the explained variation among the total variation. \\[\\frac{SS_{exp}}{SS_{tot}} = \\frac{SS_{tot} - SS_{res}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}}\\] This is the definition of $R^2$. $$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$ . ",
    "url": "/docs/statistics/notes/r-squared.html#explained-variation",
    "relUrl": "/docs/statistics/notes/r-squared.html#explained-variation"
  },"794": {
    "doc": "R-Squared",
    "title": "Interpretation",
    "content": "If the model captures all the variation in the dependent variable, the variation caused by error ($SS_{res}$) is zero, which would make $R^2$ equal to 1. This indicates that the model perfectly fits the data. The baseline model, which is just the mean of the dependent variable, results in $R^2$ equal to 0. Any model that performs worse than the baseline model will have a negative $R^2$. $$ \\begin{gather*} R^2 = 1 \\Rightarrow \\text{perfect fit} \\\\ R^2 = 0 \\Rightarrow \\text{baseline model} \\\\ R^2 &lt; 0 \\Rightarrow \\text{worse than baseline model} \\end{gather*} $$ . So generally, the higher the $R^2$ the better the model fits the data. Anything below 0 means you should really reconsider your model or check if you have a mistake, because you’re doing worse than the bare minimum which is just always predicting the mean. Just because a model has a high $R^2$ doesn’t mean it’s a good model. $R^2$ is not a good measure for non-linear models, because the sum of squares decomposition doesn’t hold for them. ",
    "url": "/docs/statistics/notes/r-squared.html#interpretation",
    "relUrl": "/docs/statistics/notes/r-squared.html#interpretation"
  },"795": {
    "doc": "R-Squared",
    "title": "Adjusted $R^2$",
    "content": " ",
    "url": "/docs/statistics/notes/r-squared.html#adjusted-r2",
    "relUrl": "/docs/statistics/notes/r-squared.html#adjusted-r2"
  },"796": {
    "doc": "R-Squared",
    "title": "The issue with regular $R^2$",
    "content": "When you add more predictors/features/independent variables to your model, $R^2$ will always increase. This is because as your model gets more complex, the $SS_{tot}$ stays the same, . Remember $SS_{tot}$ has nothing to do with the model, but only the data. but $SS_{res}$ can only decrease (to be more precise, it does not increase). Intuition Think of what it means to increase the complexity of the model. You had just a rigid line to fit your model before, but now you’ve added some features in so that it’s more flexible to fit a more complex curve. You should have been able to decrease your error squares, so $SS_{res}$ should have decreased. This results in multiple issues: . | $R^2$ is a positively biased estimator (always overshoots) | Bias towards complex models | Overfitting | . ",
    "url": "/docs/statistics/notes/r-squared.html#the-issue-with-regular-r2",
    "relUrl": "/docs/statistics/notes/r-squared.html#the-issue-with-regular-r2"
  },"797": {
    "doc": "R-Squared",
    "title": "Adjustment",
    "content": "To account for the bias, we penalize the $R^2$ by the number of predictors $k$ used in the model. The penalty is defined as: . \\[\\frac{n - 1}{n - k - 1}\\] where $n$ is the sample size. Notice that the penalty is 1 when $k = 0$, and it increases as $k$ increases. Remembering that $R^2$ is defined as: . \\[R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\\] We can penalize $R^2$ by bumping up the subtracted term with the penalty: . \\[\\frac{SS_{res}}{SS_{tot}} \\times \\frac{n-1}{n-k-1}\\] Through substitution, we get the definition for adjusted $R^2$: . $$ R^2_{adj} = 1 - (1 - R^2) \\cdot \\frac{n-1}{n-k-1} $$ . ",
    "url": "/docs/statistics/notes/r-squared.html#adjustment",
    "relUrl": "/docs/statistics/notes/r-squared.html#adjustment"
  },"798": {
    "doc": "10 - Regular Expression Matching - Hard",
    "title": "Regular Expression Matching",
    "content": ". | Problem | Using recursion . | Recursive solution | . | Using dynamic programming . | DP solution | . | . ",
    "url": "/docs/compsci/leetcode/regular-expression-matching.html#regular-expression-matching",
    "relUrl": "/docs/compsci/leetcode/regular-expression-matching.html#regular-expression-matching"
  },"799": {
    "doc": "10 - Regular Expression Matching - Hard",
    "title": "Problem",
    "content": "Given an input string s and a pattern p, implement regular expression matching with support for . and * where: . | . Matches any single character.​​​​ | * Matches zero or more of the preceding element. | . The matching should cover the entire input string (not partial). Input: s = \"aa\", p = \"a\" Output: false . Input: s = \"aa\", p = \"a*\" Output: true . Input: s = \"ab\", p = \".*\" Output: true . ",
    "url": "/docs/compsci/leetcode/regular-expression-matching.html#problem",
    "relUrl": "/docs/compsci/leetcode/regular-expression-matching.html#problem"
  },"800": {
    "doc": "10 - Regular Expression Matching - Hard",
    "title": "Using recursion",
    "content": "The key thing to first note is that the * character is a wildcard that can match zero or more of the preceding character. This means even when s is empty, it can still match p. So when do we know whether s and p match? . The base case is when p is empty. | If p is empty and s is empty, then they match. | If p is empty and s is not empty, then they don’t match. | . If p is not empty, we need to see if the next pattern is an x*: . x is any character. | p[1] == '*' ? | . If the next pattern is not an x*, then we just need to see if the current characters match: . Let’s call this isFirstMatch: . isFirstMatch = !s.isEmpty() &amp;&amp; s[0] == p[0] || p[0] == '.' . However, if the next pattern is an x*, then we have two additional scenarios to consider: . | The * matches zero characters, so we skip this pattern (advance p by 2). | The * matches the current character (isFirstMatch), so we advance s by one character and p stays with the pattern. | . ",
    "url": "/docs/compsci/leetcode/regular-expression-matching.html#using-recursion",
    "relUrl": "/docs/compsci/leetcode/regular-expression-matching.html#using-recursion"
  },"801": {
    "doc": "10 - Regular Expression Matching - Hard",
    "title": "Recursive solution",
    "content": "| 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 . | public boolean isMatch(String s, String p) { // Base case if (p.isEmpty()) return s.isEmpty(); boolean isFirstMatch; if (s.isEmpty()) isFirstMatch = false; else { Character cs = s.charAt(0); Character cp = p.charAt(0); isFirstMatch = cs.equals(cp) || cp.equals('.'); } // Are we in a '*' pattern? if (p.length() &gt; 1) { Character cp2 = p.charAt(1); if (cp2.equals('*')) { boolean noMatch = isMatch(s, p.substring(2)); boolean yesMatch = isFirstMatch &amp;&amp; isMatch(s.substring(1), p); return noMatch || yesMatch; } } // Not in a '*' pattern return isFirstMatch &amp;&amp; isMatch(s.substring(1), p.substring(1)); } . | . This solution is $O(2^n)$. ",
    "url": "/docs/compsci/leetcode/regular-expression-matching.html#recursive-solution",
    "relUrl": "/docs/compsci/leetcode/regular-expression-matching.html#recursive-solution"
  },"802": {
    "doc": "10 - Regular Expression Matching - Hard",
    "title": "Using dynamic programming",
    "content": "It is easy to see that in order to check whether s and p match, the previous parts of s and p must match. Since having a solution for the subproblem helps us solve the bigger problem, we can use dynamic programming. Let M where M[i][j] is true if s[0:i] and p[0:j] match. M is s.length() + 1 by p.length() + 1 because we need to account for the empty string. There are two base cases: . | As seen in the recursive solution, if p is empty, then s must also be empty for them to match. Hence, only M[0][0] = true in the first column. | If s is empty, then p must be empty or have a * pattern with zero matches. Hence, M[0][j] = true if p[j - 1] == '*' &amp;&amp; M[i][j - 2]. | . Then we can fill in the rest of the table by: . | If we are not in a * pattern, then we can just check if the current characters match (or .) and if the previous parts of s and p match. | If we are in a * pattern, then we need to check if s matches the previous character in p via * or if it was a zero match case. | . ",
    "url": "/docs/compsci/leetcode/regular-expression-matching.html#using-dynamic-programming",
    "relUrl": "/docs/compsci/leetcode/regular-expression-matching.html#using-dynamic-programming"
  },"803": {
    "doc": "10 - Regular Expression Matching - Hard",
    "title": "DP solution",
    "content": "| 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 . | public static boolean isMatch(String s, String p) { int n = s.length(); int m = p.length(); // Additional row and column for empty string boolean[][] M = new boolean[n + 1][m + 1]; // Rest of the first column is already false M[0][0] = true; // Base case for when s is empty for (int j = 1; j &lt; m + 1; j++) { Character pc = p.charAt(j - 1); M[0][j] = pc.equals('*') &amp;&amp; M[0][j - 2]; } for (int i = 1; i &lt; n + 1; i++) { // Current character of s Character sc = s.charAt(i - 1); for (int j = 1; j &lt; m + 1; j++) { // Current character of p Character pc = p.charAt(j - 1); boolean isFirstMatch; // Are we in a '*' pattern? if (pc.equals('*')) { // Previous character of p Character pcp = p.charAt(j - 2); // Current character of s matches previous character of p isFirstMatch = sc.equals(pcp) || pcp.equals('.'); // Zero match, pretend it doesn't exist // As long as the previous parts before '*' pattern matches we're good boolean noMatch = M[i][j - 2]; // There was a match, so check if this wildcard match was valid previously boolean yesMatch = isFirstMatch &amp;&amp; M[i - 1][j]; M[i][j] = noMatch || yesMatch; } else { // Not in a '*' pattern // Check if current characters match and previous parts match isFirstMatch = sc.equals(pc) || pc.equals('.'); M[i][j] = isFirstMatch &amp;&amp; M[i - 1][j - 1]; } } } return M[n][m]; } . | . This solution is $O(nm)$. ",
    "url": "/docs/compsci/leetcode/regular-expression-matching.html#dp-solution",
    "relUrl": "/docs/compsci/leetcode/regular-expression-matching.html#dp-solution"
  },"804": {
    "doc": "10 - Regular Expression Matching - Hard",
    "title": "10 - Regular Expression Matching - Hard",
    "content": " ",
    "url": "/docs/compsci/leetcode/regular-expression-matching.html",
    "relUrl": "/docs/compsci/leetcode/regular-expression-matching.html"
  },"805": {
    "doc": "7 - Reverse Integer - Medium",
    "title": "Reverse Integer",
    "content": ". | Problem | Explanation | Solution | . ",
    "url": "/docs/compsci/leetcode/reverse-integer.html#reverse-integer",
    "relUrl": "/docs/compsci/leetcode/reverse-integer.html#reverse-integer"
  },"806": {
    "doc": "7 - Reverse Integer - Medium",
    "title": "Problem",
    "content": "Given a signed 32-bit integer x, return x with its digits reversed. If reversing x causes the value to go outside the signed 32-bit integer range [-2^31, 2^31 - 1], then return 0. Assume the environment does not allow you to store 64-bit integers (signed or unsigned). Input: x = 123 Output: 321 . Input: x = -123 Output: -321 . Input: x = 120 Output: 21 . ",
    "url": "/docs/compsci/leetcode/reverse-integer.html#problem",
    "relUrl": "/docs/compsci/leetcode/reverse-integer.html#problem"
  },"807": {
    "doc": "7 - Reverse Integer - Medium",
    "title": "Explanation",
    "content": "Everything else is pretty straightforward, but the tricky part is checking for overflow. Because of the assumption, we cannot use a long to store the reversed integer, and then check if it is within the range of a 32-bit integer. So I decided to store the reversed integer as a string, and then if we fail to parse it as an integer, we know it is out of range. ",
    "url": "/docs/compsci/leetcode/reverse-integer.html#explanation",
    "relUrl": "/docs/compsci/leetcode/reverse-integer.html#explanation"
  },"808": {
    "doc": "7 - Reverse Integer - Medium",
    "title": "Solution",
    "content": "| 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . | public int reverse(int x) { int sign = (x &lt; 0) ? -1 : 1; x = sign * x; StringBuilder sb = new StringBuilder(); while (x &gt; 0) { sb.append(x % 10); x /= 10; } try { return sign * Integer.parseInt(sb.toString()); } catch (NumberFormatException e) { return 0; } } . | . The complexity is $O(\\log x)$. ",
    "url": "/docs/compsci/leetcode/reverse-integer.html#solution",
    "relUrl": "/docs/compsci/leetcode/reverse-integer.html#solution"
  },"809": {
    "doc": "7 - Reverse Integer - Medium",
    "title": "7 - Reverse Integer - Medium",
    "content": " ",
    "url": "/docs/compsci/leetcode/reverse-integer.html",
    "relUrl": "/docs/compsci/leetcode/reverse-integer.html"
  },"810": {
    "doc": "ROC Curve",
    "title": "Receiver Operating Characteristic Curve",
    "content": ". | Errors in classfication | ROC Curve . | Area under the curve (AUC) | . | Selecting the threshold | PR Curve using precision | . ",
    "url": "/docs/statistics/notes/roc.html#receiver-operating-characteristic-curve",
    "relUrl": "/docs/statistics/notes/roc.html#receiver-operating-characteristic-curve"
  },"811": {
    "doc": "ROC Curve",
    "title": "Errors in classfication",
    "content": "In machine learning, classifiers cannot be perfect and there will be errors. Blue64701, CC BY-SA 4.0, via Wikimedia Commons There is usually a trade-off between sensitivity and specificity. You may attempt to reduce as many false negatives in expense of false positives, or reduce false positives in expense of false negatives. However, we’d still like to know the optimal line or threshold that ultimately classifies an observation while minimizing the sacrifice. ",
    "url": "/docs/statistics/notes/roc.html#errors-in-classfication",
    "relUrl": "/docs/statistics/notes/roc.html#errors-in-classfication"
  },"812": {
    "doc": "ROC Curve",
    "title": "ROC Curve",
    "content": "We can use the ROC curve to help determine that. cmglee, MartinThoma, CC BY-SA 4.0, via Wikimedia Commons The basic idea is, we tweak the classifier’s threshold and plot a point using the $TPR$ (sensitivity, recall) and $FPR$ (fall-out)). \\[TPR \\text{ (recall)} = \\frac{TP}{TP+FN}\\] \\[FPR = \\frac{FP}{FP + TN} = 1 - TNR \\text{ (specificity)}\\] The collection of these points will form a curve for that classifier. Each point on the ROC curve is a performance measure of a threshold. Area under the curve (AUC) . The red dotted line indicates a curve of a random classifier where $TPR = FPR$, which means the number of correctly classfied positive tests and the number of incorrectly classified positive tests are equal. For any classifier above the random classifier, the number of correctly classified positives is greater than the number of incorrectly classified positives. So classifiers with ROC curves higher above have better performances. Area under the curve (AUC) ($\\le 1$) is an aggregate performance measure across all thresholds of a model. Higher AUC indicates higher accuracy of the model. AUC is a useful metric even when the class distributions are highly unbalanced. AUC is for comparing models not thresholds. ",
    "url": "/docs/statistics/notes/roc.html#roc-curve",
    "relUrl": "/docs/statistics/notes/roc.html#roc-curve"
  },"813": {
    "doc": "ROC Curve",
    "title": "Selecting the threshold",
    "content": "In every classification problem, a business decision must be made. Which of the two are more detrimental? . If the normal/abnormal classification was performed in a medical setting, a False Negative might be a patient who is diagnosed as normal, but is actually sick. Here we prefer to avoid False Negatives (or maximize True Positives) at the expense of more False Positives. In the below figure, think about moving the green bar to the far left. However, if the classification was about whether an email is spam or not, we’d pretty much prefer spam mails occasionally ending up in your inbox rather than having your job offer email sent to spam. In this case, we prefer to avoid False Positives at the expense of more False Negatives. In the below figure, think about moving the green bar to the far right. The business decision at hand is then: . | Maximize True Positive Rate | Minimize False Positive Rate | . Sharpr for svg version. original work by kakau in a png, CC BY-SA 3.0, via Wikimedia Commons The ROC curve is a visual illustration of the impact of different thresholds and helps you make these business decisions. If you increase the threshold towards right, the point on the ROC curve moves towards bottom left. If you decrease the threshold towards left, the point on the ROC curve moves towards top right. ",
    "url": "/docs/statistics/notes/roc.html#selecting-the-threshold",
    "relUrl": "/docs/statistics/notes/roc.html#selecting-the-threshold"
  },"814": {
    "doc": "ROC Curve",
    "title": "PR Curve using precision",
    "content": "Sometimes the curve is drawn with recall (TPR, sensitivity) and precision (PPV) instead. \\[precision = \\frac{TP}{TP + FP}\\] \\[FPR = \\frac{FP}{N} = \\frac{FP}{TN+FP}\\] Suppose the number of actual Condition Negative ($N$) dominates ($N \\rightarrow \\infty$; high imbalance towards class Negative). As you can see from the equation, while precision is significantly affected by the number of False Positives regardless, FPR does not change as much if $N$ is too large. Therefore, if the class distributions are highly imbalanced, precision may be more sensitive to False Positives than FPR. AUC for ROC curve is sometimes called AUROC and AUC for PR curve is called AUPR. Both AUC metrics bear analogous meaning: higher the better. If you increase the threshold towards right, the point on PR curve moves top left. References: . | Ten quick tips for machine learning in computational biology - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/figure/a-Example-of-Precision-Recall-curve-with-the-precision-score-on-the-y-axis-and-the_fig1_321672019 [accessed 4 Dec, 2021] | . ",
    "url": "/docs/statistics/notes/roc.html#pr-curve-using-precision",
    "relUrl": "/docs/statistics/notes/roc.html#pr-curve-using-precision"
  },"815": {
    "doc": "ROC Curve",
    "title": "ROC Curve",
    "content": ". ",
    "url": "/docs/statistics/notes/roc.html",
    "relUrl": "/docs/statistics/notes/roc.html"
  },"816": {
    "doc": "Row Echelon Form / Reduced Row Echelon Form",
    "title": "Row Echelon Form / Reduced Row Echelon Form",
    "content": ". | Row echelon form | Reduced row echelon form | Augmented matrix | . ",
    "url": "/docs/linalg/notes/row-echelon.html",
    "relUrl": "/docs/linalg/notes/row-echelon.html"
  },"817": {
    "doc": "Row Echelon Form / Reduced Row Echelon Form",
    "title": "Row echelon form",
    "content": "A row echelon form matrix satisfies the following conditions: . A pivot entry is the first non-zero entry of a row in a row echelon form matrix. | All-zero rows are at the bottom of the matrix. | All pivot entries are $1$. | The pivot entries of rows are strictly to the right of the pivot entries of rows above them. | All entries below a pivot entry are zero. | . \\[\\begin{bmatrix} \\mathbf{1} &amp; 0 &amp; 3 &amp; 4 \\\\ 0 &amp; \\mathbf{1} &amp; 2 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 &amp; \\mathbf{1} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix}\\] . ",
    "url": "/docs/linalg/notes/row-echelon.html#row-echelon-form",
    "relUrl": "/docs/linalg/notes/row-echelon.html#row-echelon-form"
  },"818": {
    "doc": "Row Echelon Form / Reduced Row Echelon Form",
    "title": "Reduced row echelon form",
    "content": "A reduced row echelon form matrix satisfies the following conditions: . | All the conditions of a row echelon form matrix. | The pivot entries are the only non-zero entries in their columns. | . \\[\\begin{bmatrix} \\mathbf{1} &amp; 0 &amp; 3 &amp; 0 \\\\ 0 &amp; \\mathbf{1} &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\mathbf{1} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix}\\] Common methods to find the reduced row echelon form of a matrix are Gaussian elimination and Gauss-Jordan elimination, which involve elementary row operations (row switching and linear combinations of rows). ",
    "url": "/docs/linalg/notes/row-echelon.html#reduced-row-echelon-form",
    "relUrl": "/docs/linalg/notes/row-echelon.html#reduced-row-echelon-form"
  },"819": {
    "doc": "Row Echelon Form / Reduced Row Echelon Form",
    "title": "Augmented matrix",
    "content": "Augmented matrix is a matrix that represents a system of linear equations. The following system of linear equations represented with a coefficient matrix and a constant vector: . \\[\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\\\ \\end{bmatrix}\\] can be represented as an augmented matrix: . \\[\\left[ \\begin{array}{ccc|c} 1 &amp; 2 &amp; 3 &amp; 10 \\\\ 4 &amp; 5 &amp; 6 &amp; 11 \\\\ 7 &amp; 8 &amp; 9 &amp; 12 \\\\ \\end{array} \\right]\\] Finding the solution of a system of linear equations is equivalent to finding the reduced row echelon form of the augmented matrix. ",
    "url": "/docs/linalg/notes/row-echelon.html#augmented-matrix",
    "relUrl": "/docs/linalg/notes/row-echelon.html#augmented-matrix"
  },"820": {
    "doc": "Storybook UI",
    "title": "Storybook UI",
    "content": ". | Install and add Storybook UI | Start Storybook locally | Error: PostCSS plugin tailwindcss requires PostCSS 8 | . ",
    "url": "/docs/vue/sb.html",
    "relUrl": "/docs/vue/sb.html"
  },"821": {
    "doc": "Storybook UI",
    "title": "Install and add Storybook UI",
    "content": "Inside the Vue project root, . npx sb init . ",
    "url": "/docs/vue/sb.html#install-and-add-storybook-ui",
    "relUrl": "/docs/vue/sb.html#install-and-add-storybook-ui"
  },"822": {
    "doc": "Storybook UI",
    "title": "Start Storybook locally",
    "content": "yarn storybook . ",
    "url": "/docs/vue/sb.html#start-storybook-locally",
    "relUrl": "/docs/vue/sb.html#start-storybook-locally"
  },"823": {
    "doc": "Storybook UI",
    "title": "Error: PostCSS plugin tailwindcss requires PostCSS 8",
    "content": "Tailwind CSS depends on PostCSS 8. As of now, Storybook have not yet been updated to support PostCSS 8. Therefore, you must install a compatibility build of Tailwind to use it with Storybook. See here for detail. If you already have Tailwind installed, remove by . yarn remove tailwindcss postcss autoprefixer . ",
    "url": "/docs/vue/sb.html#error-postcss-plugin-tailwindcss-requires-postcss-8",
    "relUrl": "/docs/vue/sb.html#error-postcss-plugin-tailwindcss-requires-postcss-8"
  },"824": {
    "doc": "Seasonality",
    "title": "Seasonality",
    "content": "Seasonality is a characteristic of time series data where the data exhibits a repeating pattern at regular intervals (e.g. higher number of visitors on weekends, etc). The most intuitive way to notice seasonality is to visualize the data. However, the type of plot you use can affect how you perceive seasonality. Sometimes it is harder to notice seasonality with a scatter plot than with a line plot. Aside from visualization, there are other statistical methods to detect seasonality. | Difference between seasonality and cyclicity | Different types of time series models . | Additive model | Multiplicative model | . | Decomposition | . ",
    "url": "/docs/data-science/notes/seasonality.html",
    "relUrl": "/docs/data-science/notes/seasonality.html"
  },"825": {
    "doc": "Seasonality",
    "title": "Difference between seasonality and cyclicity",
    "content": "They are similar in that they both exhibit a repeating pattern. However, cyclicity has variable periods. ",
    "url": "/docs/data-science/notes/seasonality.html#difference-between-seasonality-and-cyclicity",
    "relUrl": "/docs/data-science/notes/seasonality.html#difference-between-seasonality-and-cyclicity"
  },"826": {
    "doc": "Seasonality",
    "title": "Different types of time series models",
    "content": "A time series data can be thought of as a combination of . | Level (average) | Trend (upward or downward movement) | Seasonality (repeating pattern) | Residual (random noise) | . We must first identify how these components combine to form the data. Then we can use the appropriate decomposition model to extract seasonality. ",
    "url": "/docs/data-science/notes/seasonality.html#different-types-of-time-series-models",
    "relUrl": "/docs/data-science/notes/seasonality.html#different-types-of-time-series-models"
  },"827": {
    "doc": "Seasonality",
    "title": "Additive model",
    "content": "If a time series data is best modeled by adding the above components, then it is called an additive model. $$ y = \\text{Level} + \\text{Trend} + \\text{Seasonality} + \\text{Residual} $$ . ",
    "url": "/docs/data-science/notes/seasonality.html#additive-model",
    "relUrl": "/docs/data-science/notes/seasonality.html#additive-model"
  },"828": {
    "doc": "Seasonality",
    "title": "Multiplicative model",
    "content": "If a time series data is best modeled by multiplying the above components, then it is called a multiplicative model. $$ y = \\text{Level} \\times \\text{Trend} \\times \\text{Seasonality} \\times \\text{Residual} $$ . ",
    "url": "/docs/data-science/notes/seasonality.html#multiplicative-model",
    "relUrl": "/docs/data-science/notes/seasonality.html#multiplicative-model"
  },"829": {
    "doc": "Seasonality",
    "title": "Decomposition",
    "content": " ",
    "url": "/docs/data-science/notes/seasonality.html#decomposition",
    "relUrl": "/docs/data-science/notes/seasonality.html#decomposition"
  },"830": {
    "doc": "Flutter Setup",
    "title": "Flutter Setup",
    "content": ". | Installation . | Xcode | Android Studio / Android SDK | CocoaPods | . | . ",
    "url": "/docs/flutter/setup.html",
    "relUrl": "/docs/flutter/setup.html"
  },"831": {
    "doc": "Flutter Setup",
    "title": "Installation",
    "content": "If you’re using Apple Sillicon Mac, first install Rosetta. Install Flutter via Homebrew: . brew install --cask flutter . Run the following command to see the components installed or missing: . flutter doctor # -v for verbose . You can opt out of analytics and crash reporting by running flutter config --no-analytics. Xcode . Install Xcode from the App Store. Then, run the following commands to configure Xcode command-line tools: . sudo xcode-select --install sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer sudo xcodebuild -runFirstLaunch sudo xcodebuild -license . Android Studio / Android SDK . Install Android Studio via Homebrew: . brew install --cask android-studio . If not already, install Android SDK via Android Studio: . And Android SDK Command-line Tools: . Accept Android SDK licenses: . flutter doctor --android-licenses . CocoaPods . To use Flutter plugins with native iOS code, you need to install CocoaPods: . brew install cocoapods . References: . | Flutter: Get Started | . ",
    "url": "/docs/flutter/setup.html#installation",
    "relUrl": "/docs/flutter/setup.html#installation"
  },"832": {
    "doc": "Sparse Checkout",
    "title": "Sparse Checkout",
    "content": ". | Monorepo | Checkout only the necessary folders | Check the status | . ",
    "url": "/docs/git-hub/git/sparse.html",
    "relUrl": "/docs/git-hub/git/sparse.html"
  },"833": {
    "doc": "Sparse Checkout",
    "title": "Monorepo",
    "content": "A monorepo a single repository that contains multiple logical root directories or microservices (e.g. frontend subfolder and a backend subfolder.) . As a monorepo grows in size, tracking can become slow. However, a frontend developer may not have to consistently pull and push with the changes in a backend folder and vice versa. We don’t need them, so why keep them? . If the logics of difference microservices can be isolated, version control can prune down unnecessary structures. ",
    "url": "/docs/git-hub/git/sparse.html#monorepo",
    "relUrl": "/docs/git-hub/git/sparse.html#monorepo"
  },"834": {
    "doc": "Sparse Checkout",
    "title": "Checkout only the necessary folders",
    "content": "First clone a project without checking them out: . git clone &lt;URL&gt; --no-checkout # For efficient cloning use partial clone git clone &lt;URL&gt; --no-checkout --filter=blob:none # Or if you also don't need the commit history git clone &lt;URL&gt; --no-checkout --depth 1 . cd into the cloned project and init only the root files: . git sparse-checkout init --cone . Yes it is --cone not a typo of --clone. Set subfolders that you’d like to checkout: . git sparse-checkout set backend db/config . Then checkout: . git checkout . ",
    "url": "/docs/git-hub/git/sparse.html#checkout-only-the-necessary-folders",
    "relUrl": "/docs/git-hub/git/sparse.html#checkout-only-the-necessary-folders"
  },"835": {
    "doc": "Sparse Checkout",
    "title": "Check the status",
    "content": "Try: . git status . Now your git status will indicate that you are in sparse checkout mode. You will see that your project is still up to date with the remote even though all the other subfolders are not locally present. References: . | Bring your monorepo down to size with sparse-checkout | . ",
    "url": "/docs/git-hub/git/sparse.html#check-the-status",
    "relUrl": "/docs/git-hub/git/sparse.html#check-the-status"
  },"836": {
    "doc": "Stationarity",
    "title": "Stationarity",
    "content": "Although it seems like an intuitive concept, identifying stationarity is not as easy as it seems. Identifying stationarity is important because many statistical models assume stationarity. Also because stationarity means that some important metrics, such as mean and variance, are constant over time, it is often a desirable property for analysis. | Understanding stationarity | Quick rule of thumb to identify stationarity | Unit root test . | Unit root | Augmented Dickey-Fuller test . | Caveats of ADF | . | . | Ways to correct non-stationarity | Different types of stationarity | . ",
    "url": "/docs/data-science/notes/stationarity.html",
    "relUrl": "/docs/data-science/notes/stationarity.html"
  },"837": {
    "doc": "Stationarity",
    "title": "Understanding stationarity",
    "content": "Suppose a time series data $y_t$ is generated by a random variable of some unknown joint probability distribution. For any time lag $k$, if the joint probability distribution of $y_{t+k}$ remains the same as $y_t$, then the time series data is stationary. ",
    "url": "/docs/data-science/notes/stationarity.html#understanding-stationarity",
    "relUrl": "/docs/data-science/notes/stationarity.html#understanding-stationarity"
  },"838": {
    "doc": "Stationarity",
    "title": "Quick rule of thumb to identify stationarity",
    "content": ". | Mean is constant over time | Variance is constant over time | There is no seasonality | . You can identify these from visualizing the data. White noise is a special type of staionary time series data, where the mean is zero and variance is constant over time. ",
    "url": "/docs/data-science/notes/stationarity.html#quick-rule-of-thumb-to-identify-stationarity",
    "relUrl": "/docs/data-science/notes/stationarity.html#quick-rule-of-thumb-to-identify-stationarity"
  },"839": {
    "doc": "Stationarity",
    "title": "Unit root test",
    "content": "Most statistical tests for stationarity looks for the presence of a unit root. If a time series data has a unit root, it is non-stationary. ",
    "url": "/docs/data-science/notes/stationarity.html#unit-root-test",
    "relUrl": "/docs/data-science/notes/stationarity.html#unit-root-test"
  },"840": {
    "doc": "Stationarity",
    "title": "Unit root",
    "content": "If a time series has a characteristic equation with a root equal to 1, it is said to have a unit root. Let’s define a autoregressive process of order 1 (AR(1)) as follows: . \\[y_t = \\phi y_{t-1} + \\epsilon_t\\] The characteristic equation of this process is: . \\[1 - \\phi z = 0\\] If $\\phi = 1$, the root of this equation is $z = 1$ and thus a unit root. Then we would say this process is non-stationary. ",
    "url": "/docs/data-science/notes/stationarity.html#unit-root",
    "relUrl": "/docs/data-science/notes/stationarity.html#unit-root"
  },"841": {
    "doc": "Stationarity",
    "title": "Augmented Dickey-Fuller test",
    "content": "Augmented Dickey-Fuller (ADF) is the most commonly used hypothesis test for stationarity. The null hypothesis is that the time series data has a unit root. If the test result is significant, we reject the null hypothesis and conclude that the time series data is stationary. As the name suggests, ADF is an extension of the Dickey-Fuller test. The main difference is that ADF can handle higher order autoregressive processes. Caveats of ADF . | Struggles to distinguish between near-unit root and unit root | High false positive rate when sample size is small | . ",
    "url": "/docs/data-science/notes/stationarity.html#augmented-dickey-fuller-test",
    "relUrl": "/docs/data-science/notes/stationarity.html#augmented-dickey-fuller-test"
  },"842": {
    "doc": "Stationarity",
    "title": "Ways to correct non-stationarity",
    "content": "Although it is not always possible to correct non-stationarity (and not always what you want to do), there are some methods that can be applied to correct non-stationarity. | Differencing: gets rid of trend, fixes non-constant mean | Log transformation: fixes non-constant variance | Square root transformation: fixes non-constant variance | . ",
    "url": "/docs/data-science/notes/stationarity.html#ways-to-correct-non-stationarity",
    "relUrl": "/docs/data-science/notes/stationarity.html#ways-to-correct-non-stationarity"
  },"843": {
    "doc": "Stationarity",
    "title": "Different types of stationarity",
    "content": "To be added . There are different types of stationarity depending on the constraints enforced on the joint probability distribution. e.g. First moment (mean) and second moment (variance) should be constant, but other statistical moments can be allowed to vary, e.g. The stationary constraint has to hold only for certain time lags but not throughout. | Strict stationarity | Weak stationarity | N-th order stationarity | . ",
    "url": "/docs/data-science/notes/stationarity.html#different-types-of-stationarity",
    "relUrl": "/docs/data-science/notes/stationarity.html#different-types-of-stationarity"
  },"844": {
    "doc": "Statistical Model for Time Series",
    "title": "Statistical Model for Time Series",
    "content": ". | Shortcomings of linear regression in time series | Autoregressive (AR) Model | . ",
    "url": "/docs/data-science/time-series/statistical-model.html",
    "relUrl": "/docs/data-science/time-series/statistical-model.html"
  },"845": {
    "doc": "Statistical Model for Time Series",
    "title": "Shortcomings of linear regression in time series",
    "content": "Linear regression assumes that the data points are i.i.d. which is not true for time series data where proximate data points are likely strongly correlated. ",
    "url": "/docs/data-science/time-series/statistical-model.html#shortcomings-of-linear-regression-in-time-series",
    "relUrl": "/docs/data-science/time-series/statistical-model.html#shortcomings-of-linear-regression-in-time-series"
  },"846": {
    "doc": "Statistical Model for Time Series",
    "title": "Autoregressive (AR) Model",
    "content": "Go to page . ",
    "url": "/docs/data-science/time-series/statistical-model.html#autoregressive-ar-model",
    "relUrl": "/docs/data-science/time-series/statistical-model.html#autoregressive-ar-model"
  },"847": {
    "doc": "Sum of Squares",
    "title": "Sum of Squares",
    "content": ". | What is sum of squares? | Total sum of squares | Residual sum of squares | Explained sum of squares | Relationship between sum of squares | Ordinary least squares | . ",
    "url": "/docs/statistics/notes/sum-of-squares.html",
    "relUrl": "/docs/statistics/notes/sum-of-squares.html"
  },"848": {
    "doc": "Sum of Squares",
    "title": "What is sum of squares?",
    "content": "Sum of squares is a concept frequently used in regression analysis. Depending on what we choose to square, we end up with many different sums of squares. ",
    "url": "/docs/statistics/notes/sum-of-squares.html#what-is-sum-of-squares",
    "relUrl": "/docs/statistics/notes/sum-of-squares.html#what-is-sum-of-squares"
  },"849": {
    "doc": "Sum of Squares",
    "title": "Total sum of squares",
    "content": "The total sum of squares ($SS_{tot}$) is the sum of the squared differences between the observed dependent variable $y_i \\in Y$ and its mean $\\bar{y}$: . $$ SS_{tot} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 $$ . where $n$ is the number of observations. Graphically, in a simple linear regression with one independent variable, $SS_{tot}$ is the sum of the areas of the purple squares in the figure below. ",
    "url": "/docs/statistics/notes/sum-of-squares.html#total-sum-of-squares",
    "relUrl": "/docs/statistics/notes/sum-of-squares.html#total-sum-of-squares"
  },"850": {
    "doc": "Sum of Squares",
    "title": "Residual sum of squares",
    "content": "Also known as sum of squared errors (SSE). The residual sum of squares ($SS_{res}$) is the sum of the squared differences between the observed dependent variable $y_i \\in Y$ and the predicted value $\\hat{y}_i$ from the regression line: . $$ SS_{res} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$ . Graphically, in a simple linear regression with one independent variable, $SS_{res}$ is the sum of the areas of the red squares in the figure below. ",
    "url": "/docs/statistics/notes/sum-of-squares.html#residual-sum-of-squares",
    "relUrl": "/docs/statistics/notes/sum-of-squares.html#residual-sum-of-squares"
  },"851": {
    "doc": "Sum of Squares",
    "title": "Explained sum of squares",
    "content": "Also known as model sum of squares. The explained sum of squares ($SS_{exp}$) is the sum of the squared differences between the predicted value $\\hat{y}_i$ from the regression line and the mean $\\bar{y}$: . $$ SS_{exp} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 $$ . Graphically, in a simple linear regression with one independent variable, $SS_{exp}$ is the sum of the areas of the blue squares in the figure below. ",
    "url": "/docs/statistics/notes/sum-of-squares.html#explained-sum-of-squares",
    "relUrl": "/docs/statistics/notes/sum-of-squares.html#explained-sum-of-squares"
  },"852": {
    "doc": "Sum of Squares",
    "title": "Relationship between sum of squares",
    "content": "For linear regression models using Ordinary Least Squares (OLS) estimation, the following relationship holds: . $$ SS_{tot} = SS_{exp} + SS_{res} $$ . ",
    "url": "/docs/statistics/notes/sum-of-squares.html#relationship-between-sum-of-squares",
    "relUrl": "/docs/statistics/notes/sum-of-squares.html#relationship-between-sum-of-squares"
  },"853": {
    "doc": "Sum of Squares",
    "title": "Ordinary least squares",
    "content": "Least squares is a common approximation method for linear regression models. The idea is to fit a model that mimimizes some sum of squares (i.e. creates the least squares). Ordinary least squares (OLS) is the most common estimation method which minimizes the residual sum of squares. $$ \\hat{\\beta} = \\underset{\\beta}{\\operatorname{argmin}} \\sum_{i=1}^{n} (y_i - x_i \\beta)^2 $$ . Since we are trying to minimize the sum of squares with respect to the parameters, we solve for the following partial derivative: . \\[\\frac{\\partial}{\\partial \\beta} \\sum_{i=1}^{n} (y_i - x_i \\beta)^2 = 0\\] Given that some conditions hold, there is a closed-form estimation for $\\beta$: . $$ \\hat{\\beta} = (X^T X)^{-1} X^T y $$ . ",
    "url": "/docs/statistics/notes/sum-of-squares.html#ordinary-least-squares",
    "relUrl": "/docs/statistics/notes/sum-of-squares.html#ordinary-least-squares"
  },"854": {
    "doc": "Spurious Correlation",
    "title": "Spurious Correlation",
    "content": "If two variables are correlated, but there is no causal relationship between them, then the correlation is said to be spurious. Spurious correlation is common in time series data because chances of finding a false causal relationship increases as intermediate variables increase. In addition, temporal variables are common confounders that can cause spurious correlation. It is very tricky to avoid spurious correlation. | Things that can cause spurious correlation . | Trend | Confounder | Dependency in variables | Pure luck | . | . ",
    "url": "/docs/statistics/notes/suprious-correlation.html",
    "relUrl": "/docs/statistics/notes/suprious-correlation.html"
  },"855": {
    "doc": "Spurious Correlation",
    "title": "Things that can cause spurious correlation",
    "content": " ",
    "url": "/docs/statistics/notes/suprious-correlation.html#things-that-can-cause-spurious-correlation",
    "relUrl": "/docs/statistics/notes/suprious-correlation.html#things-that-can-cause-spurious-correlation"
  },"856": {
    "doc": "Spurious Correlation",
    "title": "Trend",
    "content": "Data with a trend is more likely to produce a spurious correlation. Compared to stationary data where nothing new is really happening, it makes sense that moving data is more likely to be falsely connected with other moving data. ",
    "url": "/docs/statistics/notes/suprious-correlation.html#trend",
    "relUrl": "/docs/statistics/notes/suprious-correlation.html#trend"
  },"857": {
    "doc": "Spurious Correlation",
    "title": "Confounder",
    "content": "A confounder is a variable that is correlated with both the independent and dependent variables. | Increase in ice cream sales leads to increase in drowning deaths? . | Summer is the confounder | . | Increase in temperature leads to increase in divorce rates? . | Time is the confounder. Things typically increase over time. | . | . ",
    "url": "/docs/statistics/notes/suprious-correlation.html#confounder",
    "relUrl": "/docs/statistics/notes/suprious-correlation.html#confounder"
  },"858": {
    "doc": "Spurious Correlation",
    "title": "Dependency in variables",
    "content": "It is easy to mistake two variables that are dependent on each other to be independent. If one is a calculated value of the other, for example, you would expect them to be highly correlated. ",
    "url": "/docs/statistics/notes/suprious-correlation.html#dependency-in-variables",
    "relUrl": "/docs/statistics/notes/suprious-correlation.html#dependency-in-variables"
  },"859": {
    "doc": "Spurious Correlation",
    "title": "Pure luck",
    "content": "Sometimes, things may seem like that just because of pure luck. ",
    "url": "/docs/statistics/notes/suprious-correlation.html#pure-luck",
    "relUrl": "/docs/statistics/notes/suprious-correlation.html#pure-luck"
  },"860": {
    "doc": "Support Vector Machine",
    "title": "Support Vector Machine",
    "content": ". | Basic concept of SVM | Support Vector Classifier . | Training data | Hyperplane | Support vectors | Optimization problem . | Hard margin SVC | Soft margin SVC | . | Kernel trick . | Radial basis function (RBF) kernel | . | . | Support Vector Regression | . ",
    "url": "/docs/statistics/notes/svm.html",
    "relUrl": "/docs/statistics/notes/svm.html"
  },"861": {
    "doc": "Support Vector Machine",
    "title": "Basic concept of SVM",
    "content": "Support vector machine (SVM) is a supervised machine learning algorithm that can be used for both classification and regression. ",
    "url": "/docs/statistics/notes/svm.html#basic-concept-of-svm",
    "relUrl": "/docs/statistics/notes/svm.html#basic-concept-of-svm"
  },"862": {
    "doc": "Support Vector Machine",
    "title": "Support Vector Classifier",
    "content": "Support vector classifier (SVC) is a binary classifier that finds the optimal hyperplane that separates the two classes. Although SVC is inherently a binary classifier, it can be extended to multi-class classification (Lookup: one-vs-one, one-vs-rest, multiclass SVM). ",
    "url": "/docs/statistics/notes/svm.html#support-vector-classifier",
    "relUrl": "/docs/statistics/notes/svm.html#support-vector-classifier"
  },"863": {
    "doc": "Support Vector Machine",
    "title": "Training data",
    "content": ". | For $i = 1, \\dots, n$, . | $\\mathbf{x}_i \\in \\mathbb{R}^p$ is the $i$-th observation. Do not confuse this notation with the $x_1$ and $x_2$ in the figure above. The $x_1$ and $x_2$ in the figure are notating the two hypothetical features just to illustrate the concept of SVC in a 2D space. In our case, each $\\mathbf{x}_i$ is a $p$-dimensional vector that represents the $p$ features of the $i$-th observation (so each colored dots in the figure above). | $y_i \\in \\{ -1, 1 \\}$ is the (binary) class label of $x_i$. | . | . ",
    "url": "/docs/statistics/notes/svm.html#training-data",
    "relUrl": "/docs/statistics/notes/svm.html#training-data"
  },"864": {
    "doc": "Support Vector Machine",
    "title": "Hyperplane",
    "content": ". | A hyperplane is a subspace of dimension $p-1$ in $\\mathbb{R}^p$. | We want to find the optimal hyperplane that separates the two classes. | The optimal hyperplane is the one that maximizes the margin. | . We want to estimate a hyperplane defined by, . | $\\mathbf{w} \\in \\mathbb{R}^p$ is the vector normal to the hyperplane. | $b \\in \\mathbb{R}$ is the bias of the hyperplane. | . $$ \\mathbf{w}^T \\mathbf{x} + b = 0 $$ . Equation looks different As in the figure above, sometimes people define the hyperplane with a negative bias term: . \\[\\mathbf{w}^T \\mathbf{x} - b = 0\\] This is just a matter of notation and does not affect the estimation. ",
    "url": "/docs/statistics/notes/svm.html#hyperplane",
    "relUrl": "/docs/statistics/notes/svm.html#hyperplane"
  },"865": {
    "doc": "Support Vector Machine",
    "title": "Support vectors",
    "content": "Support vectors are $\\mathbf{x}_i$ that satisfy the following conditions: . $$ \\begin{align*} \\mathbf{w}^T \\mathbf{x}_i + b = 1 &amp;\\wedge y_i = 1 \\\\ \\mathbf{w}^T \\mathbf{x}_i + b = -1 &amp;\\wedge y_i = -1 \\end{align*} $$ . In the figure above the support vectors are the points on the dotted margin lines (two of which are blue and one green). Support vectors are actual observations and not some hypothetical values that satisfy the conditions. ",
    "url": "/docs/statistics/notes/svm.html#support-vectors",
    "relUrl": "/docs/statistics/notes/svm.html#support-vectors"
  },"866": {
    "doc": "Support Vector Machine",
    "title": "Optimization problem",
    "content": "Hard margin SVC . If the data are linearly separable, we can find a hyperplane that perfectly separates the two classes with a hard margin. In order for classification to be perfect, we want the observations with label $y_i = 1$ to be above the blue support vectors and the observations with label $y_i = -1$ to be below the green support vector. \\[\\begin{align*} \\mathbf{w}^T \\mathbf{x}_i + b &amp;\\geq 1 &amp; \\text{if } y_i = 1 \\\\ \\mathbf{w}^T \\mathbf{x}_i + b &amp;\\leq -1 &amp; \\text{if } y_i = -1 \\end{align*}\\] This hard-margin contraint can be compactly written as: . \\[y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1\\] Now we need to find a hyperplane maximizing the margin while adhering to the constraint above, where the size of the margin is defined as: . \\[\\frac{2}{\\lVert \\mathbf{w} \\rVert}\\] What? Let $\\mathbf{x}_{+1}$ be the support vector that satisfies . \\[\\mathbf{w}^T \\mathbf{x}_{+1} + b = 1\\] Let $d$ be the distance from this support vector to the hyperplane. We know that $\\frac{\\mathbf{w}}{\\lVert \\mathbf{w} \\rVert}$ is the unit vector normal to the hyperplane. If we start at $\\mathbf{x}_{+1}$ and move along the negative direction of this unit vector by $d$, we will reach the hyperplane: . \\[\\mathbf{w}^T \\left( \\mathbf{x}_{+1} - d \\frac{\\mathbf{w}}{\\lVert \\mathbf{w} \\rVert} \\right) + b = 0 \\\\\\] We want to solve for $d$: . \\[\\begin{gather*} \\mathbf{w}^T \\mathbf{x}_{+1} - d \\frac{\\lVert \\mathbf{w} \\rVert^2}{\\lVert \\mathbf{w} \\rVert} + b = 0 \\tag{Dot product} \\\\ \\mathbf{w}^T \\mathbf{x}_{+1} - d \\lVert \\mathbf{w} \\rVert + b = 0 \\\\ d \\lVert \\mathbf{w} \\rVert = \\mathbf{w}^T \\mathbf{x}_{+1} + b \\\\ d \\lVert \\mathbf{w} \\rVert = 1 \\tag{By definition} \\\\ d = \\frac{1}{\\lVert \\mathbf{w} \\rVert} \\end{gather*}\\] Since the margin is twice the distance $d$, . $$ \\frac{2}{\\lVert \\mathbf{w} \\rVert} $$ . We instead solve a minimization problem of: . $$ \\begin{gather*} \\min_{\\mathbf{w}, b} \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 \\quad s.t.\\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\end{gather*} $$ . We use $\\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2$ instead of $\\lVert \\mathbf{w} \\rVert$ just so that the derivative is easier to calculate. This primal problem can be solved using Lagrange multipliers. Soft margin SVC . In reality, the data are not always linearly separable with a hard margin. We can relax the constraint to allow some mistakes, but instead penalize the misclassification. To account for the misclassifications, we introduce a slack variable $\\xi_i \\ge 0$ for each observation: . $$ \\xi_i \\approx \\max(0, 1 - y_i (\\mathbf{w}^T \\mathbf{x}_i + b)) $$ . The right side of the $\\approx$ is called the hinge loss. We use the approximate sign because the slack variable is not exactly the hinge loss, but some constant times the hinge loss. Geometrically, $\\xi_i$ captures the distance from the correct side of the margin defined by the support vectors to the observation, which is an intuitive penalty for misclassification. For correctly classified observations, $\\xi_i = 0$, and for greatly misclassified observations $\\xi_i$ gets bigger. With this slack variable, we now have a new constraint: . \\[\\begin{align*} \\mathbf{w}^T \\mathbf{x}_i + b &amp;\\geq 1 - \\xi_i &amp; \\text{if } y_i = 1 \\\\ \\mathbf{w}^T \\mathbf{x}_i + b &amp;\\leq -1 + \\xi_i &amp; \\text{if } y_i = -1 \\end{align*}\\] This soft-margin contraint can be compactly written as: . \\[y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i\\] Since now we have two things to minimize, the original objective in addition to the sum of the slack variables, so we solve the following minimization problem: . $$ \\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 + C \\sum_{i=1}^{n} \\xi_i \\quad s.t.\\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i $$ . where $C$ is a hyperparameter that controls the trade-off between having a large, complex model (added $\\xi_i$) and allowing more mistakes. | With a huge $C$, we are essentially solving a hard-margin problem. | Because in attempt to minimize the slack sum with a huge $C$, we will end up allowing slacks of near-zero values which is as rigid as a hard-margin. | . | With a small $C$, we are giving more rooms for slack during the minimization. | . ",
    "url": "/docs/statistics/notes/svm.html#optimization-problem",
    "relUrl": "/docs/statistics/notes/svm.html#optimization-problem"
  },"867": {
    "doc": "Support Vector Machine",
    "title": "Kernel trick",
    "content": "Another way to solve non-linearly separable classifications is to use the kernel trick to transform the data. Commonly used method to solve non-linearly separable classifications is to use the kernel trick in conjunction with the soft-margin SVC. Let feature map $\\Phi$ be a non-linear transformation that maps the data from a lower-dimensional input space to a higher-dimensional feature space. $$ \\Phi: \\mathbb{R}^p \\rightarrow \\mathbb{R}^q \\quad \\text{where}\\quad q &gt; p $$ . Cover's theorem Cover’s theorem states that a linearly non-separable dataset in a lower-dimensional space may be linearly separable in a higher-dimensional space. Kernel $K(\\cdot)$ is a function that satisfies the following: . $$ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}_j) \\quad \\forall \\mathbf{x}_i, \\mathbf{x}_j \\in \\mathbb{R}^p $$ . In other words, the kernel function is a function that given any two vectors in the input space, computes the dot product of them in the feature space. Loose intuition In the usual input space, the optimal solution given by the Lagrange multiplier method involves calculating the dot product of the observations $\\mathbf{x}_i^T \\mathbf{x}_j$. Since we now inflate the input space to a higher-dimensional feature space, we need to calculate the dot product of the observations in the feature space. So in order to replace the inner product of inputs, we need to find a kernel that can compute the dot product of the observations in the feature space for all possible pairs of observations. One key point is that as long as we can find a kernel with output that lives in the inner product space, we don’t have to have an explicitly defined feature map $\\Phi$. Radial basis function (RBF) kernel . Most commonly used kernel for SVC is the radial basis function (RBF) kernel. The RBF kernel is defined as: . \\[K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp \\left( -\\frac{\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert^2}{2 \\sigma^2} \\right)\\] $\\sigma$ is a free variable that controls the flexibility of the decision boundary. Oftentimes, we define . \\[\\gamma = \\frac{1}{2 \\sigma^2}\\] Again, $\\frac{1}{2}$ in the denominator is just for convenience. You may find $\\gamma = \\frac{1}{\\sigma^2}$ in other sources. so that the RBF kernel is defined as: . $$ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp \\left( -\\gamma \\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert^2 \\right) $$ . | Low $\\gamma$: decision boundary is less flexible, more linear | High $\\gamma$: decision boundary is more flexible | . ",
    "url": "/docs/statistics/notes/svm.html#kernel-trick",
    "relUrl": "/docs/statistics/notes/svm.html#kernel-trick"
  },"868": {
    "doc": "Support Vector Machine",
    "title": "Support Vector Regression",
    "content": "Support vector regression (SVR) is a regression model using the similar concept as the soft margin SVC. What we do is try to fit a hyperplane that minimizes the residuals, while allowing some slack. In soft margin SVC, we had the hinge loss to define the slack variable. In SVR, we use the $\\varepsilon$-insensitive loss to define the slack variable: . $$ \\xi_i \\approx max(0, \\lvert y_i - \\hat{y}_i \\rvert - \\varepsilon) $$ . where $\\hat{y}_i$ is the predicted value for $y_i$: . \\[\\hat{y}_i = \\mathbf{w}^T \\mathbf{x}_i + b\\] While the hinge loss penalizes any deviation from the correct side of the margin, the $\\varepsilon$-insensitive loss only penalizes the deviation that is greater than the tube of width $2\\varepsilon$ around the hyperplane. We are insensitive to the residuals within $\\varepsilon$ distance. Hence the name. ",
    "url": "/docs/statistics/notes/svm.html#support-vector-regression",
    "relUrl": "/docs/statistics/notes/svm.html#support-vector-regression"
  },"869": {
    "doc": "Terraform Module",
    "title": "Terraform Module",
    "content": ". | What is a Terraform module | Typical file structure | Module structure | Module input variables | How to call a module | Itty Bitties | . ",
    "url": "/docs/terraform/terraform-module.html",
    "relUrl": "/docs/terraform/terraform-module.html"
  },"870": {
    "doc": "Terraform Module",
    "title": "What is a Terraform module",
    "content": "It is sort of like a class in programming. Given input variables, it can be reused to create multiple instances of the infra described in the module. For example, when you’re creating an AWS lambda resource, there are typically some other resources associated with it, such as the REST API, IAM role, etc. If you think you’re going to be using this pattern often, you can create a module containing all the common resources and only expose some input variables that need to be configured at the top level. ",
    "url": "/docs/terraform/terraform-module.html#what-is-a-terraform-module",
    "relUrl": "/docs/terraform/terraform-module.html#what-is-a-terraform-module"
  },"871": {
    "doc": "Terraform Module",
    "title": "Typical file structure",
    "content": "The main Terraform execution point is called the root module. This is often the main.tf file in the top level driectory. Modules are often placed in a folder called modules. ├── README.md ├── main.tf ├── modules ├── outputs.tf └── variables.tf . ",
    "url": "/docs/terraform/terraform-module.html#typical-file-structure",
    "relUrl": "/docs/terraform/terraform-module.html#typical-file-structure"
  },"872": {
    "doc": "Terraform Module",
    "title": "Module structure",
    "content": "Inside modules directory, create a child directory with a name of your module: e.g. mymodule. ├── main.tf ├── modules │   └── mymodule │      ├── main.tf │      ├── outputs.tf │      └── variables.tf ├── outputs.tf └── variables.tf . The structure inside mymodule is optional. They can be separated as above or smashed into a single file. ",
    "url": "/docs/terraform/terraform-module.html#module-structure",
    "relUrl": "/docs/terraform/terraform-module.html#module-structure"
  },"873": {
    "doc": "Terraform Module",
    "title": "Module input variables",
    "content": "Unless you plan to reuse your module as-is every single time, you typically provide input variables to modules. Any variables defined with variable must be provided by the calling module or an error will be raised. ",
    "url": "/docs/terraform/terraform-module.html#module-input-variables",
    "relUrl": "/docs/terraform/terraform-module.html#module-input-variables"
  },"874": {
    "doc": "Terraform Module",
    "title": "How to call a module",
    "content": "All you need to do is feed the necessary input variables. In main.tf, . # main.tf module \"module_a\" { source = \"./modules/mymodule\" my_input_var = \"Here you go\" } . Notice that the source attribute points the the directory of the module, not any specific files . If you defined any output variables in the module with output, you can access them by: . module.module_a.my_output_var . ",
    "url": "/docs/terraform/terraform-module.html#how-to-call-a-module",
    "relUrl": "/docs/terraform/terraform-module.html#how-to-call-a-module"
  },"875": {
    "doc": "Terraform Module",
    "title": "Itty Bitties",
    "content": ". | Although you can nest your modules in multiple levels, it is recommended to keep the entire Terraform module as flat as possible. | Even if you don’t access them, module output variables is always output after terraform apply. | Think about whether a module is absolutely necessary. Sometimes you may end up feeding in as many input variablesas the original resource. | . ",
    "url": "/docs/terraform/terraform-module.html#itty-bitties",
    "relUrl": "/docs/terraform/terraform-module.html#itty-bitties"
  },"876": {
    "doc": "Provision with Terraform",
    "title": "Provision with Terraform",
    "content": ". | Configuration | . ",
    "url": "/docs/demo/flask-login-app/terraform.html",
    "relUrl": "/docs/demo/flask-login-app/terraform.html"
  },"877": {
    "doc": "Provision with Terraform",
    "title": "Configuration",
    "content": "Folder structure . flask-mongodb ├── backend │   ├── .gitignore │   ├── .dockerignore │   ├── app.py │   ├── back.dev.Dockerfile │   ├── requirements.txt │   └── venv │   └── flaskmongo └── terraform ├── main.tf └── providers.tf . We are going to be using a docker provider. # flask-mongodb/terraform/main.tf terraform { required_providers { docker = { source = \"kreuzwerker/docker\" version = \"~&gt; 2.11.0\" } } required_version = \"~&gt; 0.15.3\" } . # flask-mongodb/terraform/providers.tf provider \"docker\" { host = \"unix:///var/run/docker.sock\" } resource \"docker_container\" \"backend_tf\" { name = \"backend-tf\" image = docker_image.flask_back.latest volumes { container_path = \"/www\" host_path = \"/full/path/to/flask-mongodb/backend\" read_only = true } ports { internal = 5000 external = 5000 } } resource \"docker_image\" \"flask_back\" { name = \"flask-back:latest\" build { path = \"../backend\" dockerfile = \"back.dev.Dockerfile\" force_remove = true } } . Now initialize to download and install providers in .terraform and apply to create . terraform init terraform apply --auto-approve . To verify that docker image has been built and container is running . docker images | grep flask-back docker ps | grep backend-tf . Because volume is mounted, any change in directory backend will be reflected in the container. To destroy all resources created . terraform destroy . ",
    "url": "/docs/demo/flask-login-app/terraform.html#configuration",
    "relUrl": "/docs/demo/flask-login-app/terraform.html#configuration"
  },"878": {
    "doc": "tfenv",
    "title": "tfenv",
    "content": "Much like rbenv, pyenv . | Installation | Usage . | List all versions available | Install and uninstall | List all installed versions | Use a specific version | Print current version | Pin current version to project | . | . ",
    "url": "/docs/terraform/tfenv.html",
    "relUrl": "/docs/terraform/tfenv.html"
  },"879": {
    "doc": "tfenv",
    "title": "Installation",
    "content": "brew install tfenv . tfenv conflicts with terraform, so if you have terraform installed, you need to uninstall it first. ",
    "url": "/docs/terraform/tfenv.html#installation",
    "relUrl": "/docs/terraform/tfenv.html#installation"
  },"880": {
    "doc": "tfenv",
    "title": "Usage",
    "content": "List all versions available . tfenv list-remote . Install and uninstall . tfenv install [version] tfenv uninstall [version] . List all installed versions . tfenv list . Use a specific version . tfenv use [version] . Print current version . tfenv version-name . Pin current version to project . tfenv pin . Command above will create a .terraform-version file in the current directory. References: . | tfenv Github | . ",
    "url": "/docs/terraform/tfenv.html#usage",
    "relUrl": "/docs/terraform/tfenv.html#usage"
  },"881": {
    "doc": "Exploring Time Series Data",
    "title": "Exploring Time Series Data",
    "content": ". | What is Time Series Data? . | Univariate vs Multivariate | Obtaining prepared data | . | Collecting Time Series Data from Database | Characteristics of Time Series Data | Basic Exploratory Data Analysis on Time Series . | Line plot | Difference histogram | Scatter plot of features | Rolling windows | . | . ",
    "url": "/docs/data-science/time-series/time-series-data.html",
    "relUrl": "/docs/data-science/time-series/time-series-data.html"
  },"882": {
    "doc": "Exploring Time Series Data",
    "title": "What is Time Series Data?",
    "content": "Time series data is a sequence of observations. It does not matter what the unit of time is, what matters is: . | Unique and meaningful ordering | Intervals at which the observations were made | . ",
    "url": "/docs/data-science/time-series/time-series-data.html#what-is-time-series-data",
    "relUrl": "/docs/data-science/time-series/time-series-data.html#what-is-time-series-data"
  },"883": {
    "doc": "Exploring Time Series Data",
    "title": "Univariate vs Multivariate",
    "content": ". | Univariate: If a single variable is measured against time | Multivariate: If multiple variables are measured against time. Useful for analysis of interrelations. | . ",
    "url": "/docs/data-science/time-series/time-series-data.html#univariate-vs-multivariate",
    "relUrl": "/docs/data-science/time-series/time-series-data.html#univariate-vs-multivariate"
  },"884": {
    "doc": "Exploring Time Series Data",
    "title": "Obtaining prepared data",
    "content": "Easiest way to begin an analysis is to obtain prepared data: . | Competition data (i.e. Kaggle) | Repository of research labs (i.e. UCI Machine Learning Repository) | Data from government agencies | . For beginners, government data is not suitable for learning. Even experts devote their entire career to analyze the data due to its complexity. For starters, it is better to use them only for exploratory analysis or visualization. ",
    "url": "/docs/data-science/time-series/time-series-data.html#obtaining-prepared-data",
    "relUrl": "/docs/data-science/time-series/time-series-data.html#obtaining-prepared-data"
  },"885": {
    "doc": "Exploring Time Series Data",
    "title": "Collecting Time Series Data from Database",
    "content": "It is common to collect time series data from sources not originally intended for time series. Most common example is extracting analysis data from database. Often called data wrangling or data munging. The following are some of the DOs and DON’Ts: . | Integrating data from various tables to create a single time series brings about interesting analysis topics, but caution is needed due to: . | Disparate timestamp conventions | Different levels of granularity | . | Clarify whether each column of the data is really what you think it is . | For example, does the column status refer to the current status or the status at the time of the observation? | If a column says time, does it refer to the created time or the updated time? | . | Avoid lookahead | Understand recording conventions | Understand whether null values are included in the records . | If the DB does not record zero values, you may have to fill in the missing values with zeros. | . | Decide if you want to omit the start and end of the records . | Often, the start and end of the records are anomalous which only brings noise to the analysis. | . | Beware of time zone differences . | Although most databases store timestamps in UTC, it is not always the case. | . | Know whether the data was human-generated or machine-generated | . ",
    "url": "/docs/data-science/time-series/time-series-data.html#collecting-time-series-data-from-database",
    "relUrl": "/docs/data-science/time-series/time-series-data.html#collecting-time-series-data-from-database"
  },"886": {
    "doc": "Exploring Time Series Data",
    "title": "Characteristics of Time Series Data",
    "content": ". | Seasonality | Stationarity | Autocorrelation | Spurious correlation | . ",
    "url": "/docs/data-science/time-series/time-series-data.html#characteristics-of-time-series-data",
    "relUrl": "/docs/data-science/time-series/time-series-data.html#characteristics-of-time-series-data"
  },"887": {
    "doc": "Exploring Time Series Data",
    "title": "Basic Exploratory Data Analysis on Time Series",
    "content": "Various exploratory methods applied to time series data. ",
    "url": "/docs/data-science/time-series/time-series-data.html#basic-exploratory-data-analysis-on-time-series",
    "relUrl": "/docs/data-science/time-series/time-series-data.html#basic-exploratory-data-analysis-on-time-series"
  },"888": {
    "doc": "Exploring Time Series Data",
    "title": "Line plot",
    "content": "The most obvious thing you would do. Simply drawing a line plot of time series data actually reveals a lot of insight due to the temporal nature of the data. Stacking multiple time series data on top of each other can reveal interesting patterns. ",
    "url": "/docs/data-science/time-series/time-series-data.html#line-plot",
    "relUrl": "/docs/data-science/time-series/time-series-data.html#line-plot"
  },"889": {
    "doc": "Exploring Time Series Data",
    "title": "Difference histogram",
    "content": "When you use a value-frequency histogram for a time series, it is often beneficial to plot the histogram of the difference between adjacent values. Because each value by itself may not be very informative. It is also important to choose the right bin size. Otherwise you’ll just get a bunch of meaningless spikes that mask the underlying distribution, which is not very informative. Plotting the difference histogram can also be used to determine long-term bias in the data: whether the values tend to increase or decrease in the long run. ",
    "url": "/docs/data-science/time-series/time-series-data.html#difference-histogram",
    "relUrl": "/docs/data-science/time-series/time-series-data.html#difference-histogram"
  },"890": {
    "doc": "Exploring Time Series Data",
    "title": "Scatter plot of features",
    "content": "If we have multiple time series data, we can match the data points by timestamp and plot them against each other using a scatter plot. This can reveal correlations between different features or indexes. However, just like the histogram, each value in a time series data are not very informative. So it is often better to plot using the difference between adjacent values. Instead of matching the data points by timestamp, you can do another correlation analysis by giving a time lag to one of the time series data. Doing so can reveal whether one feature can be a predictor of another. ",
    "url": "/docs/data-science/time-series/time-series-data.html#scatter-plot-of-features",
    "relUrl": "/docs/data-science/time-series/time-series-data.html#scatter-plot-of-features"
  },"891": {
    "doc": "Exploring Time Series Data",
    "title": "Rolling windows",
    "content": "The rolling window method can be used to identify important characteristics like stationarity, etc. ",
    "url": "/docs/data-science/time-series/time-series-data.html#rolling-windows",
    "relUrl": "/docs/data-science/time-series/time-series-data.html#rolling-windows"
  },"892": {
    "doc": "Towers of Hanoi",
    "title": "Towers of Hanoi",
    "content": "One of the most famous recursive problems. | Gist of the problem | Recursive solution . | Base case | Inductive step | . | Pseudocode | . ",
    "url": "/docs/compsci/algo/towers-of-hanoi.html",
    "relUrl": "/docs/compsci/algo/towers-of-hanoi.html"
  },"893": {
    "doc": "Towers of Hanoi",
    "title": "Gist of the problem",
    "content": "There are three pegs (source, goal, auxiliary), and $n$ disks of different sizes. The disks are initially sorted on the source peg, with the largest disk at the bottom. The goal is to move all the disks to the target peg, in the same sorted order. You can only move one disk at a time (the one on the top of a peg), and you cannot place a larger disk on top of a smaller disk. ",
    "url": "/docs/compsci/algo/towers-of-hanoi.html#gist-of-the-problem",
    "relUrl": "/docs/compsci/algo/towers-of-hanoi.html#gist-of-the-problem"
  },"894": {
    "doc": "Towers of Hanoi",
    "title": "Recursive solution",
    "content": " ",
    "url": "/docs/compsci/algo/towers-of-hanoi.html#recursive-solution",
    "relUrl": "/docs/compsci/algo/towers-of-hanoi.html#recursive-solution"
  },"895": {
    "doc": "Towers of Hanoi",
    "title": "Base case",
    "content": "When there is one disk, move it from the source peg to the target peg. ",
    "url": "/docs/compsci/algo/towers-of-hanoi.html#base-case",
    "relUrl": "/docs/compsci/algo/towers-of-hanoi.html#base-case"
  },"896": {
    "doc": "Towers of Hanoi",
    "title": "Inductive step",
    "content": "Note our base case for a single disk. We can move the single disk from the source peg to the target peg. It is important to note that a solution is not limited to just moving the disk from the source peg to the target peg. It is a solution that can be used to move the disk from one peg to another peg (given the help of the auxiliary peg). Now what if there was actually a larger second disk at the bottom of the source peg? . Since this is the largest disk, it does not invalidate our previous solution. We can still use our solution for the single disk, so that one disk is now on the auxiliary peg. Then, we can move the second disk (the largest disk) from the source peg to the target peg. It is important to understand that once you’ve moved the current largest disk to the target peg, it essentially disappears from the problem as if it was never there in the first place. Now we have a single disk on the auxiliary peg, which means we can use our base case solution to move the disk to the target peg. Then we have solved the problem for two disks. There exists a way to move the two disks from one peg to another peg in the same sorted order. What if there was actually a third disk at the bottom of the initial source peg? . For the top two disks, we can use the previous solution to move them to the auxiliary peg. Then we move the third disk (largest disk) from the source peg to the target peg. Since the largest disk is on the target peg, it disappears from the problem. Now we have two disks on the auxiliary peg, we can use the previous solution to move them to the target peg. Then we have solved the problem for three disks. With $n$ disks, the same thing happens. We move the top $n-1$ disks to the auxiliary peg, using a previous solution. We move the largest disk left on the source peg to the target peg. Then this target peg disappears from the problem. We can move the $n-1$ disks from the auxiliary peg to the target peg, because we’re back to our previous case of $n-1$ disks. ",
    "url": "/docs/compsci/algo/towers-of-hanoi.html#inductive-step",
    "relUrl": "/docs/compsci/algo/towers-of-hanoi.html#inductive-step"
  },"897": {
    "doc": "Towers of Hanoi",
    "title": "Pseudocode",
    "content": "def move_disk(from, to): print \"Move disk from \" + from + \" to \" + to def towers_of_hanoi(n, source, target, aux): if n &lt; 0: return if n == 1: return move_disk(source, target) towers_of_hanoi(n - 1, source, aux, target) move_disk(source, target) towers_of_hanoi(n - 1, aux, target, source) . ",
    "url": "/docs/compsci/algo/towers-of-hanoi.html#pseudocode",
    "relUrl": "/docs/compsci/algo/towers-of-hanoi.html#pseudocode"
  },"898": {
    "doc": "1 - Two Sum - Easy",
    "title": "Two Sum",
    "content": ". | Problem | With a hash map | With sorting | . ",
    "url": "/docs/compsci/leetcode/two-sum.html#two-sum",
    "relUrl": "/docs/compsci/leetcode/two-sum.html#two-sum"
  },"899": {
    "doc": "1 - Two Sum - Easy",
    "title": "Problem",
    "content": "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. Input: nums = [2,7,11,15], target = 9 Output: [0,1] . ",
    "url": "/docs/compsci/leetcode/two-sum.html#problem",
    "relUrl": "/docs/compsci/leetcode/two-sum.html#problem"
  },"900": {
    "doc": "1 - Two Sum - Easy",
    "title": "With a hash map",
    "content": ". | Create a hash map that maps each number in nums to its index. | Go through the elements of num and check if target - num[i] is in the map. | If you found a match, return the indices i and hashmap[target - num[i]]. | . This approach is $O(n)$. ",
    "url": "/docs/compsci/leetcode/two-sum.html#with-a-hash-map",
    "relUrl": "/docs/compsci/leetcode/two-sum.html#with-a-hash-map"
  },"901": {
    "doc": "1 - Two Sum - Easy",
    "title": "With sorting",
    "content": ". | Sort the array in ascending order. | Initialize two pointers i and j at the beginning and the end of the array, respectively. | If nums[i] + nums[j] &gt; target, decrement j. | If nums[i] + nums[j] &lt; target, increment i. | If nums[i] + nums[j] == target, return i and j. | . The caveat is i and j are not the indices of the original array. You need to find the indices of nums[i] and nums[j] in the original array. This approach is $O(n \\log n)$ because of the sorting. ",
    "url": "/docs/compsci/leetcode/two-sum.html#with-sorting",
    "relUrl": "/docs/compsci/leetcode/two-sum.html#with-sorting"
  },"902": {
    "doc": "1 - Two Sum - Easy",
    "title": "1 - Two Sum - Easy",
    "content": " ",
    "url": "/docs/compsci/leetcode/two-sum.html",
    "relUrl": "/docs/compsci/leetcode/two-sum.html"
  },"903": {
    "doc": "Useful Commands",
    "title": "Useful CLI Commands",
    "content": ". | lsbom | networksetup | netstat | ifconfig | system_profiler | fzf (+ Oh-My-Zsh) . | Usage example | . | fasd (+ Oh-My-Zsh) | . ",
    "url": "/docs/learned/useful.html#useful-cli-commands",
    "relUrl": "/docs/learned/useful.html#useful-cli-commands"
  },"904": {
    "doc": "Useful Commands",
    "title": "lsbom",
    "content": "Lists the contents of an installer’s bom (bill of materials) file, which contains information on what files were added to the system. My preferred usage: . lsbom -f -l -s -pf /var/db/receipts/&lt;some-package-name&gt;.bom &gt;&gt; package-bom.txt # Inspect manually vim package-bom.txt # Then pipe it to some rm code, etc. # cat package-bom.txt | while read f; do ...; done . ",
    "url": "/docs/learned/useful.html#lsbom",
    "relUrl": "/docs/learned/useful.html#lsbom"
  },"905": {
    "doc": "Useful Commands",
    "title": "networksetup",
    "content": "To see all hardware ports (Wi-Fi, Bluetooth, Thunderbolt, etc.) . networksetup -listallhardwareports . ",
    "url": "/docs/learned/useful.html#networksetup",
    "relUrl": "/docs/learned/useful.html#networksetup"
  },"906": {
    "doc": "Useful Commands",
    "title": "netstat",
    "content": "To see all current in/outbound network connections . netstat -i . ",
    "url": "/docs/learned/useful.html#netstat",
    "relUrl": "/docs/learned/useful.html#netstat"
  },"907": {
    "doc": "Useful Commands",
    "title": "ifconfig",
    "content": "To see all network devices on machine . ifconfig . ",
    "url": "/docs/learned/useful.html#ifconfig",
    "relUrl": "/docs/learned/useful.html#ifconfig"
  },"908": {
    "doc": "Useful Commands",
    "title": "system_profiler",
    "content": "Command line version of the GUI System Profiler on macOS. To list USB devices, . system_profiler SPUSBDataType . ",
    "url": "/docs/learned/useful.html#system_profiler",
    "relUrl": "/docs/learned/useful.html#system_profiler"
  },"909": {
    "doc": "Useful Commands",
    "title": "fzf (+ Oh-My-Zsh)",
    "content": "Usage example . vim $(fzf) . ",
    "url": "/docs/learned/useful.html#fzf--oh-my-zsh",
    "relUrl": "/docs/learned/useful.html#fzf--oh-my-zsh"
  },"910": {
    "doc": "Useful Commands",
    "title": "fasd (+ Oh-My-Zsh)",
    "content": "To be added . ",
    "url": "/docs/learned/useful.html#fasd--oh-my-zsh",
    "relUrl": "/docs/learned/useful.html#fasd--oh-my-zsh"
  },"911": {
    "doc": "Useful Commands",
    "title": "Useful Commands",
    "content": " ",
    "url": "/docs/learned/useful.html",
    "relUrl": "/docs/learned/useful.html"
  },"912": {
    "doc": "Useful Notes",
    "title": "Useful Notes",
    "content": ". | Use multiple GitHub accounts with SSH . | Generate a new SSH key | Add the new SSH key to GitHub account | Modify config | Local config per repository | Add remote | . | . ",
    "url": "/docs/git-hub/github/useful.html",
    "relUrl": "/docs/git-hub/github/useful.html"
  },"913": {
    "doc": "Useful Notes",
    "title": "Use multiple GitHub accounts with SSH",
    "content": "First navigate to ~/.ssh. For organization, create a directory and name it github. All private and public keys for GitHub connection will be placed here. Generate a new SSH key . Follow the ssh-keygen prompt. It will ask you to decide on a name for the file, passphrase, etc. # If Ed25519 algorithm is supported ssh-keygen -t ed25519 -C \"your_github@email.com\" . # Legacy RSA ssh-keygen -t rsa -b 4096 -C \"your_github@email.com\" . Add the new SSH key to GitHub account . Easiest part. Refer to GitHub documentation for step-by-step screencaps. Modify config . Suppose I have two GitHub accounts each associated with personal_email@address.com and work_email@address.com. I’ll assume the private keys are named github-personal and github-work respectively. Now append the following to ~/.ssh/config . Host github-personal HostName github.com User git IdentityFile ~/.ssh/github/github-personal Host github-work HostName github.com User git IdentityFile ~/.ssh/github/github-work . You can define custom host for both as such or have one of them keep the default github.com. Local config per repository . First start a local repo with . git init . Then config local name and email that will be used for that repo . git config --local user.name \"work_name\" git config --local user.email \"work_email@address.com\" . Add remote . Normally you would add an ssh remote by . git remote add origin git@github.com:github_username:repo_name # OR git remote add origin github.com:github_username:repo_name . But this time, . git remote add origin github-work:work_username:repo_name . References: . | GitHub: SSH | . ",
    "url": "/docs/git-hub/github/useful.html#use-multiple-github-accounts-with-ssh",
    "relUrl": "/docs/git-hub/github/useful.html#use-multiple-github-accounts-with-ssh"
  },"914": {
    "doc": "Useful Git Commands",
    "title": "Useful Commands",
    "content": ". | Configuration . | See current configuration | Global user config | Local user config | Command alias | Global ignore | . | Untrack file | Fix previous commit | Rebase | Cherry Pick | Remove or modify a commit with rebase | Reset status . | To remove all uncommitted changes | Undo last (few) commit(s) without losing changes | . | Prune remote branches | . ",
    "url": "/docs/git-hub/git/useful.html#useful-commands",
    "relUrl": "/docs/git-hub/git/useful.html#useful-commands"
  },"915": {
    "doc": "Useful Git Commands",
    "title": "Configuration",
    "content": "See current configuration . git config --list . Global user config . git config --global user.name \"myname\" git config --global user.email \"myemail@example.com\" . Local user config . Useful when you need to use different identity per project, . git config --local user.name \"anothername\" git config --local user.email \"anotheremail@example.com\" . Command alias . If you’re tired of writing long git commands that you frequently use, . git config --global alias.youralias \"command to shorten (without 'git')\" . Global ignore . First create a ~/.gitignore_global file. Then, . git config --global core.excludeFile ~/.gitignore_global . ",
    "url": "/docs/git-hub/git/useful.html#configuration",
    "relUrl": "/docs/git-hub/git/useful.html#configuration"
  },"916": {
    "doc": "Useful Git Commands",
    "title": "Untrack file",
    "content": "When you have a file that you would like to untrack, . git rm -r --cached &lt;file-or-dir&gt; . ",
    "url": "/docs/git-hub/git/useful.html#untrack-file",
    "relUrl": "/docs/git-hub/git/useful.html#untrack-file"
  },"917": {
    "doc": "Useful Git Commands",
    "title": "Fix previous commit",
    "content": "This comes in handy when you make a small change in your code after you’ve committed, but you realize you probably wanted it included in your last commit. # Modify commit message as well git commit --amend . # Keep the commit message git commit --amend --no-edit . If you already pushed the commit to a remote before the ammend, then you need to force push the new changes by git push -f origin master. ",
    "url": "/docs/git-hub/git/useful.html#fix-previous-commit",
    "relUrl": "/docs/git-hub/git/useful.html#fix-previous-commit"
  },"918": {
    "doc": "Useful Git Commands",
    "title": "Rebase",
    "content": "To rebase from a remote, . git fetch git rebase origin/main . Or simply . git pull --rebase . ",
    "url": "/docs/git-hub/git/useful.html#rebase",
    "relUrl": "/docs/git-hub/git/useful.html#rebase"
  },"919": {
    "doc": "Useful Git Commands",
    "title": "Cherry Pick",
    "content": "To cherry pick a single commit from anothe branch, first check the hash of the commit. Then, . git cherry-pick &lt;hash&gt; . ",
    "url": "/docs/git-hub/git/useful.html#cherry-pick",
    "relUrl": "/docs/git-hub/git/useful.html#cherry-pick"
  },"920": {
    "doc": "Useful Git Commands",
    "title": "Remove or modify a commit with rebase",
    "content": "To modify a commit from the past, first check the number of commits you must go back to find the commit you want to modify. Then, . git rebase -i HEAD~&lt;num-commits-to-target-inclusive&gt; . Then you can choose an action for each commit by editing the text editor which looks like: . pick &lt;hash&gt; A Commit Message4 pick &lt;hash&gt; A Commit Message3 pick &lt;hash&gt; A Commit Message2 pick &lt;hash&gt; A Commit Message1 . Modify pick to a command listed in the editor comments. ",
    "url": "/docs/git-hub/git/useful.html#remove-or-modify-a-commit-with-rebase",
    "relUrl": "/docs/git-hub/git/useful.html#remove-or-modify-a-commit-with-rebase"
  },"921": {
    "doc": "Useful Git Commands",
    "title": "Reset status",
    "content": "There are 5 types of reset: --soft, --mixed, --hard, --merge, and --keep. Each type of reset has different effects on the index, working tree, and HEAD. The first three are the most commonly used, and --mixed is the default. | All of them move HEAD to the specified commit. | --soft leaves changes in the staging area. | --mixed keeps the changes in an unstaged state. | --hard discards all changes. | . To remove all uncommitted changes . Changes reset by following commands will be unrecoverable unless you have already pushed to remote. git reset --hard # Reset tracked files to your latest commit git clean -fd # Remove untracked files . Undo last (few) commit(s) without losing changes . git reset HEAD^ # Undo last commit git reset HEAD^^ # Undo last two commits git reset HEAD~3 # Undo last three commits . ",
    "url": "/docs/git-hub/git/useful.html#reset-status",
    "relUrl": "/docs/git-hub/git/useful.html#reset-status"
  },"922": {
    "doc": "Useful Git Commands",
    "title": "Prune remote branches",
    "content": "If you have deleted a remote branch, but it still shows up when you run git branch -a, then you can prune it by, . git remote prune $remote_name # --dry-run to see what will be pruned . If you have a local branch that is still tracking the deleted remote, then you need to delete the local branch first. References: . | Git: Reset | . ",
    "url": "/docs/git-hub/git/useful.html#prune-remote-branches",
    "relUrl": "/docs/git-hub/git/useful.html#prune-remote-branches"
  },"923": {
    "doc": "Useful Git Commands",
    "title": "Useful Git Commands",
    "content": " ",
    "url": "/docs/git-hub/git/useful.html",
    "relUrl": "/docs/git-hub/git/useful.html"
  },"924": {
    "doc": "venv",
    "title": "venv",
    "content": ". | What is venv | Basic usage . | Create environment | Activate environment | Deactivate environment | . | . ",
    "url": "/docs/python/envs/venv.html",
    "relUrl": "/docs/python/envs/venv.html"
  },"925": {
    "doc": "venv",
    "title": "What is venv",
    "content": "venv is Python’s default virtual environment tool. ",
    "url": "/docs/python/envs/venv.html#what-is-venv",
    "relUrl": "/docs/python/envs/venv.html#what-is-venv"
  },"926": {
    "doc": "venv",
    "title": "Basic usage",
    "content": "Create environment . venv creates a virtual environment for the Python version used to invoke it. # If you only need one environment for the project python -m venv venv # If you are going to need multiple environments python -m venv venv/myenv # Creates a nested dir . python -m venv &lt;name-of-env&gt; creates a directory in cwd. The &lt;name-of-env&gt; can be anything you like, but venv is used the most by convention. Just like node_modules, because venv exists within the project root, it is almost always added to .gitignore. Activate environment . Assuming cwd is project root where venv directory exists, . source venv/bin/activate . Once activated, which python will point to . /Path/To/Project/venv/bin/python . Deactivate environment . deactivate . ",
    "url": "/docs/python/envs/venv.html#basic-usage",
    "relUrl": "/docs/python/envs/venv.html#basic-usage"
  },"927": {
    "doc": "vim-plug",
    "title": "vim-plug",
    "content": "To be added . vim-plug Tutorial . ",
    "url": "/docs/others/vim/vim-plug.html",
    "relUrl": "/docs/others/vim/vim-plug.html"
  },"928": {
    "doc": "Stats Vocab",
    "title": "Stats Vocabulary",
    "content": "TBA . | Group | TBA | . ",
    "url": "/docs/statistics/basics/vocabs.html#stats-vocabulary",
    "relUrl": "/docs/statistics/basics/vocabs.html#stats-vocabulary"
  },"929": {
    "doc": "Stats Vocab",
    "title": "Group",
    "content": "In hypothesis testing, we often compare two groups of data. | Control group: the group that does not receive the treatment | Treatment group: the group that receives the treatment | . ",
    "url": "/docs/statistics/basics/vocabs.html#group",
    "relUrl": "/docs/statistics/basics/vocabs.html#group"
  },"930": {
    "doc": "Stats Vocab",
    "title": "TBA",
    "content": ". | skewnewss | kurtosis | . ",
    "url": "/docs/statistics/basics/vocabs.html#tba",
    "relUrl": "/docs/statistics/basics/vocabs.html#tba"
  },"931": {
    "doc": "Stats Vocab",
    "title": "Stats Vocab",
    "content": " ",
    "url": "/docs/statistics/basics/vocabs.html",
    "relUrl": "/docs/statistics/basics/vocabs.html"
  },"932": {
    "doc": "Docker Volumes",
    "title": "Docker Volumes",
    "content": ". | What is a Docker volume | Cases where Docker volume comes in handy . | You want some live-reloading features | You want to persist data upon container shutdown | . | . ",
    "url": "/docs/docker/volumes.html",
    "relUrl": "/docs/docker/volumes.html"
  },"933": {
    "doc": "Docker Volumes",
    "title": "What is a Docker volume",
    "content": "In short, it maps the volume inside a container with some local directory on your computer. If you set the volume to read-only, every change in local directory will be reflected in the container but not vice versa. If you set read-only to false, the contents inside the container and the local directory will remain same throughout the container execution. ",
    "url": "/docs/docker/volumes.html#what-is-a-docker-volume",
    "relUrl": "/docs/docker/volumes.html#what-is-a-docker-volume"
  },"934": {
    "doc": "Docker Volumes",
    "title": "Cases where Docker volume comes in handy",
    "content": "You want some live-reloading features . During development, it is really painful if you have to rebuild or rerun everytime you make a change. By mapping your container volume to a local build context, the container will update itself everytime you make a change to your local code. You want to persist data upon container shutdown . For example if you’re running a DB as a container without a volume mapped, each time the container restarts, the contents of the DB will be erased. However, if you map your container volume to a local directory, the data will be kept even after shutdown. ",
    "url": "/docs/docker/volumes.html#cases-where-docker-volume-comes-in-handy",
    "relUrl": "/docs/docker/volumes.html#cases-where-docker-volume-comes-in-handy"
  },"935": {
    "doc": "Welch's t-Test",
    "title": "Welch’s t-Test",
    "content": "To be added . ",
    "url": "/docs/statistics/basics/welch-t-test.html#welchs-t-test",
    "relUrl": "/docs/statistics/basics/welch-t-test.html#welchs-t-test"
  },"936": {
    "doc": "Welch's t-Test",
    "title": "Welch's t-Test",
    "content": " ",
    "url": "/docs/statistics/basics/welch-t-test.html",
    "relUrl": "/docs/statistics/basics/welch-t-test.html"
  },"937": {
    "doc": "Homebrew x86_64",
    "title": "Homebrew x86_64",
    "content": ". | Installing Homebrew for both arm and x86 . | Install for x86 | Set alias | Opt out of analytics (Optional) | . | . ",
    "url": "/docs/others/homebrew/x86.html",
    "relUrl": "/docs/others/homebrew/x86.html"
  },"938": {
    "doc": "Homebrew x86_64",
    "title": "Installing Homebrew for both arm and x86",
    "content": "Install for x86 . After having installed Homebrew natively, install for x86: . arch -x86_64 /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" . x86 Homebrew will be located in /usr/local/Homebrew while native Homebrew is in /opt/homebrew. Set alias . In your .zshrc, . # .zshrc alias xbrew='arch -x86_64 /usr/local/Homebrew/bin/brew' . Opt out of analytics (Optional) . xbrew analytics off . ",
    "url": "/docs/others/homebrew/x86.html#installing-homebrew-for-both-arm-and-x86",
    "relUrl": "/docs/others/homebrew/x86.html#installing-homebrew-for-both-arm-and-x86"
  },"939": {
    "doc": "6 - Zigzag Conversion - Medium",
    "title": "Zigzag Conversion",
    "content": ". | Problem | Explanation | Solution | . ",
    "url": "/docs/compsci/leetcode/zigzag-conversion.html#zigzag-conversion",
    "relUrl": "/docs/compsci/leetcode/zigzag-conversion.html#zigzag-conversion"
  },"940": {
    "doc": "6 - Zigzag Conversion - Medium",
    "title": "Problem",
    "content": "The string PAYPALISHIRING is written in a zigzag pattern on a given number of rows like this: . P A H N A P L S I I G Y I R . And then read line by line: PAHNAPLSIIGYIR. Input: s = \"PAYPALISHIRING\", numRows = 4 Output: \"PINALSIGYAHRPI\" Explanation: P I N A L S I G Y A H R P I . ",
    "url": "/docs/compsci/leetcode/zigzag-conversion.html#problem",
    "relUrl": "/docs/compsci/leetcode/zigzag-conversion.html#problem"
  },"941": {
    "doc": "6 - Zigzag Conversion - Medium",
    "title": "Explanation",
    "content": "Since the final answer string is built by reading the rows line by line, we should start by thinking our loops should to the same. for (int r = 0; r &lt; numRows; r++) . We need to find a pattern in each row. Take a loot at the example with numRows = 4: . Let’s forget about the diagonals for now and focus on the gray columns. They are evenly spaced out by $6$ each starting from their respective row index. This is because the distance between is . \\[\\text{numRows} + (\\text{numRows} - 2) = 2 * \\text{numRows} - 2\\] where $\\text{numRows}$ comes from traversing vertically along the columns, and the $\\text{numRows} - 2$ comes from traversing diagonally. For the first and last rows, there are no diagonals, so this is the only pattern that matters. For the rows in between the first and last, there are additional indices in between the gray columns. Let’s define . \\[\\text{skip} = 2 * \\text{numRows} - 2\\] At each row $r$, the additional indices can be found by traversing $r + 1$ less vertically and $r - 1$ less diagonally. Therefore, the auxiliary skips are . \\[\\text{auxSkip} = \\text{skip} - (r + 1 + r - 1) = \\text{skip} - 2r\\] . ",
    "url": "/docs/compsci/leetcode/zigzag-conversion.html#explanation",
    "relUrl": "/docs/compsci/leetcode/zigzag-conversion.html#explanation"
  },"942": {
    "doc": "6 - Zigzag Conversion - Medium",
    "title": "Solution",
    "content": "| 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 . | public String convert(String s, int numRows) { if (numRows == 1) return s; int n = s.length(); StringBuilder sb = new StringBuilder(n); int skip = numRows + (numRows - 2); for (int r = 0; r &lt; numRows; r++) { int auxSkip = skip - 2 * r; for (int i = r; i &lt; n; i += skip) { // Gray columns sb.append(s.charAt(i)); // Auxiliary diagonals for the middle rows if (r &gt; 0 &amp;&amp; r &lt; numRows - 1) { int aux = i + auxSkip; // Don't go out of bounds if (aux &lt; n) sb.append(s.charAt(aux)); } } } return sb.toString(); } . | . The complexity is $O(n)$. ",
    "url": "/docs/compsci/leetcode/zigzag-conversion.html#solution",
    "relUrl": "/docs/compsci/leetcode/zigzag-conversion.html#solution"
  },"943": {
    "doc": "6 - Zigzag Conversion - Medium",
    "title": "6 - Zigzag Conversion - Medium",
    "content": " ",
    "url": "/docs/compsci/leetcode/zigzag-conversion.html",
    "relUrl": "/docs/compsci/leetcode/zigzag-conversion.html"
  }
}
